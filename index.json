[{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/","section":"Data Lab Tech TV","summary":"","title":"Data Lab Tech TV","type":"page"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/applications/","section":"Tags","summary":"","title":"Applications","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/architecture/","section":"Tags","summary":"","title":"Architecture","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/ci-cd/","section":"Tags","summary":"","title":"Ci-Cd","type":"tags"},{"content":" Summary # On part 5 of this series, you\u0026rsquo;ll learn how to use multi-project CI/CD with the GitLab Free tier, in order to provision resources like PostgreSQL databases and credentials. You\u0026rsquo;ll also learn how easy it is to deploy your ML model with Docker, once you have the correct infrastructure setup. And, finally, we\u0026rsquo;ll look back at the pros and cons of the implemented architecture, doing a full retrospective, and proposing a redesigned architecture to fix the pitfalls of our current design.\n\u003e Changes to CI/CD # Attempted Refactoring # At first, we attempted to refactor the CI/CD pipeline as follows:\n.ci/ ├── scripts/ │ ├── kafka/ │ ├── ollama/ │ └── postgres/ └── templates/ ├── deploy.yml ├── kafka.yml ├── ollama.yml └── postgres.yml However, if we run a script instead of defining it inline, we won\u0026rsquo;t be able to run these jobs from another repository, which is a concern for our current workflow, as we want to be able to provision databases and credentials, or other resources, by calling jobs within the datalab repository as effortlessly as possible. We ended up going back to the original approach, with templates on the root of .ci/.\nWe also fixed an error in the changes globbing, where infra/services/docker/** should have been infra/services/docker/**/* so that files were matched rather than directories.\nCustom Ubuntu Image # Since we were continuously in need of a few common command line tools, like curl or jq, we decided to build a custom Ubuntu image that we pushed to the container registry for the datalab project. This let us reuse this image directly from our container registry, without the need to add a before_script block to setup the Ubuntu instance each time a new runner was launched for jobs requiring additional commands beyond the base image.\nBuilding and pushing for this image was done through the Terraform project under infra/services/gitlab, so the workflow remains unchanged, assuming you setup your CI/CD variables using your .env via Terraform.\nThe first attempt to push a large image into our container registry failed. This ended up being fixed by adding checksum_disabled to /etc/gitlab/gitlab.rb as follows:\nregistry[\u0026#39;storage\u0026#39;] = { \u0026#39;s3_v2\u0026#39; =\u0026gt; { ... \u0026#39;checksum_disabled\u0026#39; =\u0026gt; true, } } And then SSHing into the GitLab VM and running:\nsudo gitlab-ctl reconfigure This configuration was also added to the L2 (Platform) Terraform project, to the cloud-config for GitLab, so if you\u0026rsquo;re deploying this now, you don\u0026rsquo;t have to worry about this.\nWe also had to change the project visibility to \u0026ldquo;Public\u0026rdquo; for datalab, otherwise CI/CD job were unable to pull from the container registry. Of course our GitLab instance is not exposed to the outside, otherwise we would need a different strategy to handle access.\nImproving Postgres Workflow # We also improved the postgres template in order to store credentials as JSON, using a CI/CD variable, so that we could return them, if run multiple times, as opposed to just producing a new password and not providing access to the original credentials. This is a requirement that we missed during the initial design of this workflow, as we\u0026rsquo;ll need these credentials on our external application projects during deployment. This also makes the workflow idempotent.\nHere\u0026rsquo;s a summary of changes:\nCredentials no longer printed in logs. Credentials stored as JSON, using a CI/CD variable. External projects will always need to call this workflow to load credentials into the env during application container deployment. Database and credentials will be created as required, if they don\u0026rsquo;t exist. Notice that any project can access any database and credentials, as this wasn\u0026rsquo;t a concern here, but the JSON format and workflow can be expanded to handle this if required. A better way, as we\u0026rsquo;ll see next, is to just stop resisting using an extra service to handle secrets.\nMulti-Project CI/CD on GitLab Free Tier # Templates for Provisioning # We provide, on the original datalab project, CI/CD templates that can be included as required, on external projects, as well as internally, to provision resources, be it within PostgreSQL, Kafka, or Ollama.\nSince we are using the GitLab Free tier, we lack the syntactic sugar to trigger jobs on an external project, particularly when we need artifacts from that project. So, the best way to handle this is via the REST API.\nWe provide the following CI/CD templates:\n.ci/provision/ ├── kafka.yml ├── ollama.yml └── postgres.yml These are set to call the CI/CD pipeline with the appropriate input values to provision resources. The strategy is similar for all three templates. Here\u0026rsquo;s how we do it for .ci/provision/ollama.yml:\nspec: inputs: pull: description: \u0026#34;Pull Ollama model\u0026#34; type: string --- stages: - provision variables: PROVISIONER_ID: datalabtechtv%2Fdatalab provision_ollama_model: stage: provision image: gitlab:5050/datalabtechtv/datalab/ubuntu:custom variables: PULL: $[[ inputs.pull ]] script: - echo \u0026#34;Triggering downstream pipeline\u0026#34; - | PIPELINE_ID=$(curl -s -X POST \\ --form token=$GITLAB_TRIGGER_TOKEN \\ --form ref=infra \\ --form inputs[ollama_pull]=$PULL \\ $CI_API_V4_URL/projects/$PROVISIONER_ID/trigger/pipeline | jq -r \u0026#39;.id\u0026#39;) - echo \u0026#34;Triggered pipeline $PIPELINE_ID\u0026#34; - | while true; do STATUS=$(curl -s -H \u0026#34;PRIVATE-TOKEN: $GITLAB_TOKEN\u0026#34; \\ $CI_API_V4_URL/projects/$PROVISIONER_ID/pipelines/$PIPELINE_ID \\ | jq -r \u0026#39;.status\u0026#39;) echo \u0026#34;Pipeline status: $STATUS\u0026#34; [[ \u0026#34;$STATUS\u0026#34; == \u0026#34;success\u0026#34; || \u0026#34;$STATUS\u0026#34; == \u0026#34;failed\u0026#34; ]] \u0026amp;\u0026amp; break sleep 10 done The spec.inputs will be set to whatever information you need from an external project to provision the required resource:\nspec: inputs: pull: description: \u0026#34;Pull Ollama model\u0026#34; type: string --- Then, we set stages to provision, which is the only one required for these template:\nstages: - provision External projects that include this template must make sure that this stage is defined for them as well.\nWe set a global variable, PROVISIONER_ID, with the project ID for datalab in its string format:\nvariables: PROVISIONER_ID: datalabtechtv%2Fdatalab And, finally, we define the provisioning job, which always triggers the datalab pipeline regardless of where it\u0026rsquo;s called from. Let\u0026rsquo;s take a look at the script for the provision_ollama_model job.\nFirst, we trigger the pipeline, obtaining it\u0026rsquo;s run ID:\necho \u0026#34;Triggering downstream pipeline\u0026#34; PIPELINE_ID=$(curl -s -X POST \\ --form token=$GITLAB_TRIGGER_TOKEN \\ --form ref=infra \\ --form inputs[ollama_pull]=$PULL \\ $CI_API_V4_URL/projects/$PROVISIONER_ID/trigger/pipeline | jq -r \u0026#39;.id\u0026#39;) echo \u0026#34;Triggered pipeline $PIPELINE_ID\u0026#34; Notice that we need the GITLAB_TRIGGER_TOKEN we previously set. As you can see, we send the user configs via inputs[...] form data fields.\nWe then poll the API every 10 seconds to check if the pipeline has finished:\nwhile true; do STATUS=$(curl -s -H \u0026#34;PRIVATE-TOKEN: $GITLAB_TOKEN\u0026#34; \\ $CI_API_V4_URL/projects/$PROVISIONER_ID/pipelines/$PIPELINE_ID \\ | jq -r \u0026#39;.status\u0026#39;) echo \u0026#34;Pipeline status: $STATUS\u0026#34; [[ \u0026#34;$STATUS\u0026#34; == \u0026#34;success\u0026#34; || \u0026#34;$STATUS\u0026#34; == \u0026#34;failed\u0026#34; ]] \u0026amp;\u0026amp; break sleep 10 done Once this is run, control will be ceded to the external project CI/CD pipeline.\nExternal Test Project # We created an external test project called datalab-infra-test to demo how this works.\nFirst, we needed a trigger token from the datalab project, which was created by navigating to Settings → CI/CD → Pipeline trigger tokens → Add new token, under datalab. We then stored the token in the GITLAB_TRIGGER_TOKEN CI/CD variable under datalab-infra-test.\nAdditionally, we had to reconfigure /etc/gitlab-runner/config.toml to increase job concurrency, otherwise we wouldn\u0026rsquo;t be able to trigger a job, and wait for it from another job—with the default concurrency of 1, the pipeline would just freeze completely.\nWe SSHed into the GitLab VM, set concurrency = 4, and restarted the runner with:\nsudo sed -i \u0026#39;s/^concurrent = 1/concurrent = 4/\u0026#39; /etc/gitlab-runner/config.toml sudo gitlab-runner restart This configuration was also added to the L2 (Platform) Terraform project, within the cloud-config for GitLab, so if you\u0026rsquo;re deploying this now you won\u0026rsquo;t have to worry about it.\nFor example, if you need a Postgres database and credentials, you can configure your CI/CD jobs as follows:\nstages: - provision - test variables: POSTGRES_DB_USER: ci_cd_user POSTGRES_DB_NAME: ci_cd_db include: - project: datalabtechtv/datalab ref: infra file: \u0026#39;.ci/provision/postgres.yml\u0026#39; inputs: db_user: $POSTGRES_DB_USER db_name: $POSTGRES_DB_NAME test_db_connection: stage: test image: postgres:18.0-alpine needs: - fetch_db_credentials script: - \u0026#39;echo Connecting to database: $DB_NAME\u0026#39; - \u0026#39;echo Connecting with user: $DB_USER\u0026#39; - PGPASSWORD=$DB_PASS psql -h docker-shared -U $DB_USER -d $DB_NAME -c \u0026#39;\\q\u0026#39; Here, test_db_connection would usually be replaced by something like a docker compose up for your own application. The point here is that this workflow will ensure that the database you need is created, and it will handle the secrets for you, making them available as env vars.\nFor Kafka and Ollama, we only run a provisioning job, since we don\u0026rsquo;t need any credentials back from the job, but for Postgres, the pipeline will also fetch the job ID for psql_create_db, which contains the credentials.env artifact (this expires after 15m), loading those credentials as environment variables. The pipeline for the test project looks like this:\nAnd now you know of a CI/CD strategy, running on top of the GitLab Free tier, for provisioning resources in your data lab infrastructure! It might not be the best, but it works. Of course, we\u0026rsquo;ll keep improving on it, and we\u0026rsquo;ll share everything with you, as we go!\nModel Deployment # Starting Point # Last time, on our ML End-to-End Workflow we had produced a REST endpoint, using FastAPI, that provided a way to run inference over one or multiple models (A/B/n testing) that had been previously logged to MLflow. Optionally, we could log the inference to DuckLake, which was running on top of a local SQLite catalog and a remote MinIO storage. Logged inferences were streamed to a Kafka topic, and then consumed and buffered, up to a point when they were inserted into the appropriate DuckLake table.\nWhat we want to do now is prepare this REST API to be deployed on the Docker instance running on the docker-apps VM, while using available services running on docker-shared. This includes MLflow and Kafka, but also PostgreSQL and MinIO (L1) for DuckLake. Today, we\u0026rsquo;ll only be concerned with ensuring MLflow and Kafka are integrated, as we\u0026rsquo;ll have a blog post (and video) focusing on migrating your catalog from SQLite to PostgreSQL, at which time we\u0026rsquo;ll configure DuckLake adequately to run on top of docker-shared services.\nAsking CI/CD for Kafka Topics # Since our goal is essentially to expose ml.server, which is a part of the datalab project, we\u0026rsquo;ll setup the CI/CD within this project. This time, we use two trigger jobs, for each of our Kafka topics, one for the logging the inference results (provision_mlserver_results_topic), and the other one to handle inference feedback sent by our users (provision_mlserver_feedback_topic).\nBoth jobs will be similar, so let\u0026rsquo;s take a look at provision_mlserver_results_topic:\nprovision_mlserver_results_topic: stage: deploy trigger: include: - local: .ci/provision/kafka.yml inputs: topic: ml_inference_results group: lakehouse-inference-result-consumer strategy: depend rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;push\u0026#34; changes: - .ci/deploy.yml - infra/apps/docker/**/* - if: \u0026#39;\u0026#34;$[[ inputs.force_apps_deploy ]]\u0026#34; == \u0026#34;true\u0026#34;\u0026#39; Similarly to what we did for the datalab-infra-test project, we include the provision template, but this time it\u0026rsquo;s a local include. We ask that it creates topic ml_inference_results, initializing a consumer for it with group lakehouse-inference-result-consumer.\nThe job triggering rules match the ones that we use for our apps_deploy job.\nDeploying Applications # The apps_deploy job is defined under .ci/deploy.yml, for the datalab project, as follows:\napps_deploy: stage: deploy image: docker:28.4.0-cli needs: - provision_mlserver_results_topic - provision_mlserver_feedback_topic variables: DOCKER_HOST: tcp://docker-apps:2375 DOCKER_BUILDKIT: 1 INFERENCE_RESULTS_TOPIC: ml_inference_results INFERENCE_FEEDBACK_TOPIC: lakehouse-inference-result-consumer INFERENCE_RESULTS_GROUP: ml_inference_feedback INFERENCE_FEEDBACK_GROUP: lakehouse-inference-feedback-consumer script: - docker compose -p datalab -f infra/apps/docker/compose.yml up -d --build - docker ps rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;push\u0026#34; changes: - .ci/deploy.yml - infra/apps/docker/**/* - if: \u0026#39;\u0026#34;$[[ inputs.force_apps_deploy ]]\u0026#34; == \u0026#34;true\u0026#34;\u0026#39; Regarding the rules, notice that we provide a new boolean input that we can set during pipeline triggering to force redeploy the docker compose for our applications. This is useful when we just want to update the env vars for it.\nAs you can see, we set the two topic provisioning jobs as a dependency, and we then configure the environment variables required by our ml.server REST API. Also notice that we set DOCKER_BUILDKIT, which will reduce the overhead of redeploying with the --build flag, as image layers will be cached between deployments.\nDocker Compose # Let\u0026rsquo;s take a look at infra/apps/docker/compose.yml:\nservices: mlserver: build: context: ../../../ dockerfile: infra/apps/docker/mlserver/Dockerfile ports: - \u0026#34;8000:8000\u0026#34; environment: MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI} KAFKA_BROKER_ENDPOINT: ${KAFKA_BROKER_ENDPOINT} healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8000/health\u0026#34;] interval: 10s retries: 5 restart: unless-stopped Notice that we set our build context to the root of the datalab project, so that it will pickup our .env when building locally. Otherwise, environment variables will either be set via the apps_deploy CI/CD job, or as a CI/CD variable, as is the case for MLFLOW_TRACKING_URI and KAFKA_BROKER_ENDPOINT, which are set to:\nMLFLOW_TRACKING_URI=\u0026#34;http://docker-shared:5000\u0026#34; KAFKA_BROKER_ENDPOINT=\u0026#34;docker-shared:9092\u0026#34; Dockerfile # The Dockerfile for ml.server is quite minimal, and based on the official uv image bundled with Python 3.13 and running on Debian Trixie:\nFROM astral/uv:python3.13-trixie-slim RUN apt update \u0026amp;\u0026amp; apt install -y git curl WORKDIR /datalab COPY pyproject.toml pyproject.toml COPY uv.lock uv.lock RUN uv sync --frozen COPY . . RUN uv sync --frozen ENTRYPOINT [\u0026#34;./infra/apps/docker/mlserver/docker-entrypoint.sh\u0026#34;] Order matters, if you want to optimize caching.\nFirst we install system dependencies—our pyproject.toml is using a dependency straight from a git repo, so we\u0026rsquo;ll need the git command:\nRUN apt update \u0026amp;\u0026amp; apt install -y git curl Then, we switch to /datalab and copy only the require files to install uv dependencies:\nWORKDIR /datalab COPY pyproject.toml pyproject.toml COPY uv.lock uv.lock RUN uv sync --frozen Installing dependencies before copying the source code for datalab will ensure that, unless dependencies change, we\u0026rsquo;ll be able to change the code in datalab and redeploy without having to reinstall all dependencies again, which takes quite a while.\nThen, we finally copy our complete datalab repo and install our source code as the last missing dependency:\nCOPY . . RUN uv sync --frozen docker-entrypoint.sh # We set the entry point for our container as a shell script that loads the Python virtual environment and then calls the CLI command to start the REST API server:\n###!/usr/bin/env bash set -e ### shellcheck source=/dev/null . .venv/bin/activate dlctl ml server \u0026#34;$@\u0026#34; Using set -e will ensure that, if any command fails, the script will terminate there.\nRetrospective No. 1 # We\u0026rsquo;ll now do a retrospective on the architecture that we designed for our data lab infrastructure, identifying the good and the bad, and proposing a redesigned architecture that takes all of this into account.\nThese are my fairly unedited notes. For a more digestible version, please what the video on this topic, where I restructure this into smaller topics fitting of a slide deck.\nWhat Went Well # The architecture was deployable, and everything works! Having a custom Ubuntu image for GitLab runners was useful to avoid constant apt update and package installs, which take time to download and install, each time a job with these requirements is run. Container registry was already useful for the Ubuntu custom image. To Improve # Splitting Docker into multiple VMs was a bad move—a single beefier instance would have been better. It\u0026rsquo;s easier to pool resources, but it also lowers the overhead of communicating among services within the same Docker instance. Using GitLab for secret management along with .env and terraform.tfvars is a pain—we might have been better off deploying HashiCorp Vault into Layer 1 and just using that for everything, with a dev version on docker compose for a local deployment. We might use a Vault Agent to load secrets as env vars as well. We have a container registry, but we haven\u0026rsquo;t used it yet—we might need a workflow to manage images separately while tracking their versions. It might have been better to go with Gitea, which still has a container registry as well as CI/CD runners, rather than GitLab, given resource constraints. GitLab is also quite bloated for a small home lab, running a few redundant services by default, like its own PostgreSQL instance, which we don\u0026rsquo;t need, Prometheus, which we don\u0026rsquo;t care about, or Praefect for HA, which we don\u0026rsquo;t use. GitLab\u0026rsquo;s CI/CD must be defined in a single pipeline, as there are no separate workflows, like with GitHub Actions, or Gitea CI/CD for that matter. Documentation is hard to browse, mainly due to the project\u0026rsquo;s complexity and dimension. Some components can be quite slow likely due to the interpreted nature of the Ruby language (e.g., starting gitlab-rails console takes nearly 30 seconds🥶). Monetization is dependent on feature tiers, which makes it harder to get into (e.g., multi-project pipelines that require needs:project only work in Premium or Ultimate tiers). Given the single workflow/pipeline approach of GitLab CI/CD, and assuming we would continue to use GitLab, a cleaner way to activate different workflows would have been to use a boolean input for each workflow to determine whether to activate the corresponding jobs—this is cleaner and more general than relying on non-empty values. We used a few init services on our Docker Compose project, but we could have just implemented this via CI/CD and strip it completely from Compose. Maybe we could have produced a single Terraform project for all layers of the infrastructure, although it\u0026rsquo;s unclear whether setting layer dependencies would be needlessly complex to manage. Not using Ansible was a bad move—cloud-init is great for provisioning standard VMs, but not to handle configs, specially when we might need to change them. Having a proxy cache would be useful to avoid too many requests to package repositories (e.g., apt), specially during the initial setup stage, but also if we\u0026rsquo;re continuously installing packages within runners for a few workflows, it will make sense to avoid constantly connecting to the base servers, both to ease load on the servers and to improve speed locally. MLflow is running on a SQLite backend, but we do have a PostgreSQL instance that we should switch to. Redesigning the Architecture # Here are the overall changes for each layer:\nL1: Foundation Add nginx to serve as a proxy cache for apt, apk, or others. Add HashiCorp Vault, since it integrates with the shell environment, via the vault agent, with Terraform, via its official provider, and with CI/CD, either via the vault agent, or through the Terraform provider, depending on whether we prefer a more or less real-time configuration update. Keep Terraform for deployment, but replace mutable configuration management with Ansible. L2: Platform Combine the three Docker VMs into a single VM. Keep Terraform for deployment, but replace mutable configuration management with Ansible. L3: Services Nothing changes here, except we extracted DuckLake into its own \u0026ldquo;L3: Edge\u0026rdquo; layer, since it doesn\u0026rsquo;t really run on the infrastructure, at least not directly, but on client machines, like a desktop or laptop, connecting to the PostgreSQL and MinIO instances. L4: Applications Added NodeJS as an example, since we might want to deploy our own web apps (e.g., dynamic visualizations for our data science projects). Made it clear that all apps are deployed as containers in this layer. Notice that it might also be the case that Gitea cannot adequately replace GitLab for our needs, and there is nothing free capable of doing it in a satisfactory way. We\u0026rsquo;ll need to test and compare Gitea with GitLab first. We might end up keeping GitLab in the stack—it\u0026rsquo;s hard to predict at this time.\n","date":"21 October 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-apps/","section":"","summary":"Summary # On part 5 of this series, you\u0026rsquo;ll learn how to use multi-project CI/CD with the GitLab Free tier, in order to provision resources like PostgreSQL databases and credentials.","title":"Data Lab Infra - Part 5: Retrospective \u0026 MLOps - Model Deployment","type":"posts"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/categories/devops/","section":"Categories","summary":"","title":"DevOps","type":"categories"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/gitlab/","section":"Tags","summary":"","title":"Gitlab","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/homelab/","section":"Tags","summary":"","title":"Homelab","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/kafka/","section":"Tags","summary":"","title":"Kafka","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/mlflow/","section":"Tags","summary":"","title":"Mlflow","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/categories/mlops/","section":"Categories","summary":"","title":"MLOps","type":"categories"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/ollama/","section":"Tags","summary":"","title":"Ollama","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/postgres/","section":"Tags","summary":"","title":"Postgres","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/retrospective/","section":"Tags","summary":"","title":"Retrospective","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/video/","section":"Tags","summary":"","title":"Video","type":"tags"},{"content":"","date":"14 October 2025","externalUrl":null,"permalink":"/tags/core-services/","section":"Tags","summary":"","title":"Core-Services","type":"tags"},{"content":" Summary # On part 4 of this series, you\u0026rsquo;ll learn how to automate data stack deployment using docker compose and GitLab, setting up CI/CD variables with Terraform, and using GPU passthrough to a Docker VM running on Proxmox.\nWe\u0026rsquo;ll deploy Portainer, PostgreSQL, MLflow, Kafka, and Ollama with Open WebUI. We\u0026rsquo;ll also implement a CI/CD pipeline that lets us create a PostgreSQL database and associated credentials, create a Kafka topic and initialize consumers for topics/groups, and pull an Ollama model.\nBefore starting to implement our .gitlab-ci.yml, we noticed that the CPU for the GitLab VM was overloaded, and the GitLab instance was unstable and even froze over long periods of time. We describe how we addressed these issues, having to significantly increase the resource allocation for the GitLab VM.\n\u003e Shared Service Planning # Our core services are focused on three main categories: Data Storage \u0026amp; Management, Data Processing, and ML \u0026amp; AI.\nIn the future, we would like to also consider software in the following categories: Data Visualization \u0026amp; BI, Monitoring \u0026amp; Logging, Search \u0026amp; Indexing, and Backup \u0026amp; Archiving.\nData Storage \u0026amp; Management # Category Service Description Object Storage MinIO S3-compatible object store. Already a part of the [[Layer 1 - Foundation|foundation]] layer. Relational Database PostgreSQL General-purpose relational database to share among all services. Data Lakehouse DuckLake Data Lakehouse solution (runs on top of MinIO and PostgreSQL, without any additional services required). Data Processing # Category Service Description Batch Processing DuckDB DuckLake is accessed via DuckDB, which we also use for batch processing (runs on local machines, not the data lab infra). Stream Processing Kafka Even streaming platform, combined with the faust Python library for stream processing (planned but untested; takes the place of the likes of Flink or Spark Streaming). ETL Pipelines Just Makefile style task runner. Since our pipelines are based on calling dlctl commands, just is a good minimal tool to help organize our pipelines. ML \u0026amp; AI # Category Service Description ML Platforms MLflow Supports the ML lifecycle, from experimentation to deployment. LLMs Ollama + Open WebUI Provides a local chatbot via Open WebUI and it can also be used by applications to produce LLM pipelines (e.g., using LangChain). Future Services # Other services that we might consider in the future include JupyterHub (ML \u0026amp; AI), Prometheus and Grafana (Monitoring \u0026amp; Logging), Elasticsearch (Search \u0026amp; Indexing), and even Superset (Data Visualization \u0026amp; BI). Later, we\u0026rsquo;ll also explore backup solutions, and, if it makes sense, we\u0026rsquo;ll consider switching from just to a more specialized orchestration tool, like Airflow. However, it is more likely that we downsize the data stack rather than expand it, as we are looking for a minimal but robust data stack to run on-prem.\nOur current setup is already feeling overcomplex, but we will improve on this as we go. For example, if we run Ollama on the docker-shared VM, we\u0026rsquo;ll need to enable GPU passthrough for it, but what happens when we want to run project-specific apps that also require the GPU, when those are meant to run on docker-apps? Keep in mind that consumer GPUs do not support virtualization. We might be better off with a single Docker VM for everything. And this would even make it easier to use simpler UIs, like Dockge, as a lightweight alternative to Portainer.\nGPU-Passthrough for QEMU VMs # On Layer 1 - Foundation we had setup GPU support with NVIDIA drivers for the host machine, i.e., Proxmox itself, which only works for LXCs. Since our Docker host is running on a VM, we\u0026rsquo;ll need to rollback and disable any drivers on the host machine, so that we can install them on the VM instead. Let\u0026rsquo;s go through the required steps.\nUninstall NVIDIA Drivers from Host # First, let\u0026rsquo;s uninstall the NVIDIA drivers from the host machine:\nnvidia-uninstall Delete or comment out any NVIDIA related modules from /etc/modules:\n# nvidia # nvidia-modeset # nvidia_uvm And reboot.\nEnable IOMMU and Passthrough # First, we need to edit /etc/default/grub to set:\nGRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;quiet amd_iommu=on iommu=pt\u0026#34; Setting iommu=pt is what enables passthrough (pt). If your CPU is intel, use intel_iommu=on instead. You can use lscpu to check which CPU you have.\nThen, add the following modules to /etc/modules:\nvfio vfio_iommu_type1 vfio_pci vfio_virqfd Now, let\u0026rsquo;s configure VFIO, but first let\u0026rsquo;s get the device IDs for the NVIDIA device:\nlspci -nn | grep NVIDIA In my case, this is the output I get, with four IDs:\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation TU106M [GeForce RTX 2060 Mobile] [10de:1f15] (rev a1) 01:00.1 Audio device [0403]: NVIDIA Corporation TU106 High Definition Audio Controller [10de:10f9] (rev a1) 01:00.2 USB controller [0c03]: NVIDIA Corporation TU106 USB 3.1 Host Controller [10de:1ada] (rev a1) 01:00.3 Serial bus controller [0c80]: NVIDIA Corporation TU106 USB Type-C UCSI Controller [10de:1adb] (rev a1) However, we\u0026rsquo;ll only need the IDs for VGA and Audio, which, in our case, are 10de:1f15 and 10de:10f9, respectively. Now, we can edit /etc/modprobe.d/vfio.conf:\noptions vfio-pci ids=10de:1f15,10de:10f9 disable_vga=1 Make sure to replace the two IDs with the ones for your own system!\nFinally, we should update grub and initramfs, and then reboot:\nupdate-grub update-initramfs -u -k all reboot Check if Passthrough Is Active # You can check that VFIO is the active driver for your GPU, by running the following command with for your PCI device (e.g., 01:00.0):\nlspci -k -nn -s \u0026#34;01:00.0\u0026#34; Among other things, it should print:\nKernel driver in use: vfio-pci If another driver is active, you might have to edit /etc/modprobe.d/blacklist.conf and blacklist all NVIDIA drivers:\nblacklist nouveau blacklist nvidia blacklist nvidiafb blacklist nvidia_drm blacklist nvidia_modeset blacklist nvidia_uvm If you had to update the blacklist, reboot, and re-run the lspci command to check again.\nUpdating VM with GPU Access # Let\u0026rsquo;s go back to our Terraform platform project and add two PCI mappings for the GPU VGA and Audio devices:\nresource \u0026#34;proxmox_virtual_environment_hardware_mapping_pci\u0026#34; \u0026#34;gpu_vga\u0026#34; { name = \u0026#34;gpu_vga\u0026#34; map = [ { node = var.pm_node path = \u0026#34;0000:01:00.0\u0026#34; id = \u0026#34;10de:1f15\u0026#34; subsystem_id = \u0026#34;17aa:3a47\u0026#34; iommu_group = 10 } ] } resource \u0026#34;proxmox_virtual_environment_hardware_mapping_pci\u0026#34; \u0026#34;gpu_audio\u0026#34; { name = \u0026#34;gpu_audio\u0026#34; map = [ { node = var.pm_node, path = \u0026#34;0000:01:00.1\u0026#34;, id = \u0026#34;10de:10f9\u0026#34;, subsystem_id = \u0026#34;10de:10f9\u0026#34; iommu_group = 10 } ] } To find out the IOMMU group and the subsystem ID, run:\nlspci -nnv -s \u0026#34;01:00.0\u0026#34; lspci -nnv -s \u0026#34;01:00.1\u0026#34; You\u0026rsquo;ll see something like:\n01:00.1 Audio device [0403]: NVIDIA Corporation TU106 High Definition Audio Controller [10de:10f9] (rev a1) Subsystem: NVIDIA Corporation TU106 High Definition Audio Controller [10de:10f9] Flags: fast devsel, IRQ 255, IOMMU group 10 Memory at d1000000 (32-bit, non-prefetchable) [disabled] [size=16K] Capabilities: [60] Power Management version 3 Capabilities: [68] MSI: Enable- Count=1/1 Maskable- 64bit+ Capabilities: [78] Express Endpoint, IntMsgNum 0 Capabilities: [100] Advanced Error Reporting Kernel driver in use: vfio-pci Kernel modules: snd_hda_intel Under Subsystem you\u0026rsquo;ll find the ID in brackets, and under Flags you\u0026rsquo;ll find IOMMU group \u0026lt;n\u0026gt;.\nWe then modified our existing proxmox_virtual_environment_vm.docker resource to make sure that any GPU-enabled Docker VM will use q35 for the machine, adding a mapping for the GPU devices (notice that only one VM can access the GPU, so only docker-shared will have GPU support):\nmachine = try(local.docker[count.index].gpu, false) ? \u0026#34;q35\u0026#34; : \u0026#34;pc\u0026#34; dynamic \u0026#34;hostpci\u0026#34; { for_each = try(local.docker[count.index].gpu, false) ? [ proxmox_virtual_environment_hardware_mapping_pci.gpu_vga.name, proxmox_virtual_environment_hardware_mapping_pci.gpu_audio.name ] : [] content { device = \u0026#34;hostpci${hostpci.key}\u0026#34; mapping = hostpci.value pcie = true } } After we terraform apply and once the VM boots, you should login and run:\nlspci | grep NVIDIA If you find two entries, for the VGA and Audio devices, similar to this, then you\u0026rsquo;ve got access to the GPU:\n01:00.0 VGA compatible controller: NVIDIA Corporation TU106M [GeForce RTX 2060 Mobile] (rev a1) 02:00.0 Audio device: NVIDIA Corporation TU106 High Definition Audio Controller (rev a1) Installing NVIDIA Drivers on the VM # We use cloud-init to install the NVIDIA drivers and add GPU support to Docker. At the stage when we install the drivers, it\u0026rsquo;s likely that on old kernel will be running, so we need to take measures to ensure that the driver will be installed for the kernel loaded in the next boot (i.e., the latest installed). After setting the NVIDIA driver version on local.nvidia_driver_version, the installation is done as follows:\n## This retrieves the kernel name (e.g., 6.8.0-85-generic). ls /boot/vmlinuz-* | sort | tail -n1 | cut -d- -f2- \\ \u0026gt; /run/nvidia-kernel-name ## Install dependencies for building the NVIDIA driver. apt update \u0026amp;\u0026amp; apt install -y build-essential \\ dkms linux-headers-$(cat /run/nvidia-kernel-name) ## Download the installation script. wget https://us.download.nvidia.com/XFree86/Linux-x86_64/${local.nvidia_driver_version}/NVIDIA-Linux-x86_64-${local.nvidia_driver_version}.run ## Run the installation script for kernel that will be loaded on the ## next boot, and DKMS will ensure the driver will be rebuilt when a ## new kernel is installed. sh NVIDIA-Linux-x86_64-${local.nvidia_driver_version}.run \\ --silent --dkms --kernel-name=$(cat /run/nvidia-kernel-name) ## Load the nvidia driver kernel modules. cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;/etc/modules-load.d/modules.conf nvidia nvidia-modeset nvidia_uvm EOF ## Update initamfs to install the modules. update-initramfs -u ## List all GPU devices, make GPU devices accessible by all users, and ## load required NVIDIA modules dynamically, when not loaded. cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;/etc/udev/rules.d/70-nvidia.rules KERNEL==\u0026#34;nvidia\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-smi -L \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia*\u0026#39;\u0026#34; KERNEL==\u0026#34;nvidia_modeset\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-modprobe -c0 -m \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia-modeset*\u0026#39;\u0026#34; KERNEL==\u0026#34;nvidia_uvm\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-modprobe -c0 -u \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia-uvm*\u0026#39;\u0026#34; EOF We turned the cloud-init config from Docker VMs into a Terraform template, adding the previous commands as extracmds when required (i.e., gpu = true). Once the Docker VM is provisioned with GPU access, you should be able to run nvidia-smi with any user and get information about your available GPUs and running processes.\nThat\u0026rsquo;s it, we can run Ollama! 🎉\nInstalling Services # Besides Portainer, which we\u0026rsquo;ll use to monitor our Docker hosts and run small deployment tests, these are the services that we actually need to deploy:\nPortainer PostgreSQL MLflow Kafka Ollama Open WebUI With the exception of Portainer and Open WebUI, these services are already configured in datalab within the docker-compose.yml that we provide for running locally. Therefore, we really just need to adapt the existing configuration, install Portainer and Open WebUI, and make sure our VM, and then Docker, both have access to the GPU, which is the bulk of the work here.\nIn our case, we\u0026rsquo;ll get 6 GiB VRAM to load models, which, in practice, is extra memory that we get to use. The NVIDIA driver can then manage shared memory, as an overflow to RAM, that will take from the VM\u0026rsquo;s assigned memory—I would disable this completely if I could, but in consumer hardware it doesn\u0026rsquo;t seem possible.\nWe\u0026rsquo;ll also migrate our existing docker-compose.yml into the new infra/services/compose.yml file, keeping MinIO available through a dev profile. You\u0026rsquo;ll have to run:\ndocker compose --profile dev up -d This way, we\u0026rsquo;ll keep things tidy.\nSetting Up CI/CD # Before being able to setup a .gitlab-ci.yml to automate docker compose stack deployment, we hit a snag with GitLab that we had to solve. The following section describes the problem and how we solved it, while the last section covers the actual CI/CD implementations.\nHandling GitLab Instability # While I was reworking the Docker VMs, to configure GPU passthrough and install the NVIDIA drivers, I had to destroy the docker-gitlab VM and even reboot Proxmox to uninstall the NVIDIA drivers from the host, as I\u0026rsquo;m not using the GPU on LXCs. Out of nowhere, my GitLab instance started experiencing an extremely high CPU load.\nI ended up increasing the number of cores from 1 to 2, leaving only 1 core dedicated to the host. As planned, increasing the number of cores didn\u0026rsquo;t require the VM to be destroyed, but it did stop it to make change, and then booted it up again. GitLab once again resumed its operation, hitting 100% of 2 cores for a while. Moments later, it looked like it had become stable, but once I visited my repo I got a UI message saying An error occurred while fetching commit data and it froze, experiencing a high CPU load yet again.\nAt that point, after a few VM reboots to regain control, I noticed that kswapd0 was working intensively and that the VM was listing only 2 GiB RAM internally when running free -h, which was the minimum set for ballooning. While it could theoretically be assigned 6 GiB RAM, and other VMs were using only a small fraction of their RAM, memory didn\u0026rsquo;t seem to expand to 6 GiB, so I ended up increasing the floating setting to 4096, with an extra 2048 that I took from docker-apps. Since there is no swap anywhere, kswapd0 was trying to get memory memory from other VMs with ballooning enabled, and it kept trying, while hogging resources.\nDespite all efforts, the GitLab instance remained unstable, so, as a final attempt, I decided to move into resource overcommitment territory, increasing allocation to 4 cores and 8 GiB of RAM—without ballooning, i.e., the 8 GiB were 100% dedicated to the VM. This finally worked, so I decided to move forward with GitLab, for now at least, despite it feeling quite bloated, which I dislike—UNIX philosophy and all.\nBesides the bloat—which might be justified—I also started questioning whether I could even run GitLab, given my resource constraints, without compromising my main services. Gitea has been reported to run with as little as 1 core and 512 MiB RAM, and I did run it in the past on a NAS with a 4-core arm64 and 1 GiB RAM, so I know it runs on environments with limited resources, albeit slower. In the future, I might do a trial run for Gitea, and consider migrating if it runs with less resources, while still fitting my needs. Something for another blog post and video though!\nEither way, lesson learned: don\u0026rsquo;t go below 4 cores and 8 GiB of RAM for GitLab!\nUpdated Resource Allocation Table # The updated resource allocation table, with resource overcommitment, is the following:\nID VM/CT Cores Disk Dedicated Mem. Floating Mem. Free on Max. 101 minio 2 200 2048 2048 4096 201 gitlab 4 100 8192 8192 4096 202 docker-gitlab 1 100 6144 2048 0 203 docker-shared 8 200 20480 12288 -4096 204 docker-apps 2 100 10240 4096 -2048 ALL TOTALS 17 700 47104 22528 -14336 We have now oversubscribed 1 core—17 cores out of 16 available cores are allocated to VMs. This not only means that there is no dedicated core for the host, but also that, in the unlikely scenario that all VMs try to use 100% CPU power, there won\u0026rsquo;t be enough resources available, which will make them compete for CPU time—this is tracked in KVM as steal time—which is ok, as long as we don\u0026rsquo;t abuse it too much.\nWe have also overcommitted 14,336 MiB of memory, over the maximum host memory of 32 GiB. Since we no swap on the host, we\u0026rsquo;ll monitor memory usage and OOM kills so that we tune the overall memory allocation based on real-world statistics, or we decide to simply buy more RAM. In practice, we should never have negative values on the \u0026ldquo;Free on Max.\u0026rdquo; column, except for its total—negative values mean that the VM will never be able to fully allocate its \u0026ldquo;Dedicated Mem.\u0026rdquo;, even when all other VMs are at minimum usage (i.e., only \u0026ldquo;Floating Mem.\u0026rdquo;, the minimum, is allocated).\nAutomating Data Stack with CI/CD # Creating a GitLab Project # First, you need to login to a non-root user on GitLab, and create a new project. I suggest that you call it datalab, but that\u0026rsquo;s optional. Once created, go into Settings → Access tokens and add a new terraform token, with Maintainer role, and api permissions. This will be used to initialize all GitLab variables with your local .env configs, so you don\u0026rsquo;t have to manually input the variables twice (i.e., for generic local use and for CI/CD).\nUnder infra/services/gitlab/terraform.tfvars, make sure to set:\ngitlab_token = \u0026#34;glpat-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.XX.XXXXXXXXX\u0026#34; Then, after having configured your .env, run terraform apply under infra/services/gitlab as usual. This will create the corresponding GitLab variables under your project, masking and hiding all variables with a name containing PASSWORD, ACCESS_KEY, or SECRET.\nSetting Up Git for Deployment # After cloning the datalab repo from GitHub, you should setup your GitLab remotes as follows:\ncd datalab/ git remote add infra git@gitlab:datalabtechtv/datalab.git git config remote.infra.push \u0026#34;+HEAD:infra\u0026#34; This will enable push-only to a GitLab project of your choice, always to the infra branch, always force pushing. This way, we can deploy from our main or dev branch, by running:\ngit push infra Organizing CI/CD Pipeline # Once the variables are available to CI/CD, we can create our .gitlab-ci.yml file. Unfortunately, since GitLab doesn\u0026rsquo;t support multiple workflows, all of our workflows will live under the same pipeline (i.e., Docker Compose deployments, PostgreSQL database and credential creation, Kafka topics and groups, Ollama models).\nWe\u0026rsquo;ll define four stages, one per workflow, with task dependencies being set through needs rather than stages:\nstages: - deploy - postgres - kafka - ollama We\u0026rsquo;ll then split the workflows into four includable files under .ci/:\ninclude: - .ci/deploy.yml - local: .ci/postgres.yml inputs: db_user: $[[ inputs.postgres_db_user ]] db_name: $[[ inputs.postgres_db_name ]] - local: .ci/kafka.yml inputs: topic: $[[ inputs.kafka_topic ]] group: $[[ inputs.kafka_group ]] - local: .ci/ollama.yml inputs: pull: $[[ inputs.ollama_pull ]] Each of these files will have their own spec.inputs, as will our main pipeline under .gitlab-ci.yml, as the header:\nspec: inputs: postgres_db_user: description: \u0026#34;PostgreSQL username to create and grant privileges to\u0026#34; type: string default: \u0026#34;\u0026#34; postgres_db_name: description: \u0026#34;PostgreSQL database to create\u0026#34; type: string default: \u0026#34;\u0026#34; kafka_topic: description: \u0026#34;Kafka topic to create\u0026#34; type: string default: \u0026#34;\u0026#34; kafka_group: description: \u0026#34;Kafka group to create\u0026#34; type: string default: \u0026#34;\u0026#34; ollama_pull: description: \u0026#34;Ollama model name to pull\u0026#34; type: string default: \u0026#34;\u0026#34; --- Docker Deployments # Under .ci/deploy.yml, we will trigger the deployment job whenever files under infra/services/docker/ are changed—this only activates on push.\nservices_deploy: stage: deploy image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - docker compose -p datalab -f infra/services/docker/compose.yml up -d - docker ps rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;push\u0026#34; changes: - infra/services/docker/** As you can see, we use the docker image instead of the default ubuntu runner, setting DOCKER_HOST to the docker-shared context, directly using its hostname and port. The rules are set so that this job only activate on push and if there are changes to the corresponding files. This assumes that the CI/CD variables are already available within the GitLab project.\nCompose Project Details # Our Docker Compose Project has 7 services and an additional 3 init services, that run only once (we set them to restart: no).\nIn order to ensure datalab remains backward compatible in the sense that we can still run the whole stack on a local Docker instance, we include minio as a service, under an optional dev project. MinIO is not a part of the data stack running on layer 3, since we already run it on layer 1, however we keep this as a convenience for quickly spinning up the datalab stack locally.\nApart from minio, we also provide postgres, with an admin user root, ollama, open-webui, mlflow running on a SQLite backend and a MinIO bucket for artifact storage, kafka as a single node without replication, running one broker and controller, and portainer to help monitor our Docker instances.\nThe provided *-init services produce a container that runs only once for initializations: minio-init creates default buckets, ollama-init pulls default models, and kafka-init creates topics and initializes consumers for topics/groups.\nIn general, each service has its own network and volume, with init services sharing the same network as their corresponding parent services. All init services depend on their parent services being on a healthy status, so that we can run commands on them. For example, for ollama-init we set:\ndepends_on: ollama: condition: service_healthy Implementing health checks on the parent services was mostly constrained by the available tools within the specific base image used for each service (e.g., curl wasn\u0026rsquo;t necessarily available). Here are is a summary of the health check commands that we used:\nService Health Check minio curl -f http://localhost:9000/minio/health/live postgres pg_isready ollama ollama ls mlflow python -c \u0026quot;import urllib.request; urllib.request.urlopen('http://localhost:5000')\u0026quot; kafka /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:29092 --list The configuration for each service is heavily dependent on environment variables defined within a .env file on the root of the project, as we don\u0026rsquo;t deploy any secrets management service, like HashiCorp Vault.\nThe open-webui service also runs on the ollama network, and the ollama service requires GPU support, which was defined as follows:\ndeploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] Finally, portainer porta 9000 was remapped to 9080, so it wouldn\u0026rsquo;t collide with minio. We opted to change the port for Portainer rather than MinIO, since this is a secondary service used only for monitoring, and it doesn\u0026rsquo;t require any configs on datalab.\nPostgreSQL Databases # Under .ci/postgres.yml, we provide a workflow to provision a PostgreSQL database and credentials, which will be used by our applications from Layer 4 (see upcoming blog post and video).\nWe specify a spec.inputs header with db_user and db_name inputs:\nspec: inputs: db_user: description: \u0026#34;PostgreSQL username to create and grant privileges to\u0026#34; type: string db_name: description: \u0026#34;PostgreSQL database to create\u0026#34; type: string --- And two jobs, psql_create_user and psql_create_db:\npsql_create_user: stage: postgres image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 before_script: - apk add --no-cache openssl script: - # OMITTED rules: - if: \u0026#39;\u0026#34;$[[ inputs.db_user ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never psql_create_db: stage: postgres image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - # OMITTED needs: - psql_create_user rules: - if: \u0026#39;\u0026#34;$[[ inputs.db_name ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never Notice that psql_create_db needs psql_create_user, so it will always run after it.\nAlso notice that each job will only activate when the corresponding input is non-empty, which only happens when we manually launch the CI/CD pipeline, filling those values, via Build → Pipelines → New pipeline, as shown in the following screenshot:\nKafka Topics and Groups # Under .ci/kafka.yml, we provide a workflow to create a Kafka topics and initialize consumers for topics/groups. Similarly, this will be used by our applications from Layer 4.\nWe specify a spec.inputs header with topic and group inputs:\nspec: inputs: topic: description: \u0026#34;Kafka topic to create\u0026#34; type: string group: description: \u0026#34;Kafka group to create\u0026#34; type: string And two jobs, kafka_create_topic and kafka_init_consumer:\nkafka_create_topic: stage: kafka image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - # OMITTED rules: - if: \u0026#39;\u0026#34;$[[ inputs.topic ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never kafka_init_consumer: stage: kafka image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - # OMITTED needs: - kafka_create_topic rules: - if: \u0026#39;\u0026#34;$[[ inputs.topic ]]\u0026#34; != \u0026#34;\u0026#34; \u0026amp;\u0026amp; \u0026#34;$[[ inputs.group ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never Again, notice a dependency of kafka_create_topic by kafka_init_consumer, and notice that these jobs require their corresponding inputs to be non-empty to run (if: ...), otherwise they will never run (when: never).\nOllama Models # Under .ci/ollama.yml, we provide a very basic workflow to pull models. There is no workflow to otherwise manage models.\nHere is the complete content for the Ollama CI workflow:\nspec: inputs: pull: description: \u0026#34;Pull Ollama model\u0026#34; type: string --- ollama_pull: stage: ollama image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - \u0026#39;echo \u0026#34;Installing Ollama model: $[[ inputs.pull ]]\u0026#34;\u0026#39; - docker exec datalab-ollama-1 ollama pull $[[ inputs.pull ]] rules: - if: \u0026#39;\u0026#34;$[[ inputs.pull ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never The logic is similar to the one described for the PostgreSQL and Kafka CI workflows.\nFinal Remarks # There is a lot to criticize based on our design choices, as well as tech stack selection. Next time, we\u0026rsquo;ll show you how to deploy an application using this stack, but also do a retrospective on what could be improved. Overall, the same logic will apply, but there\u0026rsquo;s a few things we would definitely change, mainly regarding the decision to use GitLab versus Gitea. There are also a few added complexities that we would like to simplify, while at the same time revising our decision to purely rely on GitLab for secrets management rather than a more dedicated solution like HashiCorp Vault.\nSubscribe to @DataLabTechTV on YouTube, or keep following this blog so you don\u0026rsquo;t miss it! There\u0026rsquo;s RSS, if you\u0026rsquo;re old-school like me, or social media (Bluesky, Reddit, Discord), if you prefer it that way.\n","date":"14 October 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-services/","section":"","summary":"Summary # On part 4 of this series, you\u0026rsquo;ll learn how to automate data stack deployment using docker compose and GitLab, setting up CI/CD variables with Terraform, and using GPU passthrough to a Docker VM running on Proxmox.","title":"Data Lab Infra - Part 4: Core Services","type":"posts"},{"content":"","date":"14 October 2025","externalUrl":null,"permalink":"/tags/data-stack/","section":"Tags","summary":"","title":"Data-Stack","type":"tags"},{"content":"","date":"14 October 2025","externalUrl":null,"permalink":"/tags/open-webui/","section":"Tags","summary":"","title":"Open-Webui","type":"tags"},{"content":"","date":"14 October 2025","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"","date":"30 September 2025","externalUrl":null,"permalink":"/tags/cloud-init/","section":"Tags","summary":"","title":"Cloud-Init","type":"tags"},{"content":" Summary # On part 3 of this series, you\u0026rsquo;ll learn how to provision Proxmox VMs with Terraform, using QEMU to build on top of the Ubuntu\u0026rsquo;s official cloud image (qcow2).\nWe\u0026rsquo;ll deploy GitLab and Docker using cloud-init, configuring GitLab as a container registry, and deploying a shared Docker runner for CI/CD that executes on a separate VM.\nWe\u0026rsquo;ll also learn how to setup CI/CD variables and secrets, and how to write a simple CI/CD job, using a .gitlab-ci.yaml file, to print information about the runner instance, as well as a couple of variables that we set in via the web UI.\nAgain, keep in mind that these skills are loosely transferable to cloud platforms like AWS, GCP or Azure, with the advantage that it costs zero to setup Proxmox at home, if you\u0026rsquo;ve got some old hardware lying around.\n\u003e Resource Planning # Our Proxmox node has 16 CPU cores, 32 GiB RAM, and a 1 TiB hard drive, that we can allocate to CTs and VMs. For the foundation layer (L1), we allocated 2 CPU cores and 2048 MiB RAM to an LXC, but, for the platform layer (L2), we\u0026rsquo;ll need to plan a bit better, as L2 handles the most load, particularly on the docker-shared VM, as it runs all core services that are accessed by the other data stack components and applications.\nWe also want to keep 2 CPU cores available to the Proxmox hypervisor at all times. Proxmox also requires a minimum of 1 GiB RAM to run, but our node was using 2 GiB RAM, so this is our target free RAM.\nBelow, we plan global memory allocation based on the maximum active memory for a single VM, while assuming other VMs remain at the minimum usage set by floating memory. Such an assumption is based on the fact that this infrastructure will be used by a single person. You should do a similar exercise for your hardware and use case, but the overall logic remains useful.\nNotice that we might still observe a heavy load on gitlab, docker-gitlab, and docker-shared, at the same time—e.g., gitlab launches a docker-gitlab runner to update the docker compose stack on docker-shared. Regardless, the worst case scenario based on a single VM is still a good average to target.\nBelow, you find the resource allocation table for all the CTs and VMs in our Proxmox node. You\u0026rsquo;ll find the ID and name for the VM or CT, along with the allocated number of CPU cores, disk space (GiB), dedicated memory (MiB), and floating memory (MiB). We also include a \u0026ldquo;Free on Max.\u0026rdquo; column with the free or unallocated memory when the given VM is using all of its dedicated memory. This is calculated by subtracting the total floating memory (22,528 MiB) and the difference between the VM\u0026rsquo;s dedicated and floating memory (e.g., 20,480 MiB and 12,288 MiB for VM 203) from the total available memory (32,768 MiB). As such, for VM 203, the free memory on maximum usage is calculated as $32768 - 22528 - (20480 - 12288) = 2048$.\nID VM/CT Cores Disk Dedicated Mem. Floating Mem. Free on Max. 101 minio 2 200 2048 2048 10240 201 gitlab 1 100 6144 2048 6144 202 docker-gitlab 1 100 6144 2048 6144 203 docker-shared 8 200 20480 12288 2048 204 docker-apps 2 100 12288 4096 2048 ALL TOTALS 14 700 47104 22528 -14336 As we can see, worst case scenario, assuming maximum memory usage only happens for a single VM at a time, we get 2,048 MiB RAM free at all times.\nIf we look at the totals, we find only 14 CPU cores were allocated, leaving 2 cores for Proxmox, as required. We can also see that, at maximum usage for all CTs and VMs, our system would require 14,336 MiB of additional RAM.\nThis is an estimate, but, with real-world usage statistics, we can better tune VM configurations simply by editing our Terraform variables for cpu.cores and memory.*. And, if there aren\u0026rsquo;t enough resources, this is a good reason to go look for a mini PC to expand our home lab! 😉\nBackup Strategy? # During foundation layer deployment, we had setup a separate volume for /data. In most cloud platforms, we can easily detach a data volume while recreating the root volume, or even create snapshots for it that we can later restore. On the other hand, Proxmox only provides snapshots for the whole VM, which is not practical to manage as a backup solution. As such, we simplified our MinIO LXC to use a single root disk, and we do the same for the VMs in the platform layer.\nBackups will be added later on and they will be application-specific. The overall strategy will be based on mounting an external volume from a NAS under /backups for each VM, and scheduling a backup process based on a systemd timer and service, which will write to that directory.\nFor our current use-case, backups are not particularly concerning, as we can always re-ingest our lakehouse datasets, or recreate our GitLab repos, as these will only be used for CI/CD, which obvious also require that Docker images are re-registered, and variables and secrets are re-added. While this might justify setting up a proper backup strategy, failure wouldn\u0026rsquo;t be catastrophic. This is perfectly fine for a home lab setting, where we\u0026rsquo;re constantly tearing down and re-deploying our infrastructure.\nDocker VMs # We\u0026rsquo;ll provision three Docker VMs using Terraform, the bpg/proxmox provider, and cloud-init:\ndocker-gitlab, where our GitLab Runner will run CI/CD jobs; docker-shared, to deploy the core services for our data stack (PostgreSQL, Apache Kafka, etc.); docker-apps, where project-specific services will live. Cloud-Init # Wait, so did we drop Packer? Yes, we did. We ended up going with a simpler cloud-init config instead, since there really isn\u0026rsquo;t much to deploy on each VM, or too much to gain from building a single image to deploy only three Docker VMs. We might still do Packer in the future, for the educational value, but, for the home lab, we want to keep it as minimalistic as possible.\nHere\u0026rsquo;s the cloud-init config that we used to setup our Docker VMs. This was directly extracted from our Terraform config, which is currently available at the dev branch of the datalab repo, so you\u0026rsquo;ll find ${...} blocks which represent variable or resource output replacements.\n#cloud-config hostname: \u0026#34;${local.docker[count.index].name}\u0026#34; password: \u0026#34;${random_password.docker_vm[count.index].result}\u0026#34; chpasswd: expire: false ssh_pwauth: true apt: sources: docker: source: \u0026#34;deb [arch=amd64 signed-by=/etc/apt/trusted.gpg.d/docker.gpg] https://download.docker.com/linux/ubuntu noble stable\u0026#34; key: | ${indent(8, chomp(data.http.docker_gpg.response_body))} package_update: true package_upgrade: true packages: - qemu-guest-agent - docker-ce write_files: - path: /etc/systemd/system/docker.service.d/override.conf owner: \u0026#39;root:root\u0026#39; permissions: \u0026#39;0600\u0026#39; content: | [Service] ExecStart= ExecStart=/usr/bin/dockerd - path: /etc/docker/daemon.json owner: \u0026#39;root:root\u0026#39; permissions: \u0026#39;0600\u0026#39; content: | { \u0026#34;hosts\u0026#34;: [\u0026#34;fd://\u0026#34;, \u0026#34;tcp://0.0.0.0:2375\u0026#34;], \u0026#34;containerd\u0026#34;: \u0026#34;/run/containerd/containerd.sock\u0026#34; } runcmd: - systemctl enable --now qemu-guest-agent - netplan apply - usermod -aG docker ubuntu - reboot We use the apt block to add the official repo for Docker, so that we can install docker-ce with the latest version of Docker. Then, we override the config for docker.service, which essentially means removing the arguments from dockerd so that /etc/docker/daemon.json gets used instead. We then reproduce the original CLI config and add tcp://0.0.0.0:2375 to the listening hosts for Docker. This will let us access Docker externally, as described in the following section.\nClient Access # On your local machine, create a context for each Docker VM:\ndocker context create docker-gitlab --docker \u0026#34;host=tcp://docker-gitlab:2375\u0026#34; docker context create docker-shared --docker \u0026#34;host=tcp://docker-shared:2375\u0026#34; docker context create docker-apps --docker \u0026#34;host=tcp://docker-apps:2375\u0026#34; You can then switch to any context and run Docker commands as usual:\ndocker context use docker-gitlab docker ps Often, custom shell prompts, like Starship, will display your current Docker context, when it\u0026rsquo;s something other than the default ones. I definitely recommend Starship for this, as it supports any shell, including Bash, Fish, or even PowerShell, and it will display your Docker context by default.\nGitLab VM # We\u0026rsquo;ll provision a GitLab instance, which will power multiple features of our home lab, including the container registry for custom docker images—be it our own project-specific services or extended installations for core services—and also GitOps, using CI/CD to run docker compose deployment jobs, along with variables to store configurations and secrets.\nMinIO Bucket # As a requirement to setup the container registry component, we added the gitlab bucket to our foundation layer MinIO instance, both to the provisioning config and manually via mc (to avoid redeploying):\ncd infra/foundation mc alias set lab http://minio:9000 admin \\ $(terraform output -raw minio_admin_password) mc mb lab/gitlab This is used on the cloud-init config shown in the following section.\nCloud-Init # Here\u0026rsquo;s the cloud-init config that we used to setup our GitLab VM:\n#cloud-config hostname: ${local.gitlab.name} password: \u0026#34;${random_password.gitlab_vm.result}\u0026#34; chpasswd: expire: false ssh_pwauth: true package_update: true package_upgrade: true packages: - qemu-guest-agent - curl write_files: - path: /etc/gitlab/gitlab.rb owner: \u0026#39;root:root\u0026#39; permissions: \u0026#39;0600\u0026#39; content: | external_url \u0026#39;http://${local.gitlab.name}\u0026#39; gitlab_rails[\u0026#39;initial_root_password\u0026#39;] = \u0026#39;${random_password.gitlab_root.result}\u0026#39; gitlab_rails[\u0026#39;registry_enabled\u0026#39;] = true registry_external_url \u0026#39;http://${local.gitlab.name}:5050\u0026#39; registry[\u0026#39;database\u0026#39;] = { \u0026#39;enabled\u0026#39; =\u0026gt; true, } registry[\u0026#39;storage\u0026#39;] = { \u0026#39;s3_v2\u0026#39; =\u0026gt; { \u0026#39;regionendpoint\u0026#39; =\u0026gt; \u0026#39;${var.s3_endpoint}\u0026#39;, \u0026#39;region\u0026#39; =\u0026gt; \u0026#39;${var.s3_region}\u0026#39;, \u0026#39;accesskey\u0026#39; =\u0026gt; \u0026#39;${var.s3_access_key}\u0026#39;, \u0026#39;secretkey\u0026#39; =\u0026gt; \u0026#39;${var.s3_secret_key}\u0026#39;, \u0026#39;pathstyle\u0026#39; =\u0026gt; ${var.s3_path_style}, \u0026#39;bucket\u0026#39; =\u0026gt; \u0026#39;${var.gitlab_s3_registry_bucket}\u0026#39;, } } runcmd: - systemctl enable --now qemu-guest-agent - netplan apply - curl \u0026#34;https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh\u0026#34; | sudo bash - apt install -y gitlab-ce - gitlab-ctl reconfigure - | gitlab-rails console -e production \u0026lt;\u0026lt;EOT s = ApplicationSetting.current s.update!( signup_enabled: false, version_check_enabled: false, usage_ping_enabled: false, usage_ping_generation_enabled: false, whats_new_variant: \u0026#39;disabled\u0026#39; ) EOT - curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash - apt install -y gitlab-runner - | gitlab-runner register --non-interactive \\ --url \u0026#34;http://${local.gitlab.name}/\u0026#34; \\ --registration-token \u0026#34;$(gitlab-rails runner \u0026#39;puts ApplicationSetting.current.runners_registration_token\u0026#39;)\u0026#34; \\ --executor \u0026#34;docker\u0026#34; \\ --docker-image \u0026#34;ubuntu:latest\u0026#34; \\ --description \u0026#34;ubuntu-latest-runner\u0026#34; \\ --tag-list \u0026#34;docker,remote,ubuntu\u0026#34; \\ --run-untagged=\u0026#34;true\u0026#34; \\ --docker-host \u0026#34;tcp://${local.docker[0].name}:2375\u0026#34; - reboot Let\u0026rsquo;s break it down into. We begin by setting a few basic properties for the VM, including its hostname and password (setting to not expire, thus avoiding having to change it on the first login). We also enable password authentication for SSH, so that we can login with the default ubuntu user.\n#cloud-config hostname: ${local.gitlab.name} password: \u0026#34;${random_password.gitlab_vm.result}\u0026#34; chpasswd: expire: false ssh_pwauth: true Then, we update the system packages and install the guest agent to help Proxmox monitor the VM and handle memory ballooning, and we install curl, which will be required during GitLab installation.\npackage_update: true package_upgrade: true packages: - qemu-guest-agent - curl Next, we write the gitlab.rb file, which is used to configure GitLab when running gitlab-ctl reconfigure.\nwrite_files: - path: /etc/gitlab/gitlab.rb owner: \u0026#39;root:root\u0026#39; permissions: \u0026#39;0600\u0026#39; content: | ... You\u0026rsquo;ll find the content value below. We set the external_url to the host where GitLab will run, as well as the initial root account password. We then enable the container registry, setting it up to use the existing database, as well as our existing MinIO instance, with the newly created bucket, to store the image layers that will be pushed to the container. As you can see, most of these settings are directly managed via Terraform variables.\nexternal_url \u0026#39;http://${local.gitlab.name}\u0026#39; gitlab_rails[\u0026#39;initial_root_password\u0026#39;] = \u0026#39;${random_password.gitlab_root.result}\u0026#39; gitlab_rails[\u0026#39;registry_enabled\u0026#39;] = true registry_external_url \u0026#39;http://${local.gitlab.name}:5050\u0026#39; registry[\u0026#39;database\u0026#39;] = { \u0026#39;enabled\u0026#39; =\u0026gt; true, } registry[\u0026#39;storage\u0026#39;] = { \u0026#39;s3_v2\u0026#39; =\u0026gt; { \u0026#39;regionendpoint\u0026#39; =\u0026gt; \u0026#39;${var.s3_endpoint}\u0026#39;, \u0026#39;region\u0026#39; =\u0026gt; \u0026#39;${var.s3_region}\u0026#39;, \u0026#39;accesskey\u0026#39; =\u0026gt; \u0026#39;${var.s3_access_key}\u0026#39;, \u0026#39;secretkey\u0026#39; =\u0026gt; \u0026#39;${var.s3_secret_key}\u0026#39;, \u0026#39;pathstyle\u0026#39; =\u0026gt; ${var.s3_path_style}, \u0026#39;bucket\u0026#39; =\u0026gt; \u0026#39;${var.gitlab_s3_registry_bucket}\u0026#39;, } } Finally, under the runcmd block, which is an array of commands, we\u0026rsquo;ll essentially run the following script, which we reformat and comment:\n# This enables and starts the guest agent. systemctl enable --now qemu-guest-agent # This ensures that the hostname is broadcast to DHCP, otherwise # we\u0026#39;d need to sign in once using the IP address. netplan apply # Here we install GitLab, but this doesn\u0026#39;t start it. curl \u0026#34;https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh\u0026#34; | sudo bash apt install -y gitlab-ce # This will configure and start GitLab using the settings from # /etc/gitlab/gitlab.rb. gitlab-ctl reconfigure gitlab-rails console -e production \u0026lt;\u0026lt;EOT s = ApplicationSetting.current s.update!( signup_enabled: false, version_check_enabled: false, usage_ping_enabled: false, usage_ping_generation_enabled: false, whats_new_variant: \u0026#39;disabled\u0026#39; ) EOT # We also install the GitLab Runner locally. curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash apt install -y gitlab-runner # And we set it up as a shared runner that executes using an Ubuntu # image on a separate Docker VM (docker-gitlab). gitlab-runner register --non-interactive \\ --url \u0026#34;http://${local.gitlab.name}/\u0026#34; \\ --registration-token \u0026#34;$(gitlab-rails runner \u0026#39;puts ApplicationSetting.current.runners_registration_token\u0026#39;)\u0026#34; \\ --executor \u0026#34;docker\u0026#34; \\ --docker-image \u0026#34;ubuntu:latest\u0026#34; \\ --description \u0026#34;ubuntu-latest-runner\u0026#34; \\ --tag-list \u0026#34;docker,remote,ubuntu\u0026#34; \\ --run-untagged=\u0026#34;true\u0026#34; \\ --docker-host \u0026#34;tcp://${local.docker[0].name}:2375\u0026#34; # We reboot, as it\u0026#39;s likely Ubuntu will ask you to anyway, on first # login. reboot Testing # Repository Management # In order to test a basic git repo workflow, we do the following:\nCreate a new user with the root account, e.g., datalabtechtv. Edit the user to be able to set a default password. Sign in to the new user and change your password Create a sandbox repo. Generate a gitlab SSH key on the client machine. Add the SSH key to the new user\u0026rsquo;s settings. Clone the sandbox repo on the client machine. Container Registry # For testing the container registry, on the cloned sandbox repo, we create the following Dockerfile:\nFROM ubuntu:latest CMD [\u0026#34;echo\u0026#34;, \u0026#34;Hello GitLab Container Registry!\u0026#34;] We can optionally commit and push it:\ngit add Dockerfile git config user.name \u0026#34;Data Lab Tech TV\u0026#34; git config user.email \u0026#34;mail@datalabtechtv.com\u0026#34; git commit Dockerfile -m \u0026#34;feat: add a testing Dockerfile\u0026#34; git push Because we\u0026rsquo;re not using SSL for our GitLab instance, we\u0026rsquo;ll need to add the following config to /etc/docker/daemon.json on our client machine (also available via Docker Desktop, under Settings → Docker Engine):\n{ \u0026#34;insecure-registries\u0026#34;: [\u0026#34;gitlab:5050\u0026#34;] } We can optionally create a personal access token (PAT) with read_registry and write_registry permissions for our user account, under Settings → Access tokens to use as a password, but we can also login using our regular user and password, which is what we do. Either way, the command is the same:\ndocker login gitlab:5050 We can then build, tag, and push an image, making sure our tag includes the user account (e.g., datalabtechtv) and the project/repo name (e.g., sandbox), followed by our image name and version (e.g., test:latest):\ndocker build -t test:latest . docker tag test:latest gitlab:5050/datalabtechtv/sandbox/test:latest docker push gitlab:5050/datalabtechtv/sandbox/test:latest Once this is done, the image should be listed on GitLab\u0026rsquo;s UI under Deploy → Container registry for the sandbox project. I recommend that you pin Container registry, as we\u0026rsquo;ll be using it often on our home lab in the future.\nAlso, check the gitlab bucket on MinIO to make sure that the image blobs are being written to the proper storage.\nCI/CD Workflows # Again, on the sandbox repo, create the following .gitlab-ci.yml file:\nrunner_info: script: - lscpu - free -h - df -h / - echo \u0026#34;USER:\u0026#34; $SOME_USER - echo \u0026#34;PASS:\u0026#34; $SOME_PASS Go into GitLab and, under the sandbox project, go to Settings → CI/CD → Variables → CI/CD Variables → Add variable, and add the SOME_USER and SOME_PASS variables—set the first one to \u0026ldquo;Visible\u0026rdquo; and the second one to \u0026ldquo;Masked and hidden\u0026rdquo;.\nThen you can add, commit and push:\ngit add .gitlab-ci.yml git commit .gitlab-ci.yml -m \u0026#34;ci: basic task to test runner\u0026#34; git push The job will trigger immediately, and you\u0026rsquo;ll be able to find its status and output under Build → Jobs for the sandbox project. I also recommend that you pin this menu entry. You\u0026rsquo;ll notice that the value of SOME_USER was correctly printed, while the value of SOME_PASS was be printed as [MASKED].\nMoving forward, we\u0026rsquo;ll take advantage of the CI/CD features to setup a GitOps workflow to manage the docker compose stack for our data lab. Stay tuned for part 4 of this series!\nFinal Remarks # Despite a few bumps on the road, and questioning some decisions, we got it working smoothly in the end. Always expect the unexpected, a bit of a bumpy road, when working in operations. And be ready to make changes. Prioritize the robustness of your infrastructure rather than the successful execution of your original plan!\nOn Configuration Management\u0026hellip; # To me, it remains unclear which configuration solution is preferrable, since, to be honest, I dislike them all.\nPrebuilt images with Packer would have been ideal in a real-world scenario, if we wanted to eventually move out of qcow2 images on Proxmox VMs and into AMIs (Amazon Machine Images) on AWS EC2, but frankly this is overkill for a home lab.\nI questioned my decision not to use Ansible, which might still be the best overall solution, but it still felt slightly overkill for this. So, I ended up going with cloud-init, as it\u0026rsquo;s easily available on Proxmox, providing a few of the features that Ansible provides, but having a much simpler syntax. Although, of course, Ansible is more powerful and provides idempotency, while cloud-init is enough for most use cases, but runs only once per deployment.\nOn GitLab\u0026hellip; # After working on GitLab deployment for a while, I questioned the decision to go with GitLab and asked myself if Gitea would have been a better and more lightweight option, with it being written in Go rather than Ruby, like GitLab.\nWhile the apt package for GitLab installs flawlessly, and all the tooling is robust, there\u0026rsquo;s a lot to improve as well. For example, most configs can be done via /etc/gitlab/gitlab.rb, which is not the best file format for configurations. Why not toml or yaml instead? And even disregarding that, a few configurations cannot be done via gitlab.rb, requiring gitlab-rails console instead. Fortunately it can read from the standard input, but it\u0026rsquo;s also extremely slow—I even thought there was some error as it was starting, because it took quite a while without producing any message whatsoever.\nRegardless, I cannot complain much. Everything worked pretty much at first try, after following the documentation and asking ChatGPT a few things. Also, once deployed, performance is quite satisfactory, with GitLab being quite smooth to use.\n","date":"30 September 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-platform/","section":"","summary":"Summary # On part 3 of this series, you\u0026rsquo;ll learn how to provision Proxmox VMs with Terraform, using QEMU to build on top of the Ubuntu\u0026rsquo;s official cloud image (qcow2).","title":"Data Lab Infra - Part 3: Platform Setup with Terraform","type":"posts"},{"content":"","date":"30 September 2025","externalUrl":null,"permalink":"/tags/platform/","section":"Tags","summary":"","title":"Platform","type":"tags"},{"content":"","date":"30 September 2025","externalUrl":null,"permalink":"/tags/proxmox/","section":"Tags","summary":"","title":"Proxmox","type":"tags"},{"content":" Summary # On part 2 of this series, you\u0026rsquo;ll learn the basics of Terraform, provisioning a MinIO server as an LXC (Linux Container) running on Proxmox.\nThen, you\u0026rsquo;ll learn how to use this foundation infrastructure, consisting of the MinIO S3-compatible object store, to track the state of a separate Terraform project in a way that will let you update your infrastructure from any location with access to your Proxmox instance.\nThese skills are loosely transferable to cloud platforms like AWS, GCP or Azure, with the advantage that it costs zero to setup Proxmox at home, if you\u0026rsquo;ve got some old hardware lying around.\n\u003e Proxmox # Terraform Access # After installing Proxmox, we\u0026rsquo;ll create a terraform user. This needs to be a PAM user, since Terraform\u0026rsquo;s bpg/proxmox provider will need shell access to be able to run a few actions that are not supported by the API, like downloading Debian or Ubuntu images. We\u0026rsquo;ll disable password login for this user, because we\u0026rsquo;ll exclusively rely on SSH keys for shell access.\nWe create the system user:\nadduser --disabled-password terraform pveum user add terraform@pam And an associated role with the permissions required by the provider:\npveum role add Terraform -privs \"Realm.AllocateUser, VM.PowerMgmt, VM.GuestAgent.Unrestricted, Sys.Console, Sys.Audit, Sys.AccessNetwork, VM.Config.Cloudinit, VM.Replicate, Pool.Allocate, SDN.Audit, Realm.Allocate, SDN.Use, Mapping.Modify, VM.Config.Memory, VM.GuestAgent.FileSystemMgmt, VM.Allocate, SDN.Allocate, VM.Console, VM.Clone, VM.Backup, Datastore.AllocateTemplate, VM.Snapshot, VM.Config.Network, Sys.Incoming, Sys.Modify, VM.Snapshot.Rollback, VM.Config.Disk, Datastore.Allocate, VM.Config.CPU, VM.Config.CDROM, Group.Allocate, Datastore.Audit, VM.Migrate, VM.GuestAgent.FileWrite, Mapping.Use, Datastore.AllocateSpace, Sys.Syslog, VM.Config.Options, Pool.Audit, User.Modify, VM.Config.HWType, VM.Audit, Sys.PowerMgmt, VM.GuestAgent.Audit, Mapping.Audit, VM.GuestAgent.FileRead, Permissions.Modify\" pveum acl modify / -user terraform@pam -role Terraform We\u0026rsquo;ll need an API key as well, which we can create with:\npveum user token add terraform@pam datalabtech --privsep 0 We disable Privilege Separation to make sure that we use the Terraform role we defined previously, but API keys can have their own, more restrict permissions. Make sure to save the token ID and value to use when configuring Terraform.\nNow, on your host machines—those that will be able to run Terraform—create an SSH key to be added to the terraform account:\nssh-keygen -t ed25519 -C proxmox -f ~/.ssh/proxmox Currently, ed25519 is the default key type anyway, but making it explicit ensures the command will endure the test of time. We also use -C so that information about our user and host is not leaked in the key. Finally, the naming scheme for the key file is simply the name of the target service—if we had multiple accounts for a service (e.g., multiple GitHub accounts), then we\u0026rsquo;d use something like github-account1, github-account2, etc., but one thing we should avoid is sharing keys, even between the same user on different machines.\nCopy the public key from the host machine:\ncat ~/.ssh/proxmox.pub | pbcopy And then just drop it on Proxmox under /home/terraform/.ssh/authorized_keys:\nmkdir ~terraform/.ssh vi ~terraform/.ssh/authorized_keys # Paste public key and save From your host machine, you can then test if access was correctly setup (e.g., using proxmox as your hypervisor host):\nssh -i ~/.ssh/proxmox terraform@proxmox GPU Support # It\u0026rsquo;s possible to passthrough your GPU NVIDIA card to VMs or CTs. This is how we setup the hypervisor machine drivers.\nStep 1: Blacklist nouveau # We\u0026rsquo;ll want to use the proprietary drivers, so we need to disable the open source nouveau drivers.\nvi /etc/modprobe.d/blacklist.conf Add the following line:\nblacklist nouveau And then run:\nupdate-initramfs -u reboot Step 2: Install NVIDIA drivers # Under Updates ▶ Repositories, make sure that the No-Subscription and Ceph No-Subscription repositories are configured and that the corresponding Enterprise versions are disable.\nThen run:\napt update \u0026amp;\u0026amp; apt upgrade -y apt install pve-headers build-essential -y Download the official NVIDIA drivers for Linux 64-bit, assuming that\u0026rsquo;s your arch. I usually just search for my card and copy the link to the .run script, so I can download directly to Proxmox using wget. For example:\nwget https://us.download.nvidia.com/XFree86/Linux-x86_64/\u0026lt;version\u0026gt;/NVIDIA-Linux-x86_64-580.82.09.run sh NVIDIA-Linux-x86_64-580.82.09.run You can accept 32-bit libraries, if the option is provided (but I didn\u0026rsquo;t). Don\u0026rsquo;t run the nvidia-xconfig utility, as Proxmox is headless, and there is no X11 installation. You can also safely ignore any warnings about inferring or not finding X11 libraries.\nStep 3: Load drivers on boot # In order for drivers to be loaded on boot, you need to edit your modules.conf:\nvi /etc/modules-load.d/modules.conf Add:\nnvidia nvidia-modeset nvidia_uvm And run:\nupdate-initramfs -u Also edit:\nvi /etc/udev/rules.d/70-nvidia.rules And add:\nKERNEL==\u0026#34;nvidia\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-smi -L \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia*\u0026#39;\u0026#34; KERNEL==\u0026#34;nvidia_modeset\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-modprobe -c0 -m \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia-modeset*\u0026#39;\u0026#34; KERNEL==\u0026#34;nvidia_uvm\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-modprobe -c0 -u \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia-uvm*\u0026#39;\u0026#34; Then you can reboot, and run nvidia-smi to ensure the drivers are operational. That\u0026rsquo;s it. Everything else will be done at the VM or CT level, and we\u0026rsquo;ll discuss it at a later time.\nTerraform # Installation # The best way to install Terraform is using tfswitch. Follow the installation instructions and, once that is done, just run tfswitch and select your version of Terraform.\nIf you have other initialized Terraform projects, running tfswitch from the root directory will automatically install the version of Terraform required for that project. You\u0026rsquo;ll need to re-run this for Terraform projects that require different versions.\nThe version is usually setup under versions.hcl as follows:\nterraform { required_version = \u0026#34;~\u0026gt; 1.13.2\u0026#34; } Accessing Secrets # By default, running terraform output will redact sensitive variables (i.e., secrets), but it\u0026rsquo;s possible access the value with the -raw argument, only for a specific output. We suggest that you never print the secret plainly to the console, but instead pipe it to a copy command, like pbcopy (pasteboard copy) on Mac, or xclip -selection clipboard on Linux (I personally alias this to pbcopy on Linux as a convention).\nterraform -chdir=infra/foundation \\ output -raw minio_admin_password | pbcopy S3 State Storage # Since variables can only be accessed after terraform init, we cannot use regular variables to configure the backend for state storage. Instead, the documentation suggests that we use a state.config file for this.\nSo, we begin with an empty backend config:\nterraform { backend \u0026#34;s3\u0026#34; {} } And we produce a state.config file that looks like this:\nbucket = \u0026#34;terraform\u0026#34; key = \u0026#34;state/platform/terraform.tfstate\u0026#34; endpoints = { s3 = \u0026#34;http://minio:9000\u0026#34; } region = \u0026#34;eu-west-1\u0026#34; access_key = \u0026#34;admin\u0026#34; access_key = \u0026#34;XXXXXXXXXXXXXXXXXXXX\u0026#34; skip_credentials_validation = true skip_metadata_api_check = true skip_requesting_account_id = true use_path_style = true The previous file is already provided to you under state.config.example. You can copy it to state.config and replace the access key:\ncp infra/platform/state.config.example infra/platform/state.config vim infra/platform/state.config # Replace secret_key with your MinIO admin password and save You can then init your stored state Terraform project:\nterraform -chdir=infra/platform init -backend-config=state.config Deployment # After having run terraform init for each project, you can then deploy the infrastructure as shown below. Remember that the platform layer is built on top of the foundation layer, so this it is a requirement that is deployed first.\nterraform -chdir=infra/foundation apply -auto-approve terraform -chdir=infra/platform apply -auto-approve Now, if you clone the datalab repo to a different location and run the init command for the platform project, using the proper state.config, you\u0026rsquo;ll be able to access your latest Terraform state. This will let you work from anywhere, as long as you have access to Proxmox—I recommend setting up a VPN to your homelab using WireGuard, if you\u0026rsquo;re outside of your local network.\nJustfile Tasks # For your convenience, we provide several top-level just tasks on datalab that you can use, if you forget the commands:\ninfra-config-check – check for terraform and the required configs for all infra projects. infra-init – run the proper terraform init commands for each project (must be manually run before infra-deploy). infra-deploy – deploy each layer of the architecture in sequence (foundation and platform are the only ones supported at this time). infra-show-credentials – print all credentials, for each layer, in plain text. Any task in this video series will begin with the prefix infra-, so you can keep a look for these in the upcoming videos.\n","date":"23 September 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-foundation/","section":"","summary":"Summary # On part 2 of this series, you\u0026rsquo;ll learn the basics of Terraform, provisioning a MinIO server as an LXC (Linux Container) running on Proxmox.","title":"Data Lab Infra - Part 2: Bootstrapping with Terraform","type":"posts"},{"content":"","date":"23 September 2025","externalUrl":null,"permalink":"/tags/foundation/","section":"Tags","summary":"","title":"Foundation","type":"tags"},{"content":"","date":"23 September 2025","externalUrl":null,"permalink":"/tags/minio/","section":"Tags","summary":"","title":"Minio","type":"tags"},{"content":"","date":"23 September 2025","externalUrl":null,"permalink":"/tags/secrets-management/","section":"Tags","summary":"","title":"Secrets-Management","type":"tags"},{"content":" Summary # In this video, we\u0026rsquo;ll learn how to design a modern data stack, built for home labs, freelancing data experts, or general on-premise needs.\n\u003e Architecture Overview # We split our infrastructure 4 layers due to the following reasons:\nLayer 1 provides a foundation based on Proxmox and a single LXC running MinIO (or any other S3 object store). We add nothing more to this layer, because we will rely on S3 to track the Terraform state in layer 2, so that we can update or add to our VMs or CTs from any location with access to Proxmox. Layer 2 provides a platform for all the services that we want to run, provisioning three Docker VMs for different needs, and a VM with GitLab, which we will use mostly for configuration management (variables and secrets), as a Docker registry to track custom images we might need to build, and for CI/CD to automate docker compose stack deployment. Layer 3 is concerned with core or shared services, like PostgreSQL or MLflow, which we deploy to the corresponding Docker instance. Layer 4 is concerned with application services, like REST APIs or documentation, which we deploy to the corresponding Docker instance. Here is a diagram for the described 4-layer architecture, with the core services we aim to deploy:\nEach layer from L1 to L4 will live in the datalab repo under the infra/ directory, each on its own subfolder:\nfoundation/ platform/ services/ applications/ We\u0026rsquo;ll have a mirror of the datalab repo on GitLab, exclusively meant for deploying into production—and it conveniently works as a backup as well. The deployment workflow is run whenever a change in docker-compose.yml files are detected in the main branch, particularly under the services/ and applications/ directories.\ncd datalab/infra/ git switch -c dev # Do work git add \u0026lt;work\u0026gt; git commit -m \u0026#34;work\u0026#34; git push # Once ready for production... git switch main git merge --no-ff dev git push # And then deploy your infra. git remote add prod git@gitlab.lan:DataLabTechTV/datalab.git git push -u prod main L4, the application layer, can also be implemented under project-specific repos, when unrelated to datalab. If service initializations need to be run for a specific app (e.g., issue credentials, or create a database), then a GitLab job will be exposed on the datalab repo that will be callable from any application repo to provision the required resources without exposing any admin credentials.\nLayer 1: Foundation - Bootstrapping # Proxmox LXC: MinIO ID: 101 Name: minio Function: Terraform state storage General-purpose object store Setup using bpg/proxmox provider, which provides more granular configuration options than the Telmate/proxmox provider. Access can be configured either using the root password (not recommended), an API key (limited to API based requests—won\u0026rsquo;t fully cover all available features for the provider), or a private key (preferred option for more control).\nLayer 2: Platform - Core Infrastructure # VM: GitLab ID: 201 Name: gitlab Function: Docker registry – custom service images, project-specific microservices, etc. CI/CD – production configs as variables/secrets, deploy docker compose stacks VM: Docker ID: 202 Name: docker-gitlab Function: GitLab runners VM: Docker ID: 203 Name: docker-shared Function: core services (e.g., PostgreSQL) VM: Docker (project-specific services) ID: 204 Name: docker-apps Function: project-specific services GitLab will be installed and configured using Packer, producing a QCOW2 image to be deployed to Proxmox, under a QEMU VM. This will include the GitLab runners configuration pointing to docker-gitlab.\nA Portainer instance will run under docker-shared, letting us monitor and manage the Docker instances in the three VMs—docker-gitlab, docker-shared, and docker-apps. Deployments will be based on one or multiple docker compose stacks. Docker compose will be triggered via CI/CD on a push to main, using variables and secrets stored in GitLab, for a production deployment. For testing, during development, a .env and local Docker instance can be used instead. This also leaves room, when local resources are scarce, for a setup CI/CD where a push to a dev branch will deploy to a separate staging VM.\nLayer 3: Services - Core/Shared Services # PostgreSQL Description: Shared instance for the whole home lab—even beyond the data lab—also much easier to backup. DuckLake Description: Data Lakehouse Dependencies: PostgreSQL (catalog) MinIO (storage) Apache Kafka Description: Event log MLflow Description: ML model tracking and registry Ollama Description: LLM server Open WebUI Description: ChatGPT like UI for Ollama Dependencies: Ollama A single PostgreSQL instance will be shared among all services. On the first deployment, a root password will be set and stored on the datalab GitLab repo. Application-level deployments will rely on a GitLab\u0026rsquo;s CI/CD task for initializations (e.g., credentials, a database) that will be triggered from any application repo but always run on the datalab GitLab repo, thus keeping admin credentials isolated.\nLayer 4: Applications - Project-Specific Services # Microservices REST APIs Documentation Microservices usually have their own isolated storage layer, but in a lab setting, where resources are scarce, we won\u0026rsquo;t fully respect this constraint. An example of application-specific containers, already in our present context, would be a classifier REST API endpoint. It is at this layer that we will do such deployments.\nUnderstanding Decisions # DevOps # We considered multiple alternative tools\nTerraform vs OpenTofu\nOpenTofu—it exists because Terraform\u0026rsquo;s license went from MPL 2.0 to BSL 1.1 on August 2023; the community didn\u0026rsquo;t like this restriction and OpenTofu was born. Terraform—the BSL license is essentially designed for non-competing; if this doesn\u0026rsquo;t affect you, there\u0026rsquo;s no reason not to go with Terraform. Docker vs Podman\nPodman—looks interesting, but requires you to think about podman machine, or manually setup podman.socket to integrate with other tools, like Portainer. Docker—no reason not to use Docker, but there are a few to not use Podman, so we went with good old Docker. Secrets Management\n.env—non-committed, a .env.example is provided instead. direnv—similar to .env, but uses a .envrc that is automatically loaded in the shell when you switch to a path inside the root directory. SOPS—started at Mozilla, it\u0026rsquo;s similar to Ansible Vault, but encrypts only the values in structured formats like JSON or YAML; can be committed to a git repo and even diffed. HashiCorp Vault—beautiful web app; well-loved by the community; good integration and support all around; there is even a browser extension for it, called VaultPass; but using it represents using yet another service, and adding to the complexity of our infrastructure. GitLab—not exactly made for secrets management, but it does provide variables and a way to mask and hide them (secrets); as a bonus it provides CI/CD for convenient deployments; this is the overall best choice, and a general-purpose good thing to have. Docker Registry\nDocker Registry UI—lightweight UI for the registry. Harbor—full fledged artifact registry, with policies and role-based access control; supports multiple registries and replication; overkill for our use-case; perhaps a better option when deploying to Kubernetes. GitLab—already using it for secrets management, so it\u0026rsquo;s perfect as a Docker Registry as well; a no-brainer. Configuration Management\nAnsible—have used it before and know that works well; its approach can be overly complicated, with simple tasks requiring multiple configuration files. Packer—prioritizes building pre-configured, ready-to-use images, with the added bonus that these can easily be deployed to a cloud platform with very little effort; great skill to have; simpler solution based on HCL, the same language used in Terraform. Data Stack # Portainer\nTo use or not to use? We wanted a minimal stack, but\u0026hellip; This is useful for quick testing or to easily monitor containers, volumes, and images. PostgreSQL\nGreat as a general-purpose relational database, and not only that. Required by GitLab, although we\u0026rsquo;ll probably keep the instances separate (lives in a higher level layers). Can be used as a DuckLake catalog, or to support most apps. DuckLake\nSimplest available solution. Perfect for any scale. Separates catalog, storage, and compute, and compute will be local for us. Catalog can be setup with PostgreSQL, which we already run. Storage can be setup with an S3 object store, and we\u0026rsquo;ve got MinIO. Compute will be done on local machines, in an edge computing fashion, based on DuckDB. Apache Kafka\nWhy include Kafka? Why not a Redis queue or pub/sub? Or even custom gRPC microservices? Having middleware for inter-process communication is always easier to handle than custom gRPC solutions. But why not Redis or other lightweight solutions like ZeroMQ? Well, mostly for the learning experience, and because Kafka might be the single most useful piece of software you could use in production systems, because it can help your app scale up or down as required, and it will help you stay compliant or debug your system, since it provides replayability. A base deployment just requires ~600 MB of RAM anyway. We also thought about Apache Flink for stream processing from Kafka topics, but there is also Faust, a Python-native solution, which we\u0026rsquo;re likely to use later instead. MLflow\nWhen you need open source self-hostable ML model tracking and registration, it\u0026rsquo;s hard to find a better alternative. Other options include Weights \u0026amp; Biases (not fully open source), Neptune.ai (cloud-based service), or Kubeflow (requires Kubernetes, which is overkill for our single-machine home lab). Ollama + Open WebUI\nUsed for self-hosted LLMs. With Open WebUI, we get our own local ChatGPT Or we can use Ollama for LangChain or other implementations. Requires GPU support, which might be hard to setup, as it needs to be configured in the Proxmox host machine, as well as the VM, and possible Docker as well. From On-Premise to Cloud # Let\u0026rsquo;s establish a parallel between on-premise Proxmox based services and their cloud platform alternatives. This will help you understand how to transfer knowledge from working on your home lab into a cloud platform like AWS, GCP, or Azure.\nL1: Foundation # A cloud platform is essentially our foundation layer. These are a given for AWS, GCP, Azure, or any other cloud platform.\nVMs\nGCP Compute Engine Amazon EC2 (Elastic Compute Cloud) Azure Virtual Machines Object Stores\nGoogle Cloud Storage Amazon S3 (Simple Storage Service) Azure Blob Storage L2: Platform # In a cloud platform, L1 and L2 are essentially at the same level, at least for Docker on L2. For the Git, filling the role of our GitLab instance, it depends—if we\u0026rsquo;re talking secrets management, then each cloud platform has its own dedicated product for it, but, if we\u0026rsquo;re talking code tracking git workflows, then we might look at Git as a service instead.\nDocker\nAWS Fargate GCP Cloud Run Azure Container Instances Git\nAWS CodeCommit GCP Cloud Source Repositories (sunset) GCP Secure Source Manager Azure Repos L3: Services # There are often multiple options per service, depending on your needs. For example, you might want to be able to administer a database yourself, via the command line (e.g., use psql to login to the Postgres root user), or you might want a single database that scales horizontally transparently. Being familiar with the product catalog and knowing how to distinguish between such services is at the root of cloud engineering. Below you\u0026rsquo;ll find a few examples for our use case.\nPostgreSQL\nAmazon RDS Amazon Aurora GCP Cloud SQL GCP AlloyDB Azure Database for PostgreSQL Cosmos DB (not relational, but built on top of PostgreSQL) DuckLake\nEasily integrates with most of the PostgreSQL alternatives, regarding the catalog. And with each of the cloud object store alternatives, regarding storage. Apache Kafka\nAmazon MSK (Managed Streaming for Apache Kafka) Confluent Cloud on GCP, Azure, or AWS MLflow\nCan be deployed on a VM or as Docker a container, on either of the alternatives listed above. It integrates with most of the PostgreSQL alternatives, regarding the backend store. And with each of the cloud object store alternatives, regarding artifact storage. Ollama\nTitan on Amazon Bedrock Gemini on Google Vertex AI Phi on Azure AI Foundry Azure OpenAI Service L4: Applications # Deploying a layer 4 application will depend on the requirements, but for our use cases—REST API, microservices, web apps—it could easily be done using container services, like AWS Fargate, along with storage services, like AWS S3. There\u0026rsquo;s nothing specifically designed on a cloud platform for this, as the cloud platform is itself designed to support your applications.\nFinal Remarks # What a cloud platform offers in simplicity, it also takes in cost. Storage is cheap, and basic services like VMs are accessible as well, but once you go deeper into specialized services, e.g., for AI or highly scalable or available databases, then the cost skyrockets. At that point, you\u0026rsquo;re most likely better off just hiring someone to handle it using traditional infrastructure. Managing your cloud infrastructure cost might be the difference between staying in business or shutting down. Sometimes considering on-premise or cheaper cloud infrastructure, like basic VMs, even outside the major cloud providers, can be a cheaper and even better solution for your use case.\n","date":"16 September 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-architecture/","section":"","summary":"Summary # In this video, we\u0026rsquo;ll learn how to design a modern data stack, built for home labs, freelancing data experts, or general on-premise needs.","title":"Data Lab Infra - Part 1: Architecture Design","type":"posts"},{"content":"","date":"27 August 2025","externalUrl":null,"permalink":"/tags/ab-testing/","section":"Tags","summary":"","title":"Ab-Testing","type":"tags"},{"content":"","date":"27 August 2025","externalUrl":null,"permalink":"/tags/ducklake/","section":"Tags","summary":"","title":"Ducklake","type":"tags"},{"content":" Summary # Learn how to implement an end-to-end machine learning workflow, from data ingestion, to A/B testing and monitoring, using MLflow, Kafka and DuckLake. We\u0026rsquo;ll discuss model training and tracking with MLflow, real-world deployment approaches, how to track inference results and feedback in your data lakehouse using Kafka and DuckLake, and how to compute monitoring statistics such as prediction drift, feature drift, or estimated performance, for unlabeled data, as well as taking advantage of user feedback to estimate model error. This should give you the tools you need to build and maintain your own ML lifecycle.\n\u003e Organizational Roles # I\u0026rsquo;ll start with a bit of a comment on organizational roles in a data team, and their responsibilities. This will help clarify a design choice I made in the architecture diagram below.\nWe\u0026rsquo;re considering a team with a Data Engineer, a Data Scientist, and a ML Engineer. As most ops roles, the ML Engineer has responsibilities that overlap with other areas covered by other roles, but these usually have a distinct focus when compared to those other roles.\nFor example, both a Data Scientist and a ML Engineer will work with model training and evaluation in some capacity, but the Data Scientist is usually more interested in picking the right algorithm, hyperparameters and features for training, or the right evaluation metrics for the problem at hand, while the ML Engineer is concerned with making sure that these processes can be tracked in a way that models are properly registered and information is easily available to make a decision on the best model and its staging or production readiness.\nSimilarly, the Data Engineer also overlaps with the ML Engineer, particularly regarding the data quality aspects of the training and test datasets.\nArchitecture # In the diagram below, we highlight the Data Engineering, Data Science, and ML Engineering responsibilities. In blue, you\u0026rsquo;ll be able to track the training flow, in yellow the evaluation flow, and in green the inference flow. The middle layer is an overview on the data schemas for DuckLake tables and Kafka topics. On the bottom, you\u0026rsquo;ll find the service layer, with the REST API, and the Kafka producers and consumers.\nIn our implementation, FastAPI was responsible for initializing the Kafka producers and consumers, but in a real-world scenario this would be done separately, each running on their own container.\nWhy Kafka and not something else, like gRPC or Redis Pub/Sub? Kafka acts as an event log, so it provides topic replayability, which is great for compliance, but also for debugging. It can also front-face your product in a way that more brokers can be deployed in groups, handling partitioned messages to the same topic, which is a great way to scale up and down as required. The trade-off is that a controller and a single broker will require ~600 MiB of RAM to run, which is usually acceptable, but, depending on your priorities, it might be too much for basic RPC or queuing, and this will only provide replayability as an advantage, but not partitioning or the reliability of multiple brokers.\nOrchestration # Previously, we hadn\u0026rsquo;t implemented any approach for orchestration on datalab, but with the new ML package, this gained more relevance, so we opted to use just for handling tasks via the command line. This is yet another great tool built in Rust, that mimics the make command, but it\u0026rsquo;s specifically designed for running tasks, as a opposed to building files. Implementing tasks in a Makefile usually requires that those targets are added as dependencies of the .PHONY target, so that make knows they won\u0026rsquo;t produce a file. This would avoid that a task with a name matching an existing file would be skipped due to the file existing and being up-to-date. Instead, when using a justfile, all targets are essentially .PHONY. In addition, since just is a tool specifically designed to run tasks, it also provides several utilities that simplify your life. For example, preloading your existing .env is as easy as adding:\nset dotenv-load And while make will run using /bin/sh by default, just will use your login shell by default. Additionally, just also supports positional parameters with default values, and it provides a simple way to list all tasks and their parameters:\njust -l A good way to setup your justfile is to use this as your first task:\ndefault: just -l The first task is the one that\u0026rsquo;s run by default when invoking just without any arguments. And yes, just can call just from other tasks. For example, we do this:\ncheck-init-sql: test -r {{init_sql_path}} || just generate-init-sql It also distinguishes internal variables from environment variables:\nengine_db_path := join(local_dir, env_var(\u0026#34;ENGINE_DB\u0026#34;)) check-engine-db: test -r {{engine_db_path}} So, if your orchestration needs don\u0026rsquo;t require something like Apache Airflow or Prefect, and make or shell scripting don\u0026rsquo;t quite do it, you should consider just using just.\nTraining and Tracking # As a way to test our workflow, we decided to train a model to classify user-generated text as depression or not depression. For training and testing, we used the ShreyaR/DepressionDetection dataset from Hugging Face. This was ingested and transformed as usual, using our dbt and DuckLake workflow.\nWe then trained four models, combining two algorithms and two feature sets:\nLogistic Regression + TF-IDF Logistic Regression + Embeddings (all-MiniLM-L6-v2) XGBoost + TF-IDF XGBoost + Embeddings (all-MiniLM-L6-v2) For each model, we tracked the following metadata and artifacts on MLflow:\nMetadata Data source (dataset tags) Algorithm/method used Features used Hyperparameter grid Cross-validation config Evaluation metrics (validation and testing) Input datasets (schema and shape) Artifacts Serialized model with input/output signature Python dependencies We used the following two helper functions, at the beginning and end of our training function, which helped to keep the training code clean.\nAt the beginning of a run (training starting), we point to the correct MLflow server, create the experiment, if it doesn\u0026rsquo;t exist, and start a run with a name based on the algorithm/method and feature set. Then, we use mlflow.set_tags to track metadata, including dataset based metadata, which is not yet visible in the current version of the MLflow UI when associated with a logged dataset directly. We also use mlflow.log_inputs to log the schema and size of the training and test sets. See next:\ndef mlflow_start_run( experiment_name: str, run_name: str, tags: dict[str, Any], datasets: list[Dataset], dataset_tags: dict[str, Any], ): tracking_uri = env.str(\u0026#34;MLFLOW_TRACKING_URI\u0026#34;) mlflow.set_tracking_uri(tracking_uri) log.info(\u0026#34;MLflow tracking URI: {}\u0026#34;, tracking_uri) mlflow.set_experiment(experiment_name) log.info(\u0026#34;MLflow experiment: {}\u0026#34;, experiment_name) mlflow.start_run(run_name=run_name) log.info(\u0026#34;MLflow run: {}\u0026#34;, run_name) log.info(\u0026#34;MLflow: logging tags and input datasets\u0026#34;) mlflow.set_tags(tags | dataset_tags) contexts = [ds.name for ds in datasets] tags_list = [dataset_tags for _ in datasets] mlflow.log_inputs( datasets=datasets, contexts=contexts, tags_list=tags_list ) At the end of a run (training finished), we log the Python requirements, the serialized model, the best parameters from cross-validation (CV), and the metrics from CV and test set evaluation. See next:\ndef mlflow_end_run( model_name: str, model: Pipeline, params: dict[str, Any] | None = None, metrics: dict[str, Any] | None = None, train: pd.DataFrame | None = None, ): signature = None if train is not None: log.info(\u0026#34;MLflow: inferring model signature\u0026#34;) model_output = model.predict(train.input) signature = infer_signature(train.input, model_output) log.info(\u0026#34;Extracting pip requirements from uv\u0026#34;) pyproject = tomllib.load(open(\u0026#34;pyproject.toml\u0026#34;, \u0026#34;rb\u0026#34;)) requirements = ( pyproject .get(\u0026#34;project\u0026#34;, {}) .get(\u0026#34;dependencies\u0026#34;, {}) ) log.info(\u0026#34;MLflow: logging model\u0026#34;) mlflow.sklearn.log_model( sk_model=model, name=model_name, registered_model_name=model_name, signature=signature, pip_requirements=requirements, ) if params is not None: log.info(\u0026#34;MLflow: logging parameters\u0026#34;) mlflow.log_params(params) if metrics is not None: log.info(\u0026#34;MLflow: logging metrics\u0026#34;) mlflow.log_metrics(metrics) mlflow.end_run() MLflow Artifact Storage # MLflow uses a database, such as SQLite or PostgreSQL, as its backend store to log metadata. However, for artifacts, it can either log them locally (mostly just for testing), directly on S3, or by acting as an artifact proxy, transparently pointing to either a local or S3 backed location. Here\u0026rsquo;s an overview on how to setup these two options, when starting MLflow:\nDirect S3 Storage Set --default-artifacts-root to an S3 path (e.g., s3://mlflow/artifacts). Preferred when each user has its own AWS/S3 credentials. Artifact Proxy Set --serve-artifacts. Set --artifacts-destination to a local or S3 path. Artifacts will be stored under the default root of mlflow-artifacts:/. Preferred when users have no direct interaction with AWS/S3, or to simplify model serving (no need to create S3 credentials for inference services). Please see our docker-compose.yml file for details on setting up MLflow as an artifact proxy, with SQLite for the backend store. And also checkout the Common Setups section on the official documentation, where you\u0026rsquo;ll be able to find a diagram of the three most common MLflow deployment options.\nEvaluation and Inferencing # We used a train/test split of 80/20, and then split our training set into 3-folds for validation. Our models were trained and optimized using cross-validation and F1 scoring over a minimal hyperparameter grid. For each model, we provided the accuracy and F1 score for the best fold, as well as for the test set. These were logged into MLflow, as described above.\nOut of the four models, we selected the best model, which was XGBoost + Embeddings, along with the most different model, Logistic Regression + TF-IDF, and served these models through a unique endpoint, randomly selecting one of them per request, for A/B testing.\nEach inference was assigned a UUID and a creation date, and then sent to a Kafka topic. A topic consumer buffered these requests until hitting a large enough batch or a given timeout, at which point they were flushed into a DuckLake catalog with encrypted storage (remember, this is user data). An example of how to call this endpoint is provided in the justfile:\nmlops_test_inference_payload := \u0026#39;\u0026#39;\u0026#39; { \u0026#34;models\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;dd_logreg_tfidf\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;dd_xgboost_embeddings\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; } ], \u0026#34;data\u0026#34;: \u0026#34;...bc i have depression how are you all d\u0026#34;, \u0026#34;log_to_lakehouse\u0026#34;: true } \u0026#39;\u0026#39;\u0026#39; mlops-test-inference: check-curl curl -f -X POST \u0026#34;http://localhost:8000/inference\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{{mlops_test_inference_payload}}\u0026#39; @echo curl -f -X GET \u0026#34;http://localhost:8000/inference/logs/flush\u0026#34; We also provided an API endpoint where users were able to submit a feedback score (1.0 or 0.0 for our depression classifier), based on the inference UUID. Multiple requests with the same inference UUID appended the feedback to the feedback DOUBLE[] column.\nDeployment # Below, we cover two approaches for model deployment:\nUsing MLflow to build a Docker image per model. Building your own custom FastAPI REST endpoint. The MLflow approach provides a more straightforward and standard way to deploy, but each model requires its own separate deployment, and you can\u0026rsquo;t control the output—for example, if you want to return the output from predict_proba for a scikit-learn model, instead of using predict, then you\u0026rsquo;d need to code that into your model\u0026rsquo;s prediction function before logging it to MLflow.\nThe custom approach provides more flexibility and potential for optimization—for example, we can share resources for multiple models, or build a custom environment with minimal dependencies. Moreover, we can still create our own Docker image for a custom REST endpoint. This is the preferred approach by seasoned ML Engineers, and this is what we do here as well, even more so since our inference results return probabilities and need to be logged to DuckLake via Kafka.\nMLflow Models # While MLflow provides an out-of-the-box approach to deploy logged models, the Docker images it produces tends to be huge by default (~25 GB for the datalab project). You\u0026rsquo;ll need to properly determine the minimum required dependencies when logging your model, if you want to make sure that the resulting image size is optimized. Even then, this deployment approach will produce individual images per model, which is good for isolation, but harder on resource management and cost. Instead, you might want to build your own custom solution for deployment, which we will describe in the following section. Meanwhile, here\u0026rsquo;s an example on how to use the MLflow\u0026rsquo;s deployment workflow.\nIn order to create a Docker image for your model, you can simply run the following command, providing it a model URI and the target image name:\nmlflow models build-docker \\ -m \u0026#34;models:/dd_logreg_tfidf/latest\u0026#34; \\ -n \u0026#34;mlflow_model_dd_logreg_tfidf\u0026#34; You can then run a container based on the produced image, optionally limiting the number of workers (which defaults to all CPU cores) by setting MLFLOW_MODEL_WORKERS:\ndocker run -d \\ -p 5001:8080 \\ -e MLFLOW_MODELS_WORKERS=4 \\ \u0026#34;mlflow_model_dd_logreg_tfidf\u0026#34; In order to classify a batch of examples for a model that takes a DataFrame with a single column input, you can POST JSON to the /invocations path:\ncurl -X POST \u0026#34;http://localhost:5001/invocations\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;dataframe_split\u0026#34;: { \u0026#34;columns\u0026#34;: [\u0026#34;input\u0026#34;], \u0026#34;data\u0026#34;: [ [\u0026#34;...bc i have depression how are you all d\u0026#34;], [\u0026#34;...we were really so lucky...\u0026#34;] ] } }\u0026#39; Custom API # Like we said, in practice, it\u0026rsquo;s more common to deploy models using a custom REST API, usually created with FastAPI, often chosen due to its self-documenting approach via Swagger UI and ReDoc. This also lets you produce your own Docker image, optimizing it as much as you\u0026rsquo;d like, and you can also serve multiple models using the same API, which you cannot do with the MLflow approach.\nImagine a 25 GiB image for each of your classifiers, the time it would take to build and deploy, and the resources it would require! This is why most ML Engineers create their own web services to deploy their models.\nOur API provides the following two main endpoints, which encapsulate the whole workflow logic.\nPOST /inference to INSERT inference results:\n@app.post(\u0026#34;/inference\u0026#34;) async def inference( inference_request: InferenceRequest, request: Request ): try: inference_result = predict(inference_request) except ModelNotFound: return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Model not found\u0026#34;}, status_code=status.HTTP_404_NOT_FOUND, ) if inference_request.log_to_lakehouse: log.info(\u0026#34;Queuing lakehouse insertion for inference result\u0026#34;) await queue_inference_result( request.app.state.inference_result_producer, inference_result, ) return inference_result PATCH /inference to UPDATE an inference result by appending to its feedback:\n@app.patch(\u0026#34;/inference\u0026#34;) async def inference( inference_feedback: InferenceFeedback, request: Request ): log.info(\u0026#34;Queuing lakehouse append for inference feedback\u0026#34;) await queue_inference_feedback( request.app.state.inference_feedback_producer, inference_feedback, ) return Response(status_code=status.HTTP_204_NO_CONTENT) Monitoring Strategy # During monitoring, we compare reference data with the current data. Our reference data is always based on the training set (e.g., prediction labels or features based on the training set). Our current data, on the other hand, is based on a sliding window of 7 days, that we compute per day.\nBelow, we will describe the metrics that we considered, along with our particular implementation, which you can find in the datalab codebase, under ml.monitor.\nPrediction Drift # Prediction drift, or concept drift, is concerned with comparing the reference and current prediction probability distributions.\nWhile there are multiple possible statistical approaches to compare these distributions, we use a Kolmogorov–Smirnov test, particularly looking at the D-statistic, which illustrates the largest possible gap between the two distributions. The higher the D-statistic, the higher the prediction drift.\nFeature Drift # Feature drift, or data drift, is usually concerned with comparing the distributions of individual features. Drift scores for individual features can be aggregated into a global feature drift score, but feature drift can also be computed over all the feature set directly. Below, we briefly explain both approaches, per feature or dataset.\nPer Feature # We start by comparing each feature individually for the reference and current datasets. For example, if our features are text, we might compare term distributions or the distributions of TF-IDF over all examples. If we\u0026rsquo;re working with embeddings, it\u0026rsquo;s similar—assuming rows are examples and columns are features, we simply compare equivalent columns on the reference and current data.\nAgain, there are several statistical approaches that we can use here, but let\u0026rsquo;s assume we\u0026rsquo;re again working with a Kolmogorov–Smirnov test, which provides the D-statistic, as well as the p-value.\nOnce we compare all pairs of features, we\u0026rsquo;ll end up with a 1D array with a score per feature. If this score is the D-statistic, it\u0026rsquo;s common for us to take the median, which is less sensitive to outliers, and compute an aggregate feature drift score. Another, less informative, approach is to count the number of p-values \u0026lt; 0.05 and return that fraction, which just tells us how many features drifted significantly, thus imputing drift to the overall data.\nThis is not our implementation here. Instead, we relied on a per dataset feature drift, as described next.\nPer Dataset # For this approach, all features are used simultaneously. A simple way to achieve this is by concatenating reference and current data, assigning each subset its own label:\nReference → 0 Current → 1 Keep in mind that these are not the labels in your original data, so, for example, our depression training set contained label 0 for not depression and label 1 for depression, but here the whole dataset will have label 0, regardless of whether we\u0026rsquo;re considering a negative or positive example of depression. Now, instead, we\u0026rsquo;re trying to distinguish between examples in reference or current.\nAfter training a simple classifier (e.g., logistic regression) with the described dataset, we evaluate using ROC AUC.\nIf AUC ~ 0.5, the classifier is no better than random guessing, so it cannot distinguish between the datasets, which means there is no feature drift. If AUC \u0026gt; 0.5, the classifier is able to distinguish both datasets, so they are not the same and there is as much drift as the AUC. If AUC \u0026lt; 0.5, the interpretation is similar to that of AUC \u0026gt; 0.5 in regards to drift, but it also tells us that the classifier is predicting in reverse (this should never happen). Estimated Performance # Inspired by NannyML, which was incompatible with our remaining dependencies, we implemented confidence-based performance estimation (CBPE).\nFor the model we want to test (e.g., logistic regression with TF-IDF features), we train an isotonic regression model to predict correctness based on the output probabilities of the model we\u0026rsquo;re testing. The isotonic regressor is trained over a dataset with a single feature in the format:\n$\\text{pred_prob} \\mapsto (\\text{pred_label} = \\text{correct_label})$\nWhere $\\text{pred_prob}$ is the output of predict_proba for the model we\u0026rsquo;re testing, over examples in the original training set, $\\text{pred_label}$ is the corresponding binary output after applying the decision threshold (e.g., $\\text{pred_label} \\leftarrow \\text{pred_prob} \\ge 0.5$), and $\\text{correct_label}$ is the target on the original training set, used to train the model we\u0026rsquo;re testing.\nOnce we have the isotonic regressor (one per model to test), we\u0026rsquo;re be able to predict the inference correctness for new unseen examples, and, from that, we\u0026rsquo;re then able to obtain true and false positives and negatives, and calculate evaluation metrics like accuracy or F1.\nUser Feedback # Finally, based on user feedback, we\u0026rsquo;re also able to a calculate Brier score per inference result, comparing our binary prediction to a probabilistic user feedback score. We can then average the Brier scores to obtain a global error-like metric (notice the similarity with mean-squared error, MSE):\n$\\displaystyle\\text{Avg}(BS) = \\frac{1}{n}\\sum_{i=1}^n\\left(\\text{prediction}_i - \\frac{1}{m}\\sum_{j=1}^m\\text{feedback}_{ij}\\right)$\nWhere:\n$\\text{prediction}_i \\in \\{0, 1\\}$ $\\text{feedback}_{ij} \\in [0, 1]$. Similarly to MSE, a lower average Brier score is better:\n0 is a perfect calibration. 1 is the worst possible outcome. Extending Observability # Other elements that we might track to extend observability would be:\nData Quality Shared responsibility with data engineering. Rule-based validations (e.g., check for all zeros in predictions, or missing values). For example, expressed as an aggregate score of rule compliance. Model Explainability SHAP (SHapley Additive exPlanations) LIME (Local Interpretable Model-agnostic Explanations) Simulating Inference and Feedback # Since we do not have a production system, we simulate inference requests and user feedback, in order to obtain credible data to test our monitoring statistics with. We use a dataset external to our train/test set, also from Hugging Face, to help with this task: joangaes/depression. We call this our monitor set. This follows a similar structure to the train/test set, with a text column and a label column for depression or not depression.\nUsing this data, we simulate inference and feedback assignment in multiple passes (default 3) over the monitor set, where we:\nSample a fraction of the dataset (optional). Run inference over all examples using a given decision threshold (default 0.5). Log results to the data lakehouse with a random date (default range of 4 weeks). Provide feedback for a fraction (default range of $[0.45, 0.55]$) of those results\u0026hellip; \u0026hellip;simulating wrong feedback by using the complement over a fraction of feedback outputs (default range of $[0.10, 0.25]$) of the considered results. Simulated data should contain mostly correct feedback, with a few errors, simulating a real-world scenario. Inferences might have no feedback, or any number of feedback scores up to the number of passes, i.e., 0 to 3 feedback scores.\nA/B Testing Results # Based on the simulated inference results and user feedback, we now look at and interpret the monitoring statistics.\nInferences Over Time # We track the number of inferences ran per day, over the simulated period of one month. This also shows how many times each model was randomly selected per day. Volume is higher for mid-August and although model selection is uniform, the logistic regression model using TF-IDF features seems to have been selected more frequently when looking at daily aggregations.\nPrediction Drift # Also known as concept drift, we measure prediction drift based on the D-statistic from the Kolmogorov–Smirnov test. The lower the D-statistic, the less prediction drift there is. As we can see, prediction drift is reaching critical levels for the logistic regression model, while being much lower for the XGBoost model, despite still showing a considerable magnitude. However, the logistic regression is particularly concerning, as the D-statistic is not only high, but also extremely consistent over time, indicating an issue with that model, beyond just optimization.\nFeature Drift # Also known as data drift, feature drift is measured based on ROC AUC. As such, we are looking for values of ~0.5, indicating no feature drift. As we can see, there is also considerable feature drift for both models. This indicates that the training data was not representative of incoming examples for inference. It might also be the case that data preprocessing requires a few normalization steps to make sure the text is formatted similarly across reference and current data. For our models, we\u0026rsquo;re probably suffering from both problems, as we didn\u0026rsquo;t put much effort into the preprocessing stage. This goes to show the importance of good data engineering.\nEstimated Performance # While previous metrics indicate that our models are not properly calibrated, we\u0026rsquo;re still predicting evaluation scores as high as the ones obtained during testing. On one side, this matches expected behavior, on the other side we know that we have other problems to solve, before we can trust this score overall.\nUser Feedback # Finally, based on user feedback, we\u0026rsquo;re obtaining inconsistent average Brier scores over time, most likely due to the small monitor set sample that we used. However, overall, the lowest (best) values seem to be for the XGBoost model, indicating we might obtain better results by investing on that model, but we\u0026rsquo;d need more information.\nFinal Remarks # Hopefully, this is enough to get you started on MLOps and ML Engineering, providing a comprehensible example of an end-to-end workflow and the types of tasks we\u0026rsquo;re expected to work on as an ML Engineer. In future videos and blog posts, I expect to go deeper into the infrastructure side of Data Engineering and MLOps, moving into model deployment, and real-time model monitoring.\n","date":"27 August 2025","externalUrl":null,"permalink":"/posts/mlops-ab-testing/","section":"","summary":"Summary # Learn how to implement an end-to-end machine learning workflow, from data ingestion, to A/B testing and monitoring, using MLflow, Kafka and DuckLake.","title":"MLOps: A/B Testing with MLflow, Kafka, and DuckLake","type":"posts"},{"content":"","date":"27 August 2025","externalUrl":null,"permalink":"/tags/monitoring/","section":"Tags","summary":"","title":"Monitoring","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/categories/data-science/","section":"Categories","summary":"","title":"Data Science","type":"categories"},{"content":" Summary # In this video, we reproduce the approach that predicts Survivor winners and apply it to Economic Competition Networks to better understand world trade and economic leaders. We build a country to country competition network based on the Export Similarity Index (ESI), and we use several techniques from network science, like PageRank, community detection, weak component analysis, or the recent common out-neighbor (CON) score, to better understand how countries compete with each other within the world economy, identifying dominating or leading economies, as well as their counterpart weaker or smaller economies.\n\u003e Dataset # We use The Atlas of Economic Complexity dataset, which is summarized in the following table. We only provide a top-level overview of the data here. For an in-depth detailed description, click the Download button in each table row of the link above—that will open a popup with detailed information on the fields for each CSV file.\nTitle Description Complexity Rankings \u0026amp; Growth Projections Economic Complexity Index (ECI) and partial growth projections for world economies from 1995 to 2023. Country Trade by Product Exports and imports, per country and product, over the years. Different files provide a different product category granularity based on the number of HS92, HS12 or SITC digits. Different files are also provided for services, using a non-standard classification internal to Growth Labs that also provides different digit-based granularities. Total Trade by Country Total exports and imports, per country, over the years. Different files provide data about products and services.\nHow big is the economy for a country?\nHow did it progress over the last 28 years? Total Trade by Product Total exports and imports, per product, over the years. Again, this is provided at different product granularities based on HS92, HS12 or SITC digits. Different files are also provided for services, using a non-standard classification internal to Growth Labs that also provides different digit-based granularities.\nHow big is the market for a product?\nHow did it progress over the last 28 years? Country Trade by Partner Bilateral exports and imports between pairs of countries, over the years. ✅ Country Trade by Partner and Product Bilateral exports and imports between pairs of countries, for a given product, over the years. This is provided at 6-digit granularity based on HS92, HS12 or SITC digits. This is partitioned into multiple files in blocks of 10 years (or 5 years only for 1995-1999).\nA granularity of 4 digits would be enough to distinguish between main product types (e.g., beef vs pork vs poultry, fresh vs frozen; gasoline engines vs diesel engines). With 6 digits we get a lot more detail (e.g., carcasses and half-carcasses of bovine animals, fresh or chilled; engines for aircraft). We use the HS92 data with 6 digits—the only one available, but also ideal to capture trade competition between countries, as true competition is only uncovered at a smaller scale. We only look at the 2020-2023 period, for recency, aggregating totals for those three years. ✅ Country Classification Country metadata. Regional Classification Regional classification for countries—continent it belongs to, political region (e.g., European Union), subregion (e.g., Central America, Western Africa), trade regions (e.g., NAFTA, OPEC), etc. HS12 Product Classification Product metadata according to HS12 codes. ✅ HS92 Product Classification Product metadata according to HS92 codes.\nWe use this to inspect products traded by salient countries during the analysis. Services Product Classification Services metadata according to a non-standard classification internal to Growth Labs.\nWe use this to inspect services traded by salient countries during the analysis. SITC Product Classification Product metadata according to SITC codes. Product Space Related Edges HS92 4-digit codes for source and target products in the same space (e.g., women\u0026rsquo;s coats ⇄ sweaters). Product Space Layout HS92 4-digit codes for products along with their 2D embedding, where close products are co-exported by countries. Here are the citations for the datasets that we use:\nCountry Trade by Partner and Product:\nThe Growth Lab at Harvard University, 2025, \u0026ldquo;International Trade Data (HS92)\u0026rdquo;, https://doi.org/10.7910/DVN/T4CHWJ, Harvard Dataverse\nCountry Classification \u0026amp; HS92 Product Classification:\nThe Growth Lab at Harvard University, 2025, \u0026ldquo;Classifications Data\u0026rdquo;, https://doi.org/10.7910/DVN/3BAL1O, Harvard Dataverse\nGraph Schema # Out of the three CSV files that we identified above as being used, we produce the following nodes and relationship labels:\nNodes Country node_id – globally unique node identifier – INT64 Properties from all Country Classification columns Product node_id – globally unique node identifier – INT64 Properties from all HS92 Product Classification columns Relationships (:Country)-[:CompetesWith]-\u0026gt;(:Country) ESI – Export Similarity Index – DOUBLE (:Country)-[:Exports]-\u0026gt;(:Product) amount_usd – exports dollar amount (2020-2023) – INT128 (:Country)\u0026lt;-[:Imports]-\u0026gt;(:Product) amount_usd – imports dollar amount (2020-2023) – INT128 Take a look at the following diagram, where rectangles represent the raw CSV files, with dashed arrows illustrating the data source, and circles represent nodes, with solid arrows representing relationships.\nJupyter Notebook # The following sections are an adaptation of the Jupyter Notebook that we created to analyze the Economic Competition Network.\nSetup # ETL # For ETL, we directly call the appropriate dlctl commands for:\nIngesting the dataset Transforming using SQL on top of DuckLake Exporting from the data lakehouse into Parquet Loading the graph into Kuzu Computing general analytics scores Be sure to uncomment the cell below and run it once.\n!dlctl ingest dataset -t atlas \\ \u0026#34;The Atlas of Economic Complexity\u0026#34; !dlctl transform -m +marts.graphs.econ_comp !dlctl export dataset graphs econ_comp !dlctl graph load econ_comp !dlctl graph compute con-score econ_comp Country CompetesWith Imports # from pathlib import Path from string import Template from textwrap import dedent from typing import Any, Literal, Optional import kuzu import matplotlib.pyplot as plt import networkx as nx import numpy as np import pandas as pd from scipy.special import expit import graph.visualization as vis from shared.settings import LOCAL_DIR, env Globals # We setup access to the appropriate Kuzu path, based on the shared .env configuration, ensuring the graph exists before running the notebook. Once setup, conn will be used to query the graph directly throughout this notebook.\ndb_path = Path(LOCAL_DIR) / env.str(\u0026#34;ECON_COMP_GRAPH_DB\u0026#34;) assert db_path.exists(), \\ \u0026#34;You need to create the graph DB using dlctl first\u0026#34; db = kuzu.Database(db_path) conn = kuzu.Connection(db) Constants # In order to ensure color consistency for our plots, we extract the color palette from matplotlib into MPL_PALETTE.\nMPL_PALETTE = ( plt.rcParams[\u0026#34;axes.prop_cycle\u0026#34;] .by_key()[\u0026#34;color\u0026#34;] ) We also map a display attribute for each of our note labels, Country and Product. We\u0026rsquo;ll use the short names for both when plotting graph visualizations or related charts.\nLABEL_PROPS = { \u0026#34;Country\u0026#34;: \u0026#34;country_name_short\u0026#34;, \u0026#34;Product\u0026#34;: \u0026#34;product_name_short\u0026#34;, } Functions # We create a few reusable functions, where we run Kuzu queries. In a few cases, it was helpful to debug the query with parameters (e.g., using Kuzu Explorer), so we created a helper function for this (note that this doesn\u0026rsquo;t support string parameters, as we didn\u0026rsquo;t ned them).\ndef print_query(query: str, params: dict[str, Any]): dbg_query = dedent(query).strip() dbg_query = Template(dbg_query) dbg_query = dbg_query.substitute(params) print(dbg_query) We\u0026rsquo;ll also cluster nodes using different strategies and compare groups, so we implement a basic Jaccard similarity function.\ndef jaccard_sim(a: pd.Series, b: pd.Series) -\u0026gt; float: a = set(a) b = set(b) return len(a \u0026amp; b) / len(a | b) We might want to look at the top x% of traded products, based ona USD. The following function will help filter this.\ndef top_frac(df: pd.DataFrame, col: str, frac: float = 0.25): mask = (df[col] / df[col].sum()).cumsum() \u0026lt;= frac return df[mask] Analysis # We focus on the CompetesWith projection, a relationship given by the Export Similarity Index (ESI). Our graph analysis includes:\nDynamic competition analysis. Dominating and weaker economy identification, based on the CON score for each country. Trade basket overlap analysis for top and bottom economies. Competition network analysis. Community analysis, including community mapping, top traded product identification, and trade alignment study (self-sufficiency, external competitiveness). Weak component analysis, following a similar approach to the community analysis—weak components widen community reach. Community and weak component comparison. Economic pressure analysis. Dynamic Competition Analysis # Top 10 Dominating Economies # These are highly spread economies, able to compete with several other countries, i.e., with a high number of common out-neighbors (CON).\ndom_econ_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) RETURN c, c.node_id AS node_id, c.country_name_short AS country ORDER BY c.con_score DESC LIMIT 10 \u0026#34;\u0026#34;\u0026#34; ).get_as_df()[[\u0026#34;node_id\u0026#34;, \u0026#34;country\u0026#34;]] dom_econ_df.index = pd.RangeIndex( start=1, stop=len(dom_econ_df) + 1, name=\u0026#34;rank\u0026#34; ) dom_econ_df node_id country rank 1 206 United States of America 2 55 Canada 3 34 United Arab Emirates 4 107 Netherlands 5 132 United Kingdom 6 175 Belgium 7 134 Italy 8 223 Spain 9 131 France 10 145 Thailand Top 3 Exports # Looking at the top exports will help contextualize these economies. We only look at the top 3 products, to keep the visualization clean and readable.\ndom_econ_g = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WITH c ORDER BY c.con_score DESC LIMIT 10 MATCH (c)-[e:Exports]-\u0026gt;(p:Product) MATCH (c2:Country)-[:Exports]-\u0026gt;(p) WITH c, e, p, count(DISTINCT c2) AS exporters WHERE exporters \u0026gt; 1 WITH c, e, p ORDER BY c.node_id, e.amount_usd DESC SKIP 0 WITH c, collect({p: p, e: e}) AS export_list UNWIND list_slice(export_list, 0, 3) AS r RETURN c, r.e, r.p ORDER BY c.node_id, r.p.node_id \u0026#34;\u0026#34;\u0026#34; ).get_as_networkx() vis.set_labels(dom_econ_g, LABEL_PROPS) vis.plot(dom_econ_g, scale=1.25, seed=3) Bottom 10 Weaker Economies # These are smaller or weaker economies, in the sense that they have a lower competition power. We also find the Undeclared special country node at rank 1, showing that only a small number of products are undeclared worldwide.\nweak_econ_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) RETURN c, c.node_id AS node_id, c.country_name_short AS country ORDER BY c.con_score ASC LIMIT 10 \u0026#34;\u0026#34;\u0026#34; ).get_as_df()[[\u0026#34;node_id\u0026#34;, \u0026#34;country\u0026#34;]] weak_econ_df.index = pd.RangeIndex( start=1, stop=len(weak_econ_df) + 1, name=\u0026#34;rank\u0026#34; ) weak_econ_df node_id country rank 1 193 Undeclared 2 72 Bouvet Island 3 170 Wallis and Futuna 4 106 Norfolk Island 5 167 Saint Pierre and Miquelon 6 216 Niue 7 66 South Georgia and South Sandwich Islds. 8 121 Northern Mariana Islands 9 161 Heard and McDonald Islands 10 100 Western Sahara Top 3 Exports # If we look at the top 3 exports for each competing country in the bottom of the ranking according CON scores, as expected we find that these are more disconnected economies, mostly focusing on raw materials, or components and machinery.\nweak_econ_g = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WITH c ORDER BY c.con_score ASC LIMIT 10 MATCH (c)-[e:Exports]-\u0026gt;(p:Product) MATCH (c2:Country)-[:Exports]-\u0026gt;(p) WITH c, e, p, count(DISTINCT c2) AS exporters WHERE exporters \u0026gt; 1 WITH c, e, p ORDER BY c.node_id, e.amount_usd DESC SKIP 0 WITH c, collect({p: p, e: e}) AS export_list UNWIND list_slice(export_list, 0, 3) AS r RETURN c, r.e, r.p ORDER BY c.node_id, r.p.node_id \u0026#34;\u0026#34;\u0026#34; ).get_as_networkx() vis.set_labels(weak_econ_g, LABEL_PROPS) vis.plot(weak_econ_g, scale=1.25, seed=3) Dominating vs Weaker Economies # Do dominating economies compete in the same markets as weaker economies? If so, maybe that\u0026rsquo;s why those weaker economies are being pushed to the bottom. ☑️ If not, maybe the products exported by those weaker economies are not the most competitive. Here, we find that, due to the small export diversity, weaker economies are being crushed by dominating economies. Their position of vulnerability comes mostly from geographical isolation and limited area, leading to a lower amount of competition opportunities, where any competitor becomes a risk to the economy.\nBelow, country node classes are visually translated to a colored node border and label text. We assign two classes, for the top and bottom 10 economies, with top economies in the center, and the products and bottom economies in the surrounding area. This forms a star layout, where each arm is a weaker economy or a small cluster of weaker economies.\nWe look at the top 3 most exported products in weaker economies, but relaxing the filter on number of exported products for the weaker economies and looking at more than 3 exported products will reproduce the displayed behavior, with dominating economies still competing for the same products. This doesn\u0026rsquo;t necessarily mean that both dominating and weaker economies produce the same products, as some of them can simply be re-exported.\ndom_vs_weak_econ_g = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (wea)-[we:Exports]-\u0026gt;(p:Product) MATCH (dom)-[de:Exports]-\u0026gt;(p) WHERE dom.node_id IN $dominating_node_ids AND wea.node_id IN $weaker_node_ids WITH wea, we, p, count(DISTINCT dom) AS dom_competitors WHERE dom_competitors \u0026gt; 0 WITH wea, we, p ORDER BY wea.node_id, we.amount_usd DESC SKIP 0 WITH wea, collect({p: p, e: we}) AS export_list UNWIND list_slice(export_list, 0, 3) AS r WITH wea, r.p.node_id AS prod_node_id MATCH (wea)-[we:Exports] -\u0026gt;(prod:Product { node_id: prod_node_id }) MATCH (dom:Country)-[de:Exports]-\u0026gt;(prod) WHERE dom.node_id IN $dominating_node_ids RETURN wea, we, prod, de, dom ORDER BY wea.node_id, prod.node_id, dom.node_id \u0026#34;\u0026#34;\u0026#34;, dict( dominating_node_ids=dom_econ_df.node_id.to_list(), weaker_node_ids=weak_econ_df.node_id.to_list(), ), ).get_as_networkx() node_classes = dict( dominating=dom_econ_df.node_id.to_list(), weaker=weak_econ_df.node_id.to_list(), ) # This adjusts the visualization edge weights # to improve readability for u, v, data in dom_vs_weak_econ_g.edges(data=True): if ( dom_vs_weak_econ_g.nodes[u][\u0026#34;node_id\u0026#34;] in node_classes[\u0026#34;dominating\u0026#34;] and dom_vs_weak_econ_g.nodes[v][\u0026#34;_label\u0026#34;] == \u0026#34;Product\u0026#34; ): data[\u0026#34;vis_weight\u0026#34;] = 1e-5 if ( dom_vs_weak_econ_g.nodes[u][\u0026#34;node_id\u0026#34;] in node_classes[\u0026#34;weaker\u0026#34;] and dom_vs_weak_econ_g.nodes[v][\u0026#34;_label\u0026#34;] == \u0026#34;Product\u0026#34; ): data[\u0026#34;vis_weight\u0026#34;] = 1e-3 vis.set_labels(dom_vs_weak_econ_g, LABEL_PROPS) vis.plot( dom_vs_weak_econ_g, node_classes=node_classes, scale=1.25, seed=5, ) Competition Network # Let\u0026rsquo;s look at the competition network projection for Country nodes and CompetesWith edges. We first install the algo extension for Kuzu and create the compnet projection and NetworkX graph for it.\ntry: conn.execute( \u0026#34;\u0026#34;\u0026#34; INSTALL algo; LOAD algo; \u0026#34;\u0026#34;\u0026#34; ) except Exception as e: print(e) try: conn.execute( \u0026#34;\u0026#34;\u0026#34; CALL drop_projected_graph(\u0026#34;compnet\u0026#34;) \u0026#34;\u0026#34;\u0026#34; ) except Exception as e: print(e) conn.execute( \u0026#34;\u0026#34;\u0026#34; CALL project_graph( \u0026#34;compnet\u0026#34;, {\u0026#34;Country\u0026#34;: \u0026#34;n.country_name_short \u0026lt;\u0026gt; \u0026#39;Undeclared\u0026#39;\u0026#34;}, {\u0026#34;CompetesWith\u0026#34;: \u0026#34;true\u0026#34;} ) \u0026#34;\u0026#34;\u0026#34; ) compnet_g = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (a:Country)-[cw:CompetesWith]-\u0026gt;(b:Country) WHERE a.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND b.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN a, cw, b \u0026#34;\u0026#34;\u0026#34;, ).get_as_networkx() Inspection Functions # The following functions will be useful to plot the cluster and analyze the top exports for a specific cluster ID property:\ndef plot_cluster( prop_name: str, prop_value: int, kind: Literal[\u0026#34;graph\u0026#34;, \u0026#34;map\u0026#34;] = \u0026#34;graph\u0026#34;, ): match kind: case \u0026#34;graph\u0026#34;: compnet_cluster_g = conn.execute( f\u0026#34;\u0026#34;\u0026#34; MATCH (a:Country)-[cw:CompetesWith]-\u0026gt; (b:Country) WHERE a.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND b.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND a.`{prop_name}` = $prop_value AND b.`{prop_name}` = $prop_value RETURN a, cw, b \u0026#34;\u0026#34;\u0026#34;, dict(prop_value=prop_value), ).get_as_networkx() vis.set_labels(compnet_cluster_g, LABEL_PROPS) vis.plot(compnet_cluster_g) case \u0026#34;map\u0026#34;: compnet_cluster_df = conn.execute( f\u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND c.`{prop_name}` = $prop_value RETURN c.country_iso3_code AS iso3_code, c.`{prop_name}` AS `{prop_name}` \u0026#34;\u0026#34;\u0026#34;, dict(prop_value=prop_value), ).get_as_df() vis.plot_map( compnet_cluster_df, code_col=\u0026#34;iso3_code\u0026#34;, class_col=prop_name, ) def trade_per_cluster( prop_name: str, prop_value: int, method: Literal[\u0026#34;imports\u0026#34;, \u0026#34;exports\u0026#34;], n: Optional[int] = None, debug: bool = False, ) -\u0026gt; pd.DataFrame: match method: case \u0026#34;exports\u0026#34;: match_stmt = \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country)-[ie:Exports]-\u0026gt;(p:Product) \u0026#34;\u0026#34;\u0026#34; case \u0026#34;imports\u0026#34;: match_stmt = \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country)\u0026lt;-[ie:Imports]-(p:Product) \u0026#34;\u0026#34;\u0026#34; if n is None: limit_stmt = \u0026#34;\u0026#34; limit_param = dict() else: limit_stmt = \u0026#34;LIMIT $n\u0026#34; limit_param = dict(n=n) query = f\u0026#34;\u0026#34;\u0026#34; {match_stmt} WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND c.`{prop_name}` = $prop_value RETURN p.product_name_short AS product, sum(ie.amount_usd) AS total_amount_usd ORDER BY total_amount_usd DESC {limit_stmt} \u0026#34;\u0026#34;\u0026#34; params = dict(prop_value=prop_value) | limit_param if debug: print_query(query, params) products_df = conn.execute(query, params).get_as_df() return products_df Partner clusters are clusters that import what a cluster is exporting. These are likely to match all clusters due to high connectivity in the world economy, but it might not always be the case, depending on the clustering criteria.\ndef partner_clusters( prop_name: str, prop_value: int, include_self: bool = True, debug: bool = False, ) -\u0026gt; list[int]: include_self_stmt = ( \u0026#34;\u0026#34; if include_self else f\u0026#34;AND c2.`{prop_name}` \u0026lt;\u0026gt; $prop_value\u0026#34; ) query = f\u0026#34;\u0026#34;\u0026#34; MATCH (c:Country)-[:Exports]-(p:Product) MATCH (c2:Country)\u0026lt;-[:Imports]-(p) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND c.`{prop_name}` = $prop_value AND c2.`{prop_name}` IS NOT NULL {include_self_stmt} RETURN DISTINCT c2.`{prop_name}` AS cid \u0026#34;\u0026#34;\u0026#34; params = dict(prop_value=prop_value) if debug: print_query(query, params) result = conn.execute(query, params) partner_cluster_ids = sorted( c[0] for c in result.get_all() ) return partner_cluster_ids The following functions will help us compute the intra-cluster and inter-cluster trade alignments, i.e., self-sufficiency and external competitiveness, based on cluster-aggregated market share.\ndef trade_alignment_by_cluster( prop_name: str, prop_value: int, method: Literal[\u0026#34;intra\u0026#34;, \u0026#34;inter\u0026#34;], ) -\u0026gt; pd.DataFrame: exports_df = trade_per_cluster( prop_name, prop_value, method=\u0026#34;exports\u0026#34;, ) match method: case \u0026#34;intra\u0026#34;: imports_df = trade_per_cluster( prop_name, prop_value, method=\u0026#34;imports\u0026#34;, ) case \u0026#34;inter\u0026#34;: imports_df = [] for partner_cid in partner_clusters( prop_name, prop_value, ): partner_imports_df = trade_per_cluster( prop_name, partner_cid, method=\u0026#34;imports\u0026#34;, ) imports_df.append(partner_imports_df) imports_df = ( pd.concat(imports_df) .groupby([\u0026#34;product\u0026#34;]) .sum() ) case _: raise ValueError( f\u0026#34;method not supported: {method}\u0026#34; ) trade_df = exports_df.merge( imports_df, on=\u0026#34;product\u0026#34;, how=\u0026#34;right\u0026#34; if method == \u0026#34;intra\u0026#34; else \u0026#34;left\u0026#34;, suffixes=(\u0026#34;_exports\u0026#34;, \u0026#34;_imports\u0026#34;), ).fillna(0) trade_df[\u0026#34;sdr\u0026#34;] = ( trade_df.total_amount_usd_exports / trade_df.total_amount_usd_imports ) trade_df = trade_df.sort_values(\u0026#34;sdr\u0026#34;, ascending=False) return trade_df As a score for measuring either self-sufficiency or external competitiveness, we use weighted average of the Supply-Demand Ration (SDR), where weights are the total export amount (USD) for a given cluster.\ndef global_sdr_score( trade_df: pd.DataFrame, eps=1e-9, ) -\u0026gt; float: df = trade_df[~np.isinf(trade_df.sdr)] df[\u0026#34;log_sdr\u0026#34;] = np.log(np.clip(df.sdr, eps, None)) weights = df.total_amount_usd_exports score = expit( (weights * df.log_sdr).sum() / weights.sum() ) return score.item() Competing Communities # Are there any communities representing closely tied competitor clusters? If so, maybe there are specific products per cluster? ☑️ If not, we have a global economy that is fairly homogenous and diverse. For each property computed with the algo extension, we\u0026rsquo;ll alter the corresponding node table, recreating the property each time.\nconn.execute( \u0026#34;\u0026#34;\u0026#34; ALTER TABLE Country DROP IF EXISTS louvain_id; ALTER TABLE Country ADD IF NOT EXISTS louvain_id INT64; CALL louvain(\u0026#34;compnet\u0026#34;) WITH node, louvain_id SET node.louvain_id = louvain_id; \u0026#34;\u0026#34;\u0026#34; ) The Louvain method partitions the network by optimizing modularity, which essentially means it will find the best partition of communities within the graph, a community being a dense subgraph, i.e., a subgraph where connections among members are more frequent than to outside nodes.\ncompnet_louvain_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN c.node_id AS node_id, c.country_name_short AS label, c.louvain_id AS louvain_id \u0026#34;\u0026#34;\u0026#34; ).get_as_df() node_classes = { k: g.node_id.to_list() for k, g in compnet_louvain_df.groupby(\u0026#34;louvain_id\u0026#34;) } vis.set_labels(compnet_g, LABEL_PROPS) vis.plot( compnet_g, node_classes=node_classes, hide_edges=True, ) In complex networks, it is not uncommon for a huge community to emerge, along with a low number of moderately large communities, and then a lot of smaller communities. This behavior is not particularly exacerbated here, but it\u0026rsquo;s still visible. Below, we inspect the community size distribution.\ncomm_sizes_df = ( compnet_louvain_df[[\u0026#34;louvain_id\u0026#34;, \u0026#34;node_id\u0026#34;]] .groupby(\u0026#34;louvain_id\u0026#34;) .count() .rename(columns=dict(node_id=\u0026#34;num_nodes\u0026#34;)) ) comm_sizes_df = comm_sizes_df.reindex( comm_sizes_df.num_nodes.sort_values(ascending=False).index ) comm_sizes_df num_nodes louvain_id 5 67 6 33 4 30 8 26 7 19 1 14 2 14 0 13 3 8 9 7 10 3 fig, ax = plt.subplots(figsize=(18, 3)) comm_sizes_df.plot.bar(xlabel=\u0026#34;Community ID\u0026#34;, rot=0, ax=ax) plt.legend([\u0026#34;No. Nodes\u0026#34;]) plt.show() Let\u0026rsquo;s also take a look at the members of each community, from largest to smallest.\nfor louvain_id in comm_sizes_df.index: display(f\u0026#34;LOUVAIN ID: {louvain_id}\u0026#34;) display( compnet_louvain_df[ compnet_louvain_df.louvain_id == louvain_id ] .drop(columns=\u0026#34;louvain_id\u0026#34;) .sort_values(\u0026#34;label\u0026#34;) ) 'LOUVAIN ID: 5' node_id label 115 116 Albania 205 207 Andorra 114 115 Anguilla 173 174 Austria 126 127 Belarus ... ... ... 49 50 Tunisia 232 234 Turkiye 123 124 Turks and Caicos Islands 131 132 United Kingdom 204 206 United States of America 67 rows × 2 columns\n'LOUVAIN ID: 6' node_id label 209 211 Algeria 171 172 Angola 51 52 Aruba 11 12 Azerbaijan 177 178 Cameroon 54 55 Canada 154 155 Chad 55 56 Colombia 15 16 Democratic Republic of the Congo 220 222 Ecuador 57 58 Egypt 158 159 Equatorial Guinea 100 101 Fiji 75 76 Gabon 223 225 Greenland 211 213 Guyana 17 18 Iran 89 90 Iraq 184 185 Kazakhstan 152 153 Kuwait 135 136 Libya 138 139 Nigeria 139 140 Norway 26 27 Oman 219 221 Republic of the Congo 63 64 Russia 229 231 Sao Tome and Principe 64 65 Saudi Arabia 28 29 South Sudan 7 8 Timor-Leste 31 32 Trinidad and Tobago 112 113 Venezuela 67 68 Yemen 'LOUVAIN ID: 4' node_id label 170 171 Afghanistan 206 208 Australia 12 13 Benin 24 25 Bhutan 207 209 Bolivia 52 53 Burundi 116 117 Central African Republic 101 102 Guinea 163 164 Kyrgyzstan 185 186 Liberia 77 78 Mali 137 138 Mauritania 4 5 Mozambique 165 166 Niger 5 6 Papua New Guinea 78 79 Rwanda 141 142 Senegal 202 204 Sierra Leone 142 143 Solomon Islands 109 110 Somalia 187 188 Sudan 230 232 Suriname 29 30 Syria 124 125 Tajikistan 111 112 Tanzania 110 111 Togo 167 168 Turkmenistan 8 9 US Minor Outlying Islands 99 100 Western Sahara 216 218 Zambia 'LOUVAIN ID: 8' node_id label 96 97 Antarctica 125 126 Bahrain 176 177 Botswana 97 98 China 14 15 Cocos (Keeling) Islands 210 212 Guam 160 161 Heard and McDonald Islands 87 88 Hong Kong 162 163 Israel 134 135 Japan 102 103 Lesotho 195 197 Macao 46 47 Malaysia 62 63 Malta 164 165 Namibia 120 121 Northern Mariana Islands 215 217 Philippines 47 48 Pitcairn 233 235 Samoa 48 49 Singapore 65 66 South Georgia and South Sandwich Islds. 76 77 South Korea 113 114 Taiwan 79 80 Vatican City 95 96 Vietnam 169 170 Wallis and Futuna 'LOUVAIN ID: 7' node_id label 10 11 Argentina 127 128 Belize 217 219 Brazil 34 35 Burkina Faso 156 157 Côte d'Ivoire 94 95 Eswatini 129 130 Ethiopia 38 39 Ghana 86 87 Guatemala 117 118 Honduras 151 152 Kenya 91 92 Malawi 108 109 New Zealand 197 199 Nicaragua 93 94 Paraguay 50 51 Uganda 168 169 Uruguay 146 147 Uzbekistan 68 69 Zimbabwe 'LOUVAIN ID: 1' node_id label 69 70 Bangladesh 16 17 Cabo Verde 143 144 El Salvador 1 2 Falkland Islands 39 40 Haiti 59 60 Kiribati 43 44 Maldives 19 20 Mauritius 74 75 Micronesia 107 108 Nauru 122 123 Seychelles 136 137 Sri Lanka 66 67 Tuvalu 191 192 Vanuatu 'LOUVAIN ID: 2' node_id label 172 173 Armenia 218 220 Chile 181 182 Eritrea 2 3 Georgia 150 151 Jordan 41 42 Lebanon 42 43 Moldova 25 26 Mongolia 225 227 North Macedonia 140 141 Panama 92 93 Peru 32 33 South Africa 190 191 Ukraine 33 34 United Arab Emirates 'LOUVAIN ID: 0' node_id label 0 1 American Samoa 80 81 Antigua and Barbuda 13 14 Barbados 82 83 Curaçao 179 180 Cyprus 159 160 Greece 85 86 Grenada 58 59 Jamaica 119 120 Marshall Islands 214 216 Niue 60 61 Saint Lucia 9 10 Saint Vincent and the Grenadines 35 36 The Bahamas 'LOUVAIN ID: 3' node_id label 212 214 Cambodia 56 57 Comoros 3 4 Laos 90 91 Madagascar 226 228 Montenegro 44 45 Myanmar 227 229 Pakistan 186 187 Palau 'LOUVAIN ID: 9' node_id label 132 133 British Indian Ocean Territory 36 37 Cook Islands 182 183 Faroe Islands 155 156 French Southern and Antarctic Lands 222 224 Guinea-Bissau 161 162 Iceland 201 203 Saint Helena, Ascension and Tristan da Cunha 'LOUVAIN ID: 10' node_id label 192 194 Costa Rica 83 84 Dominica 157 158 Dominican Republic largest_louvain_id = comm_sizes_df.index[0].item() largest_louvain_id 5 smallest_louvain_id = comm_sizes_df.index[-1].item() smallest_louvain_id 10 Community Subgraphs # Community subgraphs illustrates clusters where competition is more prevalent among its members than countries outside of the community. For this graph (our Econ CompNet, or compnet), they are almost always (if not always) complete subgraphs. We can plot any cluster by its ID.\nplot_cluster(\u0026#34;louvain_id\u0026#34;, largest_louvain_id) Community Mapping # Network visualization is not always the best approach to understand your data. This is a good example of this. Since we\u0026rsquo;re working with a complete (or nearly complete) subgraph, looking at relationships is less helpful, but looking at a map for a community is a lot more helpful, as we can see below.\nplot_cluster(\u0026#34;louvain_id\u0026#34;, largest_louvain_id, kind=\u0026#34;map\u0026#34;) Top Exported Products # Is there any export overlap between large and small communities? largest_comm_top_exported = top_frac( trade_per_cluster( \u0026#34;louvain_id\u0026#34;, largest_louvain_id, method=\u0026#34;exports\u0026#34; ), \u0026#34;total_amount_usd\u0026#34;, ) largest_comm_top_exported product total_amount_usd 0 Commodities not specified, according to kind 1.506978e+12 1 Oils petroleum, bituminous, distillates 1.382886e+12 2 Medicaments, doses, nes 1.110865e+12 3 Blood 7.572479e+11 4 Petroleum oils, crude 5.977950e+11 5 Automobiles nes, gas turbine powered 5.668137e+11 6 Gold in unwrought forms 5.482768e+11 7 Automobiles, spark ignition, 1500-3000cc 5.231797e+11 8 Transmit-receive apparatus for radio, TV 4.899130e+11 9 Monolithic integrated circuits, digital 4.366876e+11 10 Trade data discrepancies 3.521978e+11 11 Parts of data processing equipment 3.358087e+11 12 Automobiles, spark ignition, 1000-1500cc 2.657018e+11 13 Fixed wing aircraft, \u0026gt;15,000kg 2.641497e+11 14 Motor vehicle parts nes 2.565987e+11 15 Vaccines, human 2.412159e+11 16 Natural gas, liquefied 2.406005e+11 17 Gold, semi-manufactured forms 2.394518e+11 smallest_comm_top_exported = top_frac( trade_per_cluster( \u0026#34;louvain_id\u0026#34;, smallest_louvain_id, method=\u0026#34;exports\u0026#34;, ), \u0026#34;total_amount_usd\u0026#34;, ) smallest_comm_top_exported product total_amount_usd 0 Instruments for medical science, nes 1.032487e+10 1 Medical needles, catheters 8.305035e+09 2 Trade data discrepancies 7.981437e+09 jaccard_sim( largest_comm_top_exported[\u0026#34;product\u0026#34;], smallest_comm_top_exported[\u0026#34;product\u0026#34;] ) 0.05 Top Imported Products # Is there any import overlap between large and small communities? largest_comm_top_imported = top_frac( trade_per_cluster( \u0026#34;louvain_id\u0026#34;, largest_louvain_id, method=\u0026#34;imports\u0026#34;, ), \u0026#34;total_amount_usd\u0026#34;, ) largest_comm_top_imported product total_amount_usd 0 Petroleum oils, crude 1.933629e+12 1 Commodities not specified, according to kind 1.411868e+12 2 Oils petroleum, bituminous, distillates 1.197450e+12 3 Transmit-receive apparatus for radio, TV 9.829336e+11 4 Medicaments, doses, nes 8.788548e+11 5 Trade data discrepancies 7.574121e+11 6 Gold in unwrought forms 7.455694e+11 7 Blood 6.453376e+11 8 Automobiles nes, gas turbine powered 5.992672e+11 9 Natural gas, as gas 5.284708e+11 10 Parts of data processing equipment 5.119915e+11 11 Automobiles, spark ignition, 1500-3000cc 4.830173e+11 12 Monolithic integrated circuits, digital 4.750667e+11 smallest_comm_top_imported = top_frac( trade_per_cluster( \u0026#34;louvain_id\u0026#34;, smallest_louvain_id, method=\u0026#34;imports\u0026#34;, ), \u0026#34;total_amount_usd\u0026#34;, ) smallest_comm_top_imported product total_amount_usd 0 Oils petroleum, bituminous, distillates 1.373957e+10 1 Commodities not specified, according to kind 7.539036e+09 2 Transmit-receive apparatus for radio, TV 3.136932e+09 3 Automobiles, spark ignition, 1500-3000cc 2.425662e+09 4 Jewellery of precious metal 2.326848e+09 5 Instruments for medical science, nes 2.297871e+09 6 Monolithic integrated circuits, digital 2.243517e+09 7 Maize except seed corn 2.138190e+09 8 Natural gas, liquefied 2.103650e+09 9 Petroleum oils, crude 2.089410e+09 10 Propane, liquefied 2.061530e+09 jaccard_sim( largest_comm_top_imported[\u0026#34;product\u0026#34;], smallest_comm_top_imported[\u0026#34;product\u0026#34;], ) 0.3333333333333333 Trade Alignment # Trade alignment can be used to determine a cluster\u0026rsquo;s self-sufficiency by looking at internal country-country trade, or it can be used to determine a cluster\u0026rsquo;s external competitiveness by looking at inter-cluster country-country trade. We determine both dimensions of trade alignment (intra and inter cluster) based on the supply/demand ratio, more specifically the weighted average of log-SDR, with weights being total amounts (USD) of exports/imports, globally per cluster.\nThis score is scaled to a 0..1 range using a sigmoid transformation, so anything above 0.5 should be good. The log-transformation ensures the distribution is not skewed.\nSelf-Sufficiency # Most communities are self-sufficient or nearly self-sufficient, with only community 5 showing a little more vulnerability.\ncomm_self_sufficiency_df = pd.DataFrame( dict( louvain_id=louvain_id, score=global_sdr_score( trade_alignment_by_cluster( \u0026#34;louvain_id\u0026#34;, louvain_id, method=\u0026#34;intra\u0026#34;, ) ), ) for louvain_id in comm_sizes_df.index ).sort_values(\u0026#34;score\u0026#34;, ascending=False) comm_self_sufficiency_df louvain_id score 9 9 0.985065 2 4 0.970089 5 1 0.959150 8 3 0.939180 10 10 0.895896 1 6 0.869495 4 7 0.860508 6 2 0.742791 3 8 0.644520 7 0 0.564580 0 5 0.493976 colors = comm_self_sufficiency_df.score.apply( lambda s: MPL_PALETTE[0] if s \u0026gt;= 0.5 else MPL_PALETTE[1] ) fig, ax = plt.subplots(figsize=(18, 3)) comm_self_sufficiency_df.plot.bar( x=\u0026#34;louvain_id\u0026#34;, y=\u0026#34;score\u0026#34;, xlabel=\u0026#34;Community ID\u0026#34;, color=colors, rot=0, ax=ax, ) plt.axhline( y=0.5, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([ \u0026#34;Self-Sufficiency Threshold\u0026#34;, \u0026#34;Global Log-SDR Score\u0026#34; ]) plt.show() compnet_louvain_df[compnet_louvain_df.louvain_id == 5] node_id label louvain_id 6 7 Qatar 5 18 19 Lithuania 5 20 21 Portugal 5 21 22 Palestine 5 22 23 British Virgin Islands 5 ... ... ... ... 221 223 Spain 5 224 226 India 5 228 230 Romania 5 231 233 Slovenia 5 232 234 Turkiye 5 67 rows × 3 columns\nExternal Competitiveness # Most communities are not particularly competitive externally, but this was to be expected due to the criteria used to cluster—community dense subgraphs also point to higher internal competition.\ncomm_external_comp_df = pd.DataFrame( dict( louvain_id=louvain_id, score=global_sdr_score( trade_alignment_by_cluster( \u0026#34;louvain_id\u0026#34;, louvain_id, method=\u0026#34;inter\u0026#34;, ) ), ) for louvain_id in comm_sizes_df.index ).sort_values(\u0026#34;score\u0026#34;, ascending=False) comm_external_comp_df louvain_id score 0 5 0.360300 3 8 0.304639 1 6 0.200342 2 4 0.121624 4 7 0.089482 6 2 0.072207 5 1 0.055127 8 3 0.033868 9 9 0.018567 10 10 0.012569 7 0 0.010781 colors = comm_external_comp_df.score.apply( lambda s: MPL_PALETTE[0] if s \u0026gt;= 0.5 else MPL_PALETTE[1] ) fig, ax = plt.subplots(figsize=(18, 3)) comm_external_comp_df.plot.bar( x=\u0026#34;louvain_id\u0026#34;, y=\u0026#34;score\u0026#34;, xlabel=\u0026#34;Community ID\u0026#34;, color=colors, rot=0, ax=ax, ) plt.axhline( y=0.5, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([ \u0026#34;External Competitiveness Threshold\u0026#34;, \u0026#34;Global SDR Score\u0026#34; ]) plt.show() compnet_louvain_df[compnet_louvain_df.louvain_id == 8] node_id label louvain_id 14 15 Cocos (Keeling) Islands 8 46 47 Malaysia 8 47 48 Pitcairn 8 48 49 Singapore 8 62 63 Malta 8 65 66 South Georgia and South Sandwich Islds. 8 76 77 South Korea 8 79 80 Vatican City 8 87 88 Hong Kong 8 95 96 Vietnam 8 96 97 Antarctica 8 97 98 China 8 102 103 Lesotho 8 113 114 Taiwan 8 120 121 Northern Mariana Islands 8 125 126 Bahrain 8 134 135 Japan 8 160 161 Heard and McDonald Islands 8 162 163 Israel 8 164 165 Namibia 8 169 170 Wallis and Futuna 8 176 177 Botswana 8 195 197 Macao 8 210 212 Guam 8 215 217 Philippines 8 233 235 Samoa 8 Weakly Connected Competitors # Strongly connected components in our graph would have capture mutual competition among peers, cyclical or balanced rivalries, or equivalent strategic positions. However, once we removed the \u0026ldquo;Undeclared\u0026rdquo; pseudo-country, we weren\u0026rsquo;t able to find any strongly connected components that were not singletons.\nAs such, we compute the weakly connected components, instead capturing the individual or isolated components of countries competing among themselves, regardless of export amount (which establishes direction, in our graph).\nconn.execute( \u0026#34;\u0026#34;\u0026#34; ALTER TABLE Country DROP IF EXISTS wcc_id; ALTER TABLE Country ADD IF NOT EXISTS wcc_id INT64; CALL weakly_connected_components(\u0026#34;compnet\u0026#34;) WITH node, group_id SET node.wcc_id = group_id; \u0026#34;\u0026#34;\u0026#34; ) compnet_wcc_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN c.node_id AS node_id, c.country_name_short AS label, c.wcc_id AS wcc_id \u0026#34;\u0026#34;\u0026#34; ).get_as_df() node_classes = { k: g.node_id.to_list() for k, g in compnet_wcc_df.groupby(\u0026#34;wcc_id\u0026#34;) } vis.set_labels(compnet_g, LABEL_PROPS) vis.plot( compnet_g, node_classes=node_classes, hide_edges=True, ) As we can see, there a multiple weakly connected competitors, but most of them are single nodes in their own SCC. Other than that, there is a large component of 64 countries, and then two other smaller components with over 20 nodes each, that we\u0026rsquo;ll inspect below.\nwcc_sizes_df = ( compnet_wcc_df[[\u0026#34;wcc_id\u0026#34;, \u0026#34;node_id\u0026#34;]] .groupby(\u0026#34;wcc_id\u0026#34;) .count() .rename(columns=dict(node_id=\u0026#34;num_nodes\u0026#34;)) ) wcc_sizes_df = wcc_sizes_df.reindex( wcc_sizes_df.num_nodes.sort_values(ascending=False).index ) wcc_sizes_df num_nodes wcc_id 0 64 1 28 4 24 5 11 2 9 ... ... 209 1 215 1 226 1 228 1 230 1 68 rows × 1 columns\nwcc_sizes_ord_df = wcc_sizes_df.reset_index(drop=True) wcc_singleton_threshold = ( wcc_sizes_ord_df[wcc_sizes_ord_df.num_nodes \u0026lt;= 1] .index[0] .item() ) fig, ax = plt.subplots(figsize=(30, 5)) wcc_sizes_df.plot.bar(rot=0, ax=ax) plt.axvline( x=wcc_singleton_threshold, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([\u0026#34;Singleton Threshold\u0026#34;, \u0026#34;No. Nodes\u0026#34;]) plt.show() Let\u0026rsquo;s take a look at the members of each weak component, from largest to smallest.\nfor wcc_id in wcc_sizes_df[wcc_sizes_df.num_nodes \u0026gt; 1].index: display(f\u0026#34;WCC ID: {wcc_id}\u0026#34;) display( compnet_wcc_df[compnet_wcc_df.wcc_id == wcc_id] .drop(columns=\u0026#34;wcc_id\u0026#34;) ) 'WCC ID: 0' node_id label 0 1 American Samoa 6 7 Qatar 11 12 Azerbaijan 14 15 Cocos (Keeling) Islands 18 19 Lithuania ... ... ... 211 213 Guyana 215 217 Philippines 221 223 Spain 224 226 India 231 233 Slovenia 64 rows × 2 columns\n'WCC ID: 1' node_id label 1 2 Falkland Islands 16 17 Cabo Verde 19 20 Mauritius 26 27 Oman 39 40 Haiti 43 44 Maldives 47 48 Pitcairn 55 56 Colombia 59 60 Kiribati 66 67 Tuvalu 67 68 Yemen 69 70 Bangladesh 74 75 Micronesia 79 80 Vatican City 102 103 Lesotho 122 123 Seychelles 125 126 Bahrain 132 133 British Indian Ocean Territory 155 156 French Southern and Antarctic Lands 161 162 Iceland 162 163 Israel 176 177 Botswana 177 178 Cameroon 182 183 Faroe Islands 191 192 Vanuatu 201 203 Saint Helena, Ascension and Tristan da Cunha 220 222 Ecuador 223 225 Greenland 'WCC ID: 4' node_id label 4 5 Mozambique 12 13 Benin 38 39 Ghana 50 51 Uganda 52 53 Burundi 73 74 Finland 75 76 Gabon 77 78 Mali 78 79 Rwanda 99 100 Western Sahara 101 102 Guinea 111 112 Tanzania 116 117 Central African Republic 121 122 Sweden 128 129 Switzerland 131 132 United Kingdom 163 164 Kyrgyzstan 165 166 Niger 170 171 Afghanistan 184 185 Kazakhstan 193 195 Ireland 205 207 Andorra 219 221 Republic of the Congo 230 232 Suriname 'WCC ID: 5' node_id label 5 6 Papua New Guinea 8 9 US Minor Outlying Islands 58 59 Jamaica 60 61 Saint Lucia 83 84 Dominica 85 86 Grenada 120 121 Northern Mariana Islands 145 146 Tonga 157 158 Dominican Republic 192 194 Costa Rica 210 212 Guam 'WCC ID: 2' node_id label 2 3 Georgia 3 4 Laos 27 28 North Korea 33 34 United Arab Emirates 44 45 Myanmar 172 173 Armenia 174 175 Belgium 212 214 Cambodia 226 228 Montenegro 'WCC ID: 10' node_id label 10 11 Argentina 86 87 Guatemala 93 94 Paraguay 117 118 Honduras 168 169 Uruguay 197 199 Nicaragua 217 219 Brazil 'WCC ID: 9' node_id label 9 10 Saint Vincent and the Grenadines 56 57 Comoros 90 91 Madagascar 119 120 Marshall Islands 186 187 Palau 'WCC ID: 15' node_id label 15 16 Democratic Republic of the Congo 32 33 South Africa 92 93 Peru 181 182 Eritrea 218 220 Chile 'WCC ID: 42' node_id label 42 43 Moldova 190 191 Ukraine 203 205 Serbia 228 230 Romania 'WCC ID: 22' node_id label 22 23 British Virgin Islands 36 37 Cook Islands 81 82 Bermuda 178 179 Cayman Islands 'WCC ID: 17' node_id label 17 18 Iran 31 32 Trinidad and Tobago 158 159 Equatorial Guinea 'WCC ID: 137' node_id label 137 138 Mauritania 185 186 Liberia 206 208 Australia 'WCC ID: 40' node_id label 40 41 Indonesia 232 234 Turkiye 'WCC ID: 24' node_id label 24 25 Bhutan 216 218 Zambia 'WCC ID: 94' node_id label 94 95 Eswatini 127 128 Belize 'WCC ID: 49' node_id label 49 50 Tunisia 61 62 Morocco 'WCC ID: 110' node_id label 110 111 Togo 141 142 Senegal 'WCC ID: 109' node_id label 109 110 Somalia 187 188 Sudan 'WCC ID: 136' node_id label 136 137 Sri Lanka 143 144 El Salvador 'WCC ID: 21' node_id label 21 22 Palestine 199 201 Poland 'WCC ID: 167' node_id label 167 168 Turkmenistan 207 209 Bolivia 'WCC ID: 160' node_id label 160 161 Heard and McDonald Islands 166 167 Saint Pierre and Miquelon 'WCC ID: 223' node_id label 222 224 Guinea-Bissau 233 235 Samoa largest_wcc_id = wcc_sizes_df.index[0].item() largest_wcc_id 0 smallest_wcc_id = wcc_sizes_df.index[-1].item() smallest_wcc_id 230 Component Subgraphs # plot_cluster(\u0026#34;wcc_id\u0026#34;, largest_wcc_id) Component Mapping # plot_cluster(\u0026#34;wcc_id\u0026#34;, largest_wcc_id, kind=\u0026#34;map\u0026#34;) Top Exported Products # Is there any export overlap between large and small components? largest_wcc_top_exported = top_frac( trade_per_cluster(\u0026#34;wcc_id\u0026#34;, largest_wcc_id, \u0026#34;exports\u0026#34;), \u0026#34;total_amount_usd\u0026#34;, ) largest_wcc_top_exported product total_amount_usd 0 Monolithic integrated circuits, digital 2.880361e+12 1 Petroleum oils, crude 2.815121e+12 2 Oils petroleum, bituminous, distillates 2.185186e+12 3 Commodities not specified, according to kind 1.859842e+12 4 Transmit-receive apparatus for radio, TV 1.629116e+12 5 Trade data discrepancies 9.872075e+11 6 Parts of data processing equipment 7.646240e+11 7 Medicaments, doses, nes 7.542306e+11 smallest_wcc_top_exported = top_frac( trade_per_cluster(\u0026#34;wcc_id\u0026#34;, smallest_wcc_id, \u0026#34;exports\u0026#34;), \u0026#34;total_amount_usd\u0026#34;, ) smallest_wcc_top_exported product total_amount_usd 0 Petroleum oils, crude 24676511.0 jaccard_sim( largest_wcc_top_exported[\u0026#34;product\u0026#34;], smallest_wcc_top_exported[\u0026#34;product\u0026#34;] ) 0.125 Top Imported Products # Is there any import overlap between large and small components? largest_wcc_top_imported = top_frac( trade_per_cluster(\u0026#34;wcc_id\u0026#34;, largest_wcc_id, \u0026#34;imports\u0026#34;), \u0026#34;total_amount_usd\u0026#34;, ) largest_wcc_top_imported product total_amount_usd 0 Petroleum oils, crude 3.075323e+12 1 Monolithic integrated circuits, digital 2.780868e+12 2 Commodities not specified, according to kind 1.510984e+12 3 Oils petroleum, bituminous, distillates 1.420618e+12 4 Transmit-receive apparatus for radio, TV 1.332514e+12 5 Trade data discrepancies 1.248542e+12 6 Medicaments, doses, nes 8.050835e+11 7 Parts of data processing equipment 6.992992e+11 8 Automobiles, spark ignition, 1500-3000cc 6.571986e+11 smallest_wcc_top_imported = top_frac( trade_per_cluster(\u0026#34;wcc_id\u0026#34;, smallest_wcc_id, \u0026#34;imports\u0026#34;), \u0026#34;total_amount_usd\u0026#34;, ) smallest_wcc_top_imported product total_amount_usd 0 Oils petroleum, bituminous, distillates 88922018.0 1 Cargo vessels, not tanker or refrigerated 23292230.0 2 Commodities not specified, according to kind 21288342.0 3 Rice, semi- or wholly-milled 15654678.0 jaccard_sim( largest_comm_top_imported[\u0026#34;product\u0026#34;], smallest_wcc_top_imported[\u0026#34;product\u0026#34;] ) 0.13333333333333333 Trade Alignment # Again, trade alignment can be used to determine a cluster\u0026rsquo;s self-sufficiency by looking at internal country-country trade, or it can be used to determine a cluster\u0026rsquo;s external competitiveness by looking at inter-cluster country-country trade. We determine both dimensions of trade alignment (intra and inter cluster) based on the supply/demand ratio, more specifically the weighted average of log-SDR, with weights being total amounts (USD) of exports/imports, globally per cluster.\nThis score is scaled to a 0..1 range using a sigmoid transformation, so anything above 0.5 should be good. The log-transformation ensures the distribution is not skewed.\nSelf-Sufficiency # Most components are self-sufficient or nearly self-sufficient, with only three of them, components 209, 22 and 196, showing a little more vulnerability.\nwcc_self_sufficiency_df = pd.DataFrame( dict( wcc_id=wcc_id, score=global_sdr_score( trade_alignment_by_cluster( \u0026#34;wcc_id\u0026#34;, wcc_id, method=\u0026#34;intra\u0026#34;, ) ), ) for wcc_id in wcc_sizes_df.index ).sort_values(\u0026#34;score\u0026#34;, ascending=False) wcc_self_sufficiency_df wcc_id score 20 167 0.999962 57 197 0.999932 25 34 0.999926 23 25 0.999721 52 156 0.999568 ... ... ... 2 4 0.586408 0 0 0.518923 63 209 0.464203 9 22 0.300144 56 196 0.280055 68 rows × 2 columns\ncolors = wcc_self_sufficiency_df.score.apply( lambda s: MPL_PALETTE[0] if s \u0026gt;= 0.5 else MPL_PALETTE[1] ) fig, ax = plt.subplots(figsize=(30, 5)) wcc_self_sufficiency_df.plot.bar( x=\u0026#34;wcc_id\u0026#34;, y=\u0026#34;score\u0026#34;, xlabel=\u0026#34;Weak Component ID\u0026#34;, color=colors, rot=0, ax=ax, ) plt.axhline( y=0.5, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([ \u0026#34;Self-Sufficiency Threshold\u0026#34;, \u0026#34;Global SDR Score\u0026#34; ]) plt.show() compnet_wcc_df[compnet_wcc_df.wcc_id == 0] node_id label wcc_id 0 1 American Samoa 0 6 7 Qatar 0 11 12 Azerbaijan 0 14 15 Cocos (Keeling) Islands 0 18 19 Lithuania 0 ... ... ... ... 211 213 Guyana 0 215 217 Philippines 0 221 223 Spain 0 224 226 India 0 231 233 Slovenia 0 64 rows × 3 columns\nExternal Competitiveness # Most components are not particularly competitive externally, even less so than communities, with the large majority having a SDR-based score lower than 0.1.\nwcc_external_comp_df = pd.DataFrame( dict( wcc_id=wcc_id, score=global_sdr_score( trade_alignment_by_cluster( \u0026#34;wcc_id\u0026#34;, wcc_id, method=\u0026#34;inter\u0026#34;, ) ), ) for wcc_id in wcc_sizes_df.index ).sort_values(\u0026#34;score\u0026#34;, ascending=False) wcc_external_comp_df wcc_id score 0 0 0.424935 11 137 0.135049 2 4 0.112296 7 15 0.086413 5 10 0.084617 ... ... ... 59 195 0.000212 21 160 0.000073 67 230 0.000053 43 114 0.000023 26 65 0.000009 68 rows × 2 columns\ncolors = wcc_external_comp_df.score.apply( lambda s: MPL_PALETTE[0] if s \u0026gt;= 0.5 else MPL_PALETTE[1] ) fig, ax = plt.subplots(figsize=(30, 5)) wcc_external_comp_df.plot.bar( x=\u0026#34;wcc_id\u0026#34;, y=\u0026#34;score\u0026#34;, xlabel=\u0026#34;Weak Component ID\u0026#34;, color=colors, rot=0, ax=ax, ) plt.axhline( y=0.5, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([ \u0026#34;External Competitiveness Threshold\u0026#34;, \u0026#34;Global SDR Score\u0026#34; ]) plt.show() compnet_louvain_df[compnet_louvain_df.louvain_id == 0] node_id label louvain_id 0 1 American Samoa 0 9 10 Saint Vincent and the Grenadines 0 13 14 Barbados 0 35 36 The Bahamas 0 58 59 Jamaica 0 60 61 Saint Lucia 0 80 81 Antigua and Barbuda 0 82 83 Curaçao 0 85 86 Grenada 0 119 120 Marshall Islands 0 159 160 Greece 0 179 180 Cyprus 0 214 216 Niue 0 Communities vs Components # By matching the clustering (communities and weak components) with the highest number of clusters, and therefore smaller clusters, to the clustering with the lowest number of clusters, we can run a pairwise cluster comparison:\nWhich countries belong to a community, but not the weak component? Which countries belong to a weak component, but not the community? Which countries belong to both? Is there a particular semantic to these countries? len(wcc_sizes_df), len(comm_sizes_df) (68, 11) NN-Clusters # We compute community to weak component similarities, selecting the nearest-neighbor community for each component. Given the higher number of components when compared to communities, we\u0026rsquo;ll necessarily have repeated nearest-neighbor communities.\ncluster_sim_df = [] for wcc_id, wcc in compnet_wcc_df.groupby(\u0026#34;wcc_id\u0026#34;): for louvain_id, comm in ( compnet_louvain_df.groupby(\u0026#34;louvain_id\u0026#34;) ): cluster_sim_df.append( dict( wcc_id=wcc_id, louvain_id=louvain_id, sim=jaccard_sim(wcc.label, comm.label), ) ) cluster_sim_df = pd.DataFrame(cluster_sim_df) cluster_sim_df = cluster_sim_df.loc[ cluster_sim_df .groupby([\u0026#34;wcc_id\u0026#34;]) .idxmax() .sim ] cluster_sim_df wcc_id louvain_id sim 5 0 5 0.297030 12 1 1 0.354839 25 2 3 0.307692 37 4 4 0.317073 54 5 10 0.272727 ... ... ... ... 693 215 0 0.076923 713 223 9 0.125000 717 226 2 0.071429 729 228 3 0.125000 743 230 6 0.030303 68 rows × 3 columns\nFor example, community 5 matches with 20 different weak components.\ncluster_sim_df.louvain_id.value_counts() louvain_id 5 20 7 10 4 10 2 7 6 6 8 4 1 3 3 3 0 2 9 2 10 1 Name: count, dtype: int64 cluster_sim_df[cluster_sim_df.louvain_id == 5] wcc_id louvain_id sim 5 0 5 0.297030 126 21 5 0.029851 192 40 5 0.029851 225 49 5 0.029851 269 72 5 0.014925 280 84 5 0.014925 313 98 5 0.014925 335 103 5 0.014925 401 114 5 0.014925 423 126 5 0.014925 489 144 5 0.014925 511 147 5 0.014925 544 153 5 0.014925 599 173 5 0.014925 610 195 5 0.014925 632 197 5 0.014925 643 199 5 0.014925 654 201 5 0.014925 676 209 5 0.014925 687 214 5 0.014925 Set Comparison # Let\u0026rsquo;s select a weakest component and retrieve its NN community to compare.\n## comp_wcc_id = largest_wcc_id comp_wcc_id = compnet_wcc_df.loc[ compnet_wcc_df.label == \u0026#34;Australia\u0026#34;, \u0026#34;wcc_id\u0026#34; ].item() comp_comm_id = cluster_sim_df.loc[ cluster_sim_df.wcc_id == comp_wcc_id, \u0026#34;louvain_id\u0026#34;, ].item() comp_wcc_id, comp_comm_id (137, 4) comp_wcc_countries = set( compnet_wcc_df.loc[ compnet_wcc_df.wcc_id == comp_wcc_id, \u0026#34;label\u0026#34; ] ) comp_louvain_countries = set( compnet_louvain_df.loc[ compnet_louvain_df.louvain_id == comp_comm_id, \u0026#34;label\u0026#34; ] ) WCC Exclusive # pd.Series( list(comp_wcc_countries - comp_louvain_countries), name=\u0026#34;country\u0026#34;, ).sort_values().to_frame() country Community Exclusive # pd.Series( list(comp_louvain_countries - comp_wcc_countries), name=\u0026#34;country\u0026#34;, ).sort_values().to_frame() country 5 Afghanistan 26 Benin 20 Bhutan 14 Bolivia 24 Burundi 18 Central African Republic 4 Guinea 1 Kyrgyzstan 23 Mali 8 Mozambique 25 Niger 17 Papua New Guinea 7 Rwanda 21 Senegal 2 Sierra Leone 13 Solomon Islands 0 Somalia 15 Sudan 12 Suriname 10 Syria 19 Tajikistan 6 Tanzania 3 Togo 9 Turkmenistan 11 US Minor Outlying Islands 16 Western Sahara 22 Zambia WCC and Community Overlap # pd.Series( list(comp_wcc_countries | comp_louvain_countries), name=\u0026#34;country\u0026#34;, ).sort_values().to_frame() country 17 Afghanistan 9 Australia 14 Benin 10 Bhutan 5 Bolivia 29 Burundi 26 Central African Republic 2 Guinea 1 Kyrgyzstan 27 Liberia 28 Mali 8 Mauritania 19 Mozambique 13 Niger 6 Papua New Guinea 18 Rwanda 11 Senegal 15 Sierra Leone 4 Solomon Islands 0 Somalia 24 Sudan 23 Suriname 21 Syria 7 Tajikistan 3 Tanzania 16 Togo 20 Turkmenistan 22 US Minor Outlying Islands 25 Western Sahara 12 Zambia Economic Pressure (PageRank) # Economic pressure can easily be measured using PageRank, as it is a converging metric that aggregates the overall incoming competition strength, increasing its value as the contributing competing countries are themselves under economic pressure.\nconn.execute( \u0026#34;\u0026#34;\u0026#34; ALTER TABLE Country DROP IF EXISTS pagerank; ALTER TABLE Country ADD IF NOT EXISTS pagerank DOUBLE; CALL page_rank(\u0026#34;compnet\u0026#34;, maxIterations := 100) WITH node, rank SET node.pagerank = rank \u0026#34;\u0026#34;\u0026#34; ) Most Pressured Countries # most_pressured_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN c.node_id AS node_id, c.country_name_short AS label, c.pagerank AS pagerank ORDER BY c.pagerank DESC LIMIT 25 \u0026#34;\u0026#34;\u0026#34; ).get_as_df() fig, ax = plt.subplots(figsize=(5, 8)) ( most_pressured_df.iloc[::-1] .plot.barh(x=\u0026#34;label\u0026#34;, y=\u0026#34;pagerank\u0026#34;, ax=ax) ) plt.ylabel(None) plt.legend([]) plt.show() Least Pressured Countries # least_pressured_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN c.node_id AS node_id, c.country_name_short AS label, c.pagerank AS pagerank ORDER BY c.pagerank ASC LIMIT 25 \u0026#34;\u0026#34;\u0026#34; ).get_as_df() fig, ax = plt.subplots(figsize=(5, 8)) ( least_pressured_df.iloc[::-1] .plot.barh(x=\u0026#34;label\u0026#34;, y=\u0026#34;pagerank\u0026#34;, ax=ax) ) plt.ylabel(None) plt.legend([]) plt.show() Closing Remarks # Economies are complex systems, and the complex relations between markets can be captured using a graph. Determining which nodes and relationships to model is crucial to interpretation—our graph focused on competition relationships, and so our metrics and partition approaches illustrated this.\nNetwork analysis tools are usually not as exotic as they want to make us believe. Useful graph data science is usually not that complex, particularly now that tooling is widely available, but it can certainly be extremely insightful, specially when the graph is correctly modeled.\nThis is only a small introduction to this topic, using world economy and trade as an example topic, which I have been particularly interested in.\nThe economy and the world overall is suffering. Graphs will help us find solution to complex problems, but it requires the commitment to always ask yourself: could I do this without a graph? When the answer is yes, then you should rethink your approach. If you\u0026rsquo;re not looking at complex relations, you\u0026rsquo;re just doing more of the same.\nBottom line, use graphs and use them correctly.\n","date":"6 August 2025","externalUrl":null,"permalink":"/posts/economic-competition-networks/","section":"","summary":"Summary # In this video, we reproduce the approach that predicts Survivor winners and apply it to Economic Competition Networks to better understand world trade and economic leaders.","title":"Economic Competition Networks","type":"posts"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/economy/","section":"Tags","summary":"","title":"Economy","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/market/","section":"Tags","summary":"","title":"Market","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/network-analysis/","section":"Tags","summary":"","title":"Network-Analysis","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/network-science/","section":"Tags","summary":"","title":"Network-Science","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/world-trade/","section":"Tags","summary":"","title":"World-Trade","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/embeddings/","section":"Tags","summary":"","title":"Embeddings","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/gemma3/","section":"Tags","summary":"","title":"Gemma3","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/graph-rag/","section":"Tags","summary":"","title":"Graph-Rag","type":"tags"},{"content":" Summary # In this video, I\u0026rsquo;ll delve into GraphRAG, learning about KùzuDB, node embeddings vs text embeddings, and LangChain, running on top of Ollama with phi4 and gemma3.\n\u003e What is GraphRAG? # RAG stands for Retrieval Augmented Generation, which means that, instead of simply providing an answer to the user via prompt engineering (i.e., just using an LLM), we also factor in a knowledge base into the process, to provide context and enrich the generated answer. This knowledge base is usually an index that we can search (hence RAG), but it can also be a graph. If we\u0026rsquo;re retrieving from a graph, to produce a context for an LLM, we\u0026rsquo;re doing GraphRAG. That\u0026rsquo;s it.\nGenerally, a GraphRAG task consists of taking a user query, mapping it into a knowledge graph—usually through text embedding—and querying that knowledge graph to build a context around relevant nodes and relationships. That context is then combined with the user query and fed to an LLM to produce a final and improved answer.\nThere also seems to be trend going around that\u0026rsquo;s highlighting \u0026ldquo;context engineering\u0026rdquo; as the hot new skill in in AI—see The rise of \u0026ldquo;context engineering\u0026rdquo; on LangChain\u0026rsquo;s blog, and The New Skill in AI is Not Prompting, It\u0026rsquo;s Context Engineering on Philipp Schmid\u0026rsquo;s blog, who is a Senior AI Relation Engineer at Google DeepMind.\nIf you want to stay on trend and learn something cool, then read bellow on how to use graphs to fetch context, and become better at \u0026ldquo;context engineering\u0026rdquo; right now!\nOur Approach # We approach GraphRAG in an unconventional way. Instead of relying on text embeddings, we add an extra step to the process, so that we can alternatively use node embeddings. I\u0026rsquo;ll explain the reason for this decision in the following paragraphs.\nWhen using text to describe nodes (text embeddings) rather than the neighboring graph structure (node embeddings), the user query, which is also text, can be embedded into the same vector space. This means that ANN (approximate nearest neighbors) is directly used to map the user query into a context subgraph—this is not only efficient, but also open ended—ANN based on text embeddings is able to reach semantically similar text, and therefore its associated nodes, rather than just doing exact matches.\nOn the other hand, when using node embeddings, we\u0026rsquo;ll need an extra step to map named entities in the user query to nodes in the knowledge graph. Only then can we compute ANN for those nodes, working as an expansion stage that builds the context. This provides a higher specificity, since we try to exactly match the entities that the user mentions, but, more importantly, it\u0026rsquo;s actually based on the graph and its structure rather than text.\nWhen working with graphs, always ask yourself whether you could have implemented the same workflow without using a graph structure at all. If the answer is yes, then you\u0026rsquo;re not really taking advantage of your graph. To me, that\u0026rsquo;s just RAG, not GraphRAG. On the other hand, if you find yourself expanding to neighbors, computing shortest paths, clustering nodes, or ranking and selecting nodes based on centrality metrics, then you\u0026rsquo;re definitely doing GraphRAG—and a bit of network science as well.\nArchitecture # We\u0026rsquo;ll be working with music data that includes friendship relations between users in a music platform, as well as user listening profiles pointing to preferred genres and genre-tagged tracks. As such, we are assuming that the system will take a music-related prompt as input. The high level workflow of a GraphRAG system is illustrated below, and then we detail each step for our specific use case.\nStep 1: User Query # The user asks for music recommendations, either mentioning artists, tracks, or genres that they like. Step 2: Graph Retriever # The user query is passed into the Graph Retriever module, which uses an LLM to build a cypher query that matches named entities to nodes in the graph. This will describe the user taste, working as a mini user profile. Step 3: Context Assembler # Each node will have precomputed node embeddings—we\u0026rsquo;ll describe this process later on—which we use to find the approximate nearest neighbors, and extend the mini user profile. For each identified node, we compute a sample of relevant paths to depict context—shortest paths between user profile nodes and neighbor nodes, and random walks from neighbor nodes. Each path is described using its nodes and properties, along with pairwise relationships and their properties, in a format that naturally fits a natural language prompt—it\u0026rsquo;s not valid cypher code, nor it\u0026rsquo;s properly structured text, but rather a semi-structured, human-readable format. Step 4: Final Answer # The context is composed into the original user prompt and a final prompt is produced. The user is presented with the final response generated using the contextualized prompt—this answer should leverage LLM knowledge and the contextual neighborhood of the user\u0026rsquo;s taste according to the knowledge graph, to hopefully provide better or more informed recommendations. Integrating Data as a Graph # We use two Kaggle datasets to build our knowledge graph: Deezer Social Networks and Million Song Dataset + Spotify + Last.fm (MSDSL). If you want to know more about the ETL process, you can read the blog post on Data Lakehouse with dbt and DuckLake, and watch the video about it.\nIn this blog post, we assume that the required node and relationship data is already available as Apache Parquet files that are ready to load into KùzuDB. So, we\u0026rsquo;ll focus on discussing possible integration strategies—direct mapping or integrated mapping—along with their pros and cons.\nDirect Mapping # From an ontological or information preservation perspective, directly mapping the entities in our two datasets to their own nodes would make sense. However, purely from a recommender system\u0026rsquo;s perspective, there isn\u0026rsquo;t a clear advantage to doing this, as long as we can identify where a user is from (e.g., via a node property or a connector node).\nKeeping separate node labels with similar semantics—Deezer User and MSDSL User, or Genre and Tag—would be unhelpful for the recommendation task, as it would increase complexity while offering no real advantage in exchange.\nIntegrated Mapping # Even if Deezer users will never overlap with MSDSL users, we still benefit from combining users as a single entity—and we can keep that additional information as a source property. While the Friend and Likes relationships exist only for Deezer users, and the ListenedTo and Tagged relationships exist only for MSDSL, we\u0026rsquo;re still able to recommend tracks from MSDSL based on liked genres from Deezer users, or recommend new genres to MSDSL users based on the tags of listened tracks and liked genres by Deezer users.\nThis modeling approach is also more than enough to exemplify how GraphRAG can be useful in this task, showing off how different datasets can live in a common space, and how connections are established. The more data you have, the richer your knowledge graph will be, and the more detail you can provide to your users.\nGraph Storage with KùzuDB # For storage, we picked KùzuDB, an embedded graph database solution that, in my opinion, promises to shake the market for the first time since Neo4j and OrientDB were fighting for their place in the community. To me, KùzuDB feels like the DuckDB of graph databases—it\u0026rsquo;s for analytics, it has a similar extensions system and functions API, and it even lets you attach DuckDB, so they play well together.\nI spent a few days bugging the team with a few questions on Discord, after having exchanged a couple of e-mails with them a few months ago, where I tried to understand whether it was worth checking it out or not. So many graph databases have come and gone, but this is not one of them. I have only good things to say about the product and the team—KùzuDB feels technically sound and robust, and the team is approachable, willing to help, and pragmatic. If you\u0026rsquo;re reading this, thanks guys!\nPointers vs Partitions # Let\u0026rsquo;s delve a little into the storage scheme for KùzuDB, putting it in context by looking at its counterpart from Neo4j. While I\u0026rsquo;m partial to Neo4j\u0026rsquo;s approach, and always look for index-free adjacency in graph databases, KùzuDB\u0026rsquo;s columnar approach looks rock solid and ideal for batch operations, as expected from an analytical store.\nNeo4j: Pointers # OLTP graph databases like Neo4j implement index-free adjacency using a pointer-based storage layout that supports chained traversals. When reading a node, we get a pointer to one of its incident relationships. That relationship then provides two other relationship pointers, one to the next relationship from the source node, and another one to the next relationship from the target node. This produces a linked list of incident relationships, making it possible to iterate over all rels for a given node, as well as traverse the graph by following the rels for source/target nodes that match or do not match the origin node, respectively.\nTo illustrate this, let\u0026rsquo;s assume that, starting from n1, we reach (n1)-[r1]-(n2), and from there we can choose to either jump to another rel incident in n1 (origin node) or another rel incident in n2 (neighbor node). When iterating over all incident rels on n1, then we might jump to (n1)-[r2]-(n3), but if we\u0026rsquo;re traversing we\u0026rsquo;ll likely jump to another rel, such as (n2)-[r3]-(n4), which forms the path (n1)-[r1]-(n2)-[r3]-(n4). Each step in a traversal essentially costs one disk seek operation, including node-to-edge as a step, and not accounting for caching.\nKùzuDB: Partitions # KùzuDB approaches storage as an OLAP database, using Node-group based Storage (Issue #1474), which was inspired by the concept of row groups from columnar formats like Apache ORC or Apache Parquet.\nEach node group represents a partition with a given number of nodes or edges with multiple column chunks. A basic column chunk stores a bitmap of null values as well as the values themselves—for string and list columns, offsets are also stored, with each value representing the length of the string or list for each value.\nA node group can also represent a column chunk of relationships (rel-lists). Each rel-lists column chunk contains a non-null column of neighbor IDs, and a header with offsets representing the number of neighbors of each node at the corresponding offset in the equivalent node group of nodes. The rel-lists column chunk can also store relationship properties in the same way as node properties are stored. Bit/byte packing and string dictionary compression techniques are also used at the column level to accelerate querying. And a metadata file is kept in memory to help find column chunks based on the node group ID and column name (that can be a rel-lists).\nWhile not index-free in the sense of pointer-chained traversals, KùzuDB still provides an extremely efficient low-level traversal logic, with the added advantage of columnar storage, which is ideal for analytical tasks—it really is the DuckDB of graph databases.\nTo illustrate the process, let\u0026rsquo;s say that we start from n1. We lookup the corresponding node group for n1 using the metadata file (always available in memory) and, knowing the node group ID, we repeat the process for the rels-list column chunk in the equivalent node group. Through offset arithmetic we can easily obtain the node IDs for neighboring nodes, and repeat the process to keep traversing. Traversing a path like (n1)-[r1]-(n2)-[r3]-(n4) should require at most one disk read per node group, similar in principle to Neo4j\u0026rsquo;s per-hop seeks, though amortized over a batch of nodes and properties. While in Neo4j we traverse one pointer at a time, but filter in-memory, in KùzuDB we traverse by loading node groups, being able to filter out entire rows in a single batch, and skipping decompression for irrelevant rows, or irrelevant neighbor lists in rel-lists.\nI might have missed a few details, or gotten a few of them wrong, but this is not meant as an in-depth comparison of the storage and querying approaches of Neo4j and KùzuDB. It\u0026rsquo;s rather a testament to the effort that the KùzuDB team has put into designing a robust storage schema that is technically sound and efficient.\nTo put this into context, the graph operations we run in this study are for a graph with over 1.15M nodes and 11.62M relationships (see Graph Statistics), which KùzuDB easily supports on a single node with moderately-good specs (24 CPU cores, 24 GiB RAM allocated to WSL2).\nGraph Loading # If you watched Data Lakehouse with dbt and DuckLake, then you already knew that we had gone with the integrated mapping approach, and we have our node and relationship parquet files ready to load into a graph database.\nBelow we will describe the steps taken to load the nodes and edges into KùzuDB. We implemented this in Python, under the KuzuOps class, which run the cypher queries that we describe below.\nCreate Schema # In KùzuDB, we are required to create tables for all of our nodes and relationships. This is what our schema looks like for the Deezer and MSDSL music data. Let\u0026rsquo;s begin with nodes.\nCREATE NODE TABLE User ( node_id INT64, user_id STRING, source STRING, country STRING, PRIMARY KEY (node_id) ); CREATE NODE TABLE Genre ( node_id INT64, genre STRING, PRIMARY KEY (node_id) ); CREATE NODE TABLE Track ( node_id INT64, track_id STRING, name STRING, artist STRING, year INT16, PRIMARY KEY (node_id) ); As you can see, we defined an INT64 node_id property that is unique across all nodes, regardless of their label (i.e., there is no node_id collision for User, Genre and Track). This is set as the primary key for our nodes, which means an index is also created for node_id.\nThen, we create our relationship tables.\nCREATE REL TABLE Friend( FROM User TO User, MANY_MANY ); CREATE REL TABLE Likes( FROM User TO Genre, MANY_MANY ); CREATE REL TABLE ListenedTo( FROM User TO Track, play_count INT32, MANY_MANY ); CREATE REL TABLE Tagged( FROM Track TO Genre, MANY_MANY ); All of our relationships are MANY_TO_MANY, with ListenedTo also storing a play_count property.\nThis produces a graph schema with three node labels and four rel labels, as illustrated in Integrated Mapping.\nImport Nodes and Edges # Our node and edge parquet files are stored in S3 and, while KùzuDB can directly read from S3, it does not support disabling SSL, so we were unable to use an S3-based workflow. In production, it\u0026rsquo;s common for SSL to be enabled, but in a lab or prototyping environment it\u0026rsquo;s not. As such, we simply downloaded each parquet file using boto3 and then ran the following COPY commands (we replace filenames with readable names, but we actually used temporary files).\nWe load nodes as follows:\nCOPY User(node_id, user_id, country, source) FROM \u0026#39;nodes/dsn_nodes_users.parquet\u0026#39;; COPY User(node_id, user_id, source) FROM \u0026#39;nodes/msdsl_nodes_users.parquet\u0026#39;; COPY Track(node_id, track_id, name, artist, year) FROM \u0026#39;nodes/msdsl_nodes_tracks.parquet\u0026#39;; COPY Genre(node_id, genre) FROM \u0026#39;nodes/nodes_genres.parquet\u0026#39;; We load relationships as follows:\nCOPY Friend FROM \u0026#39;edges/dsn_edges_friendships.parquet\u0026#39;; COPY Likes FROM \u0026#39;edges/dsn_edges_user_genres.parquet\u0026#39;; COPY ListenedTo FROM \u0026#39;edges/msdsl_edges_user_tracks.parquet\u0026#39;; COPY Tagged FROM \u0026#39;edges/msdsl_edges_track_tags.parquet\u0026#39;; Graph Statistics # After loading our graph, we computed a few basic statistics using the following query:\nMATCH (n) RETURN \u0026#34;No. \u0026#34; + label(n) + \u0026#34; Nodes\u0026#34; AS stat, count(*) AS val UNION MATCH (n) RETURN \u0026#34;Total No. Nodes\u0026#34; AS stat, count(*) AS val UNION MATCH ()-[r]-\u0026gt;() RETURN \u0026#34;No. \u0026#34; + label(r) + \u0026#34; Rels\u0026#34; AS stat, count(*) AS val UNION MATCH ()-[]-\u0026gt;() RETURN \u0026#34;Total No. Rels\u0026#34; AS stat, count(*) AS val; Statistic Value No. User Nodes 1,105,921 No. Genre Nodes 171 No. Track Nodes 50,683 Total No. Nodes 1,156,775 No. Friend Rels 846,915 No. Likes Rels 880,698 No. ListenedTo Rels 9,711,301 No. Tagged Rels 185,313 Total No. Rels 11,624,227 Computing Node Embeddings # KùzuDB has a vector extension that supports HNSW indexing for vectors, similar to Pinecone, Weaviate, or pgvector. It supports semantic search via ANN, which we\u0026rsquo;ll use on the Graph Retriever component to establish a context based on graph paths.\nWe precompute node embeddings based on a PyTorch implementation—see the graph.embedding module in the datalab repo. We use a simplified version of the Fast Random Projection (FastRP) algorithm, where the $L$ component corresponds directly to the node degrees, the $R$ component samples from a normal distribution, and a Multi-Layer Perceptron (MLP) is applied on top of the fixed FastRP embeddings to introduce non-linear activations, enabling the model to learn complex, non-linear mappings for downstream tasks.\nThis implementation can be run for a graph (e.g., music_taste) by calling the following command:\ndlctl graph compute embeddings \u0026#34;music_taste\u0026#34; \\ -d 256 -b 9216 -e 5 This will iterate over batches of 9216 nodes and compute embeddings of dimension 256 over 5 epochs, which are stored in the embedding property of each node. Batching is handled by the KuzuNodeBatcher class that we implement, which relies on the query_nodes_batch and query_neighbors methods from the KuzuOps class.\nThe first method implement the following cypher query for paginating nodes:\nMATCH (n) RETURN n.node_id AS node_id ORDER BY n.node_id SKIP $skip LIMIT $limit And the second method loads the source and target node IDs for all outgoing relationships starting any of the nodes in the batch:\nMATCH (n)--\u0026gt;(m) WHERE n.node_id IN CAST($nodes AS INT64[]) RETURN n.node_id AS source_id, m.node_id AS target_id ORDER BY source_id, target_id We can then create the HNSW indexes, dropping any existing indexes automatically, by running:\ndlctl graph reindex \u0026#34;music_taste\u0026#34; To make this possible, the previous command relies on show_tables(), table_info(), show_indexes(), drop_vector_index() and create_vector_index(), all of which are individually called and arguments are passed from functions like show_tables() to table_index() strictly via f-strings—since variables are not supported with CALL, this needs to be done programmatically. Check out the full reindex_embeddings methods to learn the details.\nThe whole process takes approximately 20m to run for the music_taste graph.\nGraphRAG Chain # We use two LLM models to support our operations: phi4 and gemma3. Specifically, we rely on phi4 for tasks that involve following instructions and generating code (cypher), and we use gemma3 to produce user-facing text (the final answer). We tested gemma3 as a general model for all required tasks, replacing phi4 in code generation, but it struggled to generate valid cypher code—syntactically and conceptually. The same goes for several other models as well—phi4 seems to produce the best outcome when it comes to code generation (pending formal evaluation, of course, but I\u0026rsquo;m vibing right now).\nWe implemented GraphRAG using LangChain, as a Runnable, taking advantage of langchain-kuzu by partially reusing some of its components, like the KUZU_GENERATION_PROMPT template, or KuzuGraph for getting a schema to feed to the prompt and to handle graph querying.\n📌 Note\nWhen we decided to implement our workflow using LangChain, we also looked into LangGraph, to determine whether it could be useful when working with Graph RAG.\nWhile LangGraph is quite an interesting framework, designed for the orchestration of stateful agents, it does not provide specific tooling for working with graphs—think about graphs in the sense of TensorFlow computational graphs, not graphs like knowledge graphs or social networks. LangGraph supports the integration of LLMs, tools, memory, and other useful features, by settings the conditions under which these components interact.\nSince we didn\u0026rsquo;t need to setup something at this complexity level, we did not use LangGraph here. If you\u0026rsquo;re focusing on stateless Graph RAG, then LangChain is really all you need. In the future, however, we might explore LangGraph, and add state to the implementation we describe here.\nBelow you\u0026rsquo;ll find a diagram detailing the how the Graph RAG chain works, and covering the three major components: Graph Retriever, Context Assembler, and Answer Generator. The floating rectangles on top, with a filling, are the legend. Each small rectangle inside the three major components is a part of the RunnableSequence that makes the component. The code that implements the diagram is available under the graph.rag module—symbol names in the code should match the names in the small rectangles below.\nGraph Retriever # The Graph Retriever component extracts and maps named entities to nodes in the graph. It takes the user_query as input, which will be passed to the entities_prompt via the entities_prompt_to_kuzu_inputs step.\nentities_prompt # At first, we tried a zero-shot prompt, but the only way we got a consistent outcome was by providing it an explicit example. Turning our prompt into one-shot was enough to get this to work for our specific graph. While this might not be generalizable—the example we provide is specific to our graph—it is enough to demonstrante an end-to-end pipeline, and it is also closer to what a real-world implementations would do, which are usually designed for a specific use case. This template expects only that the {user_query} slot is replaced.\nYou are an AI assistant that extracts entities from a given user query using named entity recognition and matches them to nodes in a knowledge graph, returning the node_id properties of those nodes, and nothing more. Input: User query: a sentence or question mentioning entities to retrieve from the knowledge graph. Task: Extract all relevant entities from the user query as nodes represented by their node_id property. Rules: - Only use node properties defined in the schema. - Use exact property names and values as extracted from the user query. - If a property value is not specified, do not guess it. - Ignore user query requests, and just return the node_id property for nodes matching named entities explicitly mentioned in the user query. - Do not make recommendations. Only return the node_id properties for extracted entities that have a node in the graph. Example: If the user mentions Nirvana and there is an artist property on a Track node, then all nodes matching Nirvana should be retrieved as follows: MATCH (t:Track) WHERE LOWER(t.artist) = LOWER(\"Nirvana\") RETURN t.node_id AS node_id; If, in addition to Nirvana, the user algo mentions the grunge genre, and there is a genre property of a Genre node, then all nodes matching grunge should be added to be previous query as follows: MATCH (t:Track) WHERE LOWER(t.artist) = LOWER(\"Nirvana\") RETURN t.node_id AS node_id UNION MATCH (g:Genre) WHERE LOWER(g.genre) = LOWER(\"grunge\") RETURN g.node_id AS node_id User query: \"{user_query}\" --- Here are the node_id properties for all nodes matching the extracted entities: [Your output here] langchain-kuzu # This is a LangChain integration for KùzuDB that helps you query the graph, automatically taking into account its schema, through this prompt (directly extracted from langchain-kuzu):\nYou are an expert in translating natural language questions into Cypher statements. You will be provided with a question and a graph schema. Use only the provided relationship types and properties in the schema to generate a Cypher statement. The Cypher statement could retrieve nodes, relationships, or both. Do not include any explanations or apologies in your responses. Do not respond to any questions that might ask anything else than for you to construct a Cypher statement. Task: Generate a Cypher statement to query a graph database. Schema: {schema} The question is: {question} This means that the final prompt that we pass to our LLM is this prompt with the schema slot replaced by the return value of graph.get_schema—from KuzuGraph, available after calling graph.refresh_schema()—and the {question} slot replaced by our entities_prompt prompt template.\nCode LLM and KG Query # The previously generated prompt is passed to the phi4 LLM model, running on a local instance of Ollama—this requires 8 GiB VRAM—producing cypher code that will match named entities, extracted from the original user query, to nodes in the knowledge graph.\nAs an example, let\u0026rsquo;s consider the following user query:\nIf I like metal artists like Metallica or Iron Maiden, but also listen to IDM, what other artists and genres could I listen to? The cypher output produced by the code LLM will look something like this:\nMATCH (g:Genre) WHERE LOWER(g.genre) = LOWER(\u0026#34;metal\u0026#34;) RETURN g.node_id AS node_id UNION MATCH (t:Track) WHERE LOWER(t.artist) = LOWER(\u0026#34;Metallica\u0026#34;) OR LOWER(t.artist) = LOWER(\u0026#34;Iron Maiden\u0026#34;) RETURN t.node_id AS node_id UNION MATCH (g:Genre) WHERE LOWER(g.genre) = LOWER(\u0026#34;idm\u0026#34;) RETURN g.node_id AS node_id By calling query_graph(shuffle=True, limit=100), we produce a runnable that will use KuzuGraph to run the generate cypher code, shuffling the output data frame and returning only the first 100 rows—if the user mentions more than 100 entities in the query, this will introduce a cap and some run time predictability.\nContext Assembler # The Context Assembler component expands source nodes to nearest neighbors, producing additional context to extend the prompt. It takes the entities data frame of nodes as input, computing nearest-neighbors and finding relevant paths to produce a context. The idea is that this context will guide the LLM into providing a better answer.\nApproximate Nearest Neighbors # We use ANN to build a context with the top $k$ nodes that are the most similar overall to our source nodes (i.e., directly representing entities mentioned in the user query—for example, if an artist is mentioned, then the tracks for that artist will directly represent it). When a node appears multiple times, for a being NN with multiple source nodes, then the average distance is used to rank that node. Source nodes will be added to an exclusion list, so they will never be returned as a NN, and nodes are only considered up to a maximum distance, so we might return less than $k$-NN.\nComputing Relevant Paths # Once we return the combined $k$-NN, we compute two types of paths to produce a context:\nSample of shortest paths between each node representing an entity in the user query, and its nearest neighbors. Random length paths departing from nearest neighbors, as a way to profile the neighbors. Think of individual relationships in paths as having the value of sentences—e.g., \u0026lsquo;John, a Deezer user, listened to Metallica\u0026rsquo;s Nothing Else Matters track 5 times\u0026rsquo; is represented in our graph by (u:User)-[:ListenedTo]-\u0026gt;(t:Track). While we could convert this into natural language, the LLM already has the ability to understand structured languages like cypher. As such, we use a minimal semi-structured representation to list node properties, and relationships from the selected paths:\nNodes: Track(node_id=175760, track_id=TRLOVTV128F92E846E, name=Wrathchild, artist=Iron Maiden, year=1981) Track(node_id=145255, track_id=TRVBEWU128F422E551, name=The Secrets Of Merlin, artist=Grave Digger, year=1999) Genre(node_id=1156680, genre=Indie Rock) Track(node_id=188155, track_id=TRGNOEX128F4265F3E, name=Snap Your Fingers, Snap Your Neck, artist=Prong, year=2014) ... User(node_id=56121, user_id=36144, source=Deezer, country=RO) User(node_id=114060, user_id=3007, source=Deezer, country=HU) Genre(node_id=1156733, genre=Metal) Genre(node_id=1156613, genre=Thrash Metal) Relationships: ({ node_id: 175888 })-[:Tagged]-({ node_id: 1156613 }) ({ node_id: 1156613 })-[:Tagged]-({ node_id: 192487 }) ({ node_id: 113118 })-[:Likes]-({ node_id: 1156680 }) ... ({ node_id: 175760 })-[:ListenedTo {play_count=3}]-({ node_id: 200466 }) ({ node_id: 175748 })-[:Tagged]-({ node_id: 1156613 }) ({ node_id: 1156733 })-[:Likes]-({ node_id: 122589 }) ... ({ node_id: 53808 })-[:Friend]-({ node_id: 114060 }) ... Answer Generation # The text output from the previous component will be fed to the chat LLM, as context within its prompt, and a final answer will be produced by the gemma3 model running on the local instance of Ollama:\nBased on your interest in metal artists like Metallica and Iron Maiden, as well as your enjoyment of IDM (Intelligent Dance Music), you might appreciate exploring a blend of genres that incorporate complex rhythms, intricate compositions, and experimental sounds. Here are some suggestions for artists and genres to explore: #### Artists: 1. **Meshuggah**: Known for their technical proficiency and polyrhythms, Meshuggah blends elements of metal with progressive and avant-garde influences. 2. **Opeth**: This band combines death metal with progressive rock and folk music, offering complex compositions that might appeal to your taste in IDM. 3. **Gojira**: With a focus on environmental themes and technical musicianship, Gojira's sound is both heavy and intricate. 4. **Tool**: Known for their complex rhythms and philosophical lyrics, Tool blends metal with progressive rock and experimental elements. 5. **Cynic**: A band that merges death metal with jazz fusion and ambient music, creating a unique and cerebral listening experience. #### Genres: 1. **Progressive Metal**: This genre often features complex song structures, time signature changes, and intricate instrumental work, similar to the complexity found in IDM. 2. **Technical Death Metal**: Known for its fast tempos, complex guitar riffs, and precise drumming, this subgenre offers a challenging listening experience. 3. **Avant-Garde Metal**: This genre pushes the boundaries of traditional metal with experimental sounds and unconventional song structures. 4. **Math Rock**: Characterized by odd time signatures and syncopated rhythms, math rock shares some similarities with IDM in terms of complexity and structure. #### Additional Recommendations: - **Djent**: A subgenre of progressive metal known for its heavy use of palm-muted, low-pitched guitar riffs and complex rhythms. - **Neoclassical Metal**: Combines elements of classical music with metal, often featuring virtuosic guitar solos and orchestral arrangements. Exploring these artists and genres can provide a rich listening experience that bridges your interests in metal and IDM. And that\u0026rsquo;s it! There\u0026rsquo;s one way to do GraphRAG while actually taking advantage of the graph structure, despite its lower efficiency.\nFinal Remarks # Keep in mind that this was an experiment! For production, I would still rely on text embeddings for to compute node vectors. This will enable us to directly embed a user query into the same vector space, and match it to nodes in the knowledge graph, while seamlessly expanding the context to nodes other than the named entities—we match semantically similar nodes rather than the exact entities mentioned in the user query It\u0026rsquo;s a lot less overhead, if we rely purely on text.\nWhen running graph algorithms based on the NN nodes is required—e.g., traversals or clustering that dynamically depend on those source nodes—then stick with GraphRAG. When you simply don\u0026rsquo;t need to run computations on the graph, then just use plain RAG based on a vector store, although we KùzuDB you essentially get both.\nKùzuDB Consumer Notes # To close this study, I\u0026rsquo;ll share a few of my notes, that I took as I was working on this project, starting with a few quick highlights on data types and functions, and then providing: a wishlist, with features I missed or would like KùzuDB to support; a snags list, with small annoyances that I hit during development; and a bugs list, with a few issues I found, some of them already resolved, while others that might require further investigation.\nData Types and Functions # KùzuDB provides several specialized data types and functions, some of them similar to DuckDB, while others unique to kuzu or specific to graphs, Below, I list some of my favorites, as I went through the docs:\nLists – with arrays being a special case of lists. Arrays – focusing on vector distance and similarity functions. Maps and Structs – maps/dictionaries, with added access options via struct operators (i.e., my_map.my_field). Recursive Relationships – node, edge and property accessors for paths, but also functions to test the trail or acyclic properties of paths, or to measure length and cost. Text – several standard text utilities (e.g., initcap for title case), also provides a levenshtein implementation for string similarity. Unions – makes it possible for multiple data types to be stored in the same column. Wishlist # Integration with dbt 🙏🏻 — there is already a dbt-duckdb, so this would fit nicely with the workflow (DuckDB for ETL and analytics, and KuzuDB for graph data science). No support for S3_USE_SSL, which makes it hard to access object storage setup for testing (had to create a boto3 wrapper based on temporary files). In kuzu CLI, a query terminating with ; should always run when RETURN is pressed, like DuckDB does, even in :multiline mode. And :singleline mode should actually be single line—pasting might convert new lines to spaces. Alternatively, there could be a secondary shortcut, like Shift+Enter, that would run the query regardless of where the cursor is. Having better schema introspection for edge tables with table_info() would be useful—specifically, showing information about FROM and TO. Calling functions using variables and parameters instead of just literals would also be appreciated, e.g., table_info(). Altering a column\u0026rsquo;s type is essential, for a schema-full graph database—a practical example was when creating a vector index over a DOUBLE[], without a fixed dimension, and it wouldn\u0026rsquo;t work. It would be nice if KùzuDB Explorer supported multitenancy—e.g., I\u0026rsquo;d like to be able to provide a read-only graph catalog to my data scientists. A catalog of node embedding algorithms would be appreciated as well—I had to implement my own Fast Random Projection. I didn\u0026rsquo;t use PyG, but considered it—something to look forward to, in the future, for sure. A rand() cypher function and a random walk algorithm, maybe using pattern matching—both are essential for sampling the graph in different ways. Random walks need to be efficient, so implementing this using cypher, in python, is not the best idea, unless absolutely necessary. This is a fundamental feature for an OLAP graph database! Snags # While having NODE and REL tables with a fixed schema is interesting, this also limits operations a bit. For example, we cannot create an embeddings index over nodes of different types, as KùzuDB does not support multi-labelled nodes. Having to create individual indexes still works, but it makes it harder to generalize APIs—e.g., in Neo4j I\u0026rsquo;d use something like a base node label where all nodes of this type were expected to have a specific set of properties, like embedding—but, on Kuzu, I\u0026rsquo;ll need to know all relevant node types and also create and query $N$ separate vector indexes, one for each node type. \u0026ldquo;Current Pipeline Progress\u0026rdquo; stuck at 0% until completed for many cases—tested in KùzuDB Explorer, e.g., CALL create_vector_index(...). This is not as useful as it could be—it needs more granularity. Creating vector indexes requires fixed size arrays, created with DOUBLE[n] or FLOAT[n], but this is not clear from the docs. Spent a couple of hours computing embeddings only to find they couldn\u0026rsquo;t be indexed—also, the error wasn\u0026rsquo;t clear, telling me it only worked with DOUBLE[] or FLOAT[], which I was using, but not with fixed size. Functions like show_indexes() have column names with a space—why not just use an underscore, instead of making me use backticks? There is no real support for subqueries—something like CALL { ... } in Neo4j. Bugs # When SKIP is over the maximum number of records, the query will never return and a CPU core will be maxed out ad aeternum. There was a bug with COPY, when running twice for the same table, with different data. This was reported via Discord, and the team promptly confirmed it was in fact a bug and fixed it (PR #5534). When creating a vector index, then dropping it and recreating, it will sometimes fail with what seems to be an upper/lower-case related bug, e.g.: Error: Binder exception: _1_genre_embedding_idx_UPPER already exists in catalog According to one of the devs, a workaround is to run CALL enable_internal_catalog=true; and then manually drop any tables that should have been dropped along with the index. This unblocked me, but the issue was also reported and should be fixed in a future version—I was using 0.10.0. For UNION queries, the final ORDER statement should affect the global result, not just the last statement—either that, or add support for subqueries. ","date":"15 July 2025","externalUrl":null,"permalink":"/posts/graphrag-with-kuzudb/","section":"","summary":"Summary # In this video, I\u0026rsquo;ll delve into GraphRAG, learning about KùzuDB, node embeddings vs text embeddings, and LangChain, running on top of Ollama with phi4 and gemma3.","title":"GraphRAG with KùzuDB","type":"posts"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/kuzudb/","section":"Tags","summary":"","title":"Kuzudb","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/langchain/","section":"Tags","summary":"","title":"Langchain","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/neo4j/","section":"Tags","summary":"","title":"Neo4j","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/phi4/","section":"Tags","summary":"","title":"Phi4","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"Rag","type":"tags"},{"content":" Summary # Did you know you can rely on conventional commits, and a simple git branching workflow, to automate semantic releases for your Python projects, using GitHub Actions?\nConventional commits can help you standardize your commit messages and improve the readability of your git history, but they can also be used to automate releases by bumping up version components (MAJOR.MINOR.PATCH) based on the content of your commit messages, since the previous release, as identified by a git tag (e.g., v0.1.0). This is a must-have for any self-respecting developer, who maintains an evolving codebase, even when solo coding.\nRead below, if you want to know how to \u0026lsquo;set it and forget\u0026rsquo;, adding automated semantic releases to your git repo, so you won\u0026rsquo;t ever have to handle it yourself, or manually trigger a workflow to do a release.\n\u003e Conventional Commits # Conventional commits is \u0026ldquo;a specification for adding human and machine readable meaning to commit messages\u0026rdquo;. This means that it will, not only, make it easier for you and other coders to navigate through your repo history, but also enable software tools to advantage of this standard—this is what makes semantic release possible.\nIn the table below, there is an overview of conventional commits, based on message types from the Angular convention, which is widely adopted—used by commitlint, Dependabot, etc. I also describe my personal logic for using each message type, and what version component it affects.\nThe conventional commits spec provides a well-defined template for commit messages, but, at the same time, it\u0026rsquo;s quite loose regarding which message types to use, relying mostly on the Angular convention, softly introduced in the spec as an example tied to commitlint. This is why I believe further comment is required, and I would go as far as to proposing the next revision to conventional commits should include a consensual list of valid, non-overlapping, message types.\nCheck out the following table for my comments on this.\nType Usage Version \u0026lt;type\u0026gt;! The most common trigger for breaking changes and major releases is feat!. I usually don\u0026rsquo;t use BREAKING CHANGE in the message body, since I might want to do a MAJOR bump up regardless of whether there is a breaking change (e.g., when reaching the next maturity level).\nIt\u0026rsquo;s less likely that other types will introduce breaking changes, but it can still happen. For example, a chore!(deps) might update a dependency that changed its output file format and provides no backward compatibility from that point on.\nI know I\u0026rsquo;m being opinionated, but, personally, I avoid breaking changes at all cost in my software. This is why I break convention here. I prefer the idea of \u0026lsquo;maturity level\u0026rsquo; to \u0026lsquo;breaking change\u0026rsquo;, and maybe we should discuss this for future versions of the conventional commits spec. MAJOR feat This is just a regular feature, large or small, added to your software.\nWe avoid constant version bump ups by not having a rolling release here (i.e., avoid using only the main branch—see bellow for our proposed minimal git branching workflow. MINOR fix This represents a bug fix—that\u0026rsquo;s what patches mostly are.\nSometimes we might consider a change in coding logic to be a kind of fix, but that\u0026rsquo;s a refactor, not a fix. Be critical about it. PATCH perf Performance improvements are tagged with perf, and these do represent a patch—rightfully so, in my opinion.\nThis is also the default for python-semantic-release when commit_parser is set to conventional (also the default). PATCH chore This is not a part of the Angular convention, but it\u0026rsquo;s still supported by commitlint, and I prefer to use it with a scope instead of the build type—e.g., chore(dep). none refactor Should be strictly used to signal code rewritings that change nothing besides structure or naming (i.e., code might be reorganized into functions, separate modules, etc., and these might be renamed, but no fixes, features, or performance improvements are to be added in a refactor).\nI use this a lot, as I tend to begin with a single file per Python package, and then refactor it into multiple modules (i.e., separate files) as the code grows.\nIn the course of a refactor, if I find a bug, I usually commit all other files first with refactor and then commit that specific file using fix . Similarly, I give priority to feat over fix. This avoids having commits with multiple message types, which break the convention. It\u0026rsquo;s best to avoid this workflow altogether, as much as possible, but, you know, stuff happens. none style Any purely cosmetic changes, like reformatting or theming, should be tagged with the style type.\nI rarely use this, as I tend to just enable automatic formatting, on save, with black. An obvious application of style in this context might include switching from black to another formatting approach, or switching to lower case SQL strings, when you had been using upper case before. It might also include fringe cases, like mixing different line endings by mistake, when coding in a different OS. It should also be used when visually changing components, for example based on CSS. none test Anything done on tests is tagged with test.\nRegardless of whether the commit is for a new test, a bug fix inside a test, a refactoring within tests, I always use this tag for my tests-related commits. none ci Used when modifying GitHub Actions and similar workflows. This includes any CI/CD configuration files.\nI mostly use it when changing files inside the .github directory, as I haven\u0026rsquo;t found any other case for my personal repos yet. I find it awkward that the type isn\u0026rsquo;t named with CD as well, but maybe I\u0026rsquo;m missing something—why only CI? none docs Any update to documentation, be it files like README.md or docstrings inside your code, should be tagged docs.\nAnother use case is when you produce web pages with the documentation for your project (e.g., building and updating the /docs directory that\u0026rsquo;s being served as GitHub Pages). none build It can be used to signal any change to the build system, including dependencies, or changes to a docker image.\nI personally don\u0026rsquo;t use it all, preferring chore instead—e.g., chore(deps) or chore(docker). What other non-build-related chores would you have? It\u0026rsquo;s just too tiny of a use case to consider using another type, in my view. none revert When you run git revert, make sure to edit the message so that it uses the revert type. This is not the default, and I\u0026rsquo;ve broken this rule a few times, but I will stick to it in the future.\nA revert might also imply a rollback in version, but this usually requires manual input. Since it\u0026rsquo;s not automated, I consider that the version is unaffected for revert commits. none Git Branching Workflow # In order to avoid a constant trigger of the semantic release process, we bundle all commits, to be considered for a potential release, in a separate branch called dev. Then, we use merge commits to merge to main. The process is simple and it looks something like this:\nIn a scenario where multiple contributors exist, it might make sense to use multiple feat/* branches as well, that would merge to dev. For our particular case, however, keeping only two branches also enables us to keep a tidy git history, without loss of information—if we used squash merging, we\u0026rsquo;d lose individual commits, and, if we used rebase merging, we\u0026rsquo;d lose the branch of origin instead.\nSemantic Release # Below, you\u0026rsquo;ll find a few examples of how the proposed git branching workflow will affect version updates when using semantic release.\nMINOR Release # The following diagram shows an example of a MINOR version bump up. We branch from main into dev and create three commits, a feat, a fix, and a docs update. We then merge to main and a semantic release happens, bumping up the version from v0.1.0 to v0.2.0. Out of the three commits, only feat or fix could influence version bump up, but feat takes precedence over fix, so we only bump up the MINOR.\ngitGraph commit id: \" \" commit id: \"0.1.0\" tag: \"v0.1.0\" branch dev checkout dev commit id: \"feat: tiny feature\" type: HIGHLIGHT commit id: \"fix: small bug\" commit id: \"docs: update readme\" checkout main merge dev commit id: \"0.2.0\" tag: \"v0.2.0\" commit id: \" \" PATCH Release # Below you\u0026rsquo;ll find two examples, one where there is no version bump up, and another one where PATCH is bumped up. The first merge to main includes only two commits with types chore and docs—these do not trigger a version update (see the table above). Then, we keep working on dev and create a fix commit, with no other commits—this triggers a PATCH bump up from v0.2.0 to v0.2.1.\ngitGraph commit id: \" \" commit id: \"0.2.0\" tag: \"v0.2.0\" branch dev checkout dev commit id: \"chore(deps): bump up\" commit id: \"docs: install instructions\" checkout main merge dev checkout dev commit id: \"fix(graph.ops): embedder\" type: HIGHLIGHT checkout main merge dev commit id: \"0.2.1\" tag: \"v0.2.1\" commit id: \" \" MAJOR Release # Finally, we have an example of a MAJOR bump up, which, according to conventional commits, should only happen when there is a breaking change. In the diagram below, this is identified by feat!, with BREAKING CHANGE being merely decorative, as it\u0026rsquo;s not a part of the message body—were we to remove the ! and a MINOR bump up would occur instead.\ngitGraph commit id: \" \" commit id: \"0.2.1\" tag: \"v0.2.1\" branch dev checkout dev commit commit commit id: \"feat!: BREAKING CHANGE\" type: HIGHLIGHT commit checkout main merge dev commit id: \"1.0.0\" tag: \"v1.0.0\" commit id: \" \" Release Automation # We use python-semantic-release to automate version tagging and release creation on GitHub. This tool will decide, based on your git log, which part of the semantic version to bump up—MAJOR.MINOR.PATCH—as long as you follow conventional commits.\nGitHub Actions # We automate semantic releases using GitHub Actions, based on a very simple workflow that we break down next.\nThe following block creates a Git Workflow named \u0026ldquo;Release\u0026rdquo; that activates on push to main, which means that, once we merge dev into main, it will be triggered. Since the workflow will need to create a commit, push it to main, and tag it with the correct version (e.g., v0.2.0), we also need to give it write permissions—this is a better approach than setting \u0026ldquo;Read and write permissions\u0026rdquo; for all actions on your GitHub repo.\nname: Release on: push: branches: - main permissions: contents: write jobs: release: runs-on: ubuntu-latest steps: ... The first step on the release job is to checkout the repo with fetch-depth: 0 to make sure that all history will be fetched, including previous tags, which are factored into the release logic.\n- uses: actions/checkout@v4 with: fetch-depth: 0 Then, we setup uv, to help ups run semantic-release via uvx. We could have, instead, used the official GHA provided as part of python-semantic-release, but, since that required a Docker image to be built on each run, we decided to use this simpler and faster approach instead.\n- name: Set up uv and python uses: astral-sh/setup-uv@v6 with: python-version: 3.13 We define a shell function sr that call semantic-release and then run version and publish. When version is run, it updates the CHANGELOG.md and the project version under pyproject.toml, commits these changes, and tags the commit (e.g., v0.2.0). Then we run publish, which pushes the changes to the repo—thus requiring GH_TOKEN—and creates a GitHub Release adding the appropriate section from CHANGELOG.md to the release description, along with zip and tar.gz archives containing the source code for the release version.\n- name: Run semantic-release env: GH_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: | #shell sr() { uvx --from=\u0026#34;python-semantic-release@9.21.1\u0026#34; \\ semantic-release \u0026#34;$@\u0026#34; } sr version sr publish pyproject.toml # This is the configuration we\u0026rsquo;re using under pyproject.toml:\n[tool.semantic_release] commit_parser = \u0026#34;conventional\u0026#34; version_toml = [\u0026#34;pyproject.toml:project.version\u0026#34;] allow_zero_version = true [tool.semantic_release.changelog.default_templates] changelog_file = \u0026#34;CHANGELOG.md\u0026#34; In here, commit_parser will be set to work with conventional commits, version_toml ensures that the release version under pyproject.toml is bumped up, and allow_zero_version will produce versions starting from 0.1.0 rather than 1.0.0. While we were also explicit with changelog_file, this was already the default.\nRollback Strategy # Finally, if, for some reason, you need to rollback a release, you can use the following workflow.\nPrepare dev # First, switch to dev and make sure it contains all changes from main:\ngit merge-base --is-ancestor main dev \u0026amp;\u0026amp; \\ echo \u0026#34;OK\u0026#34; || \\ echo \u0026#34;NOT OK\u0026#34; If it doesn\u0026rsquo;t (NOT OK), then rebase main—with the git branching workflow we\u0026rsquo;re following, there should be no conflicts, specially if you had just merged dev into main:\ngit rebase main Revert Commit # Once dev is synced up with main, let\u0026rsquo;s revert the automatic commit generated by semantic-release (e.g., undo v0.2.0). Don\u0026rsquo;t forget to edit the commit message, when the editor opens (default behavior), and use revert: \u0026hellip; to comply with conventional commits.\ngit revert v0.2.0 git push Delete Tag # Then, we\u0026rsquo;ll need to delete the v0.2.0 tag:\ngit tag -d v0.2.0 git push origin :refs/tags/v0.2.0 Delete Release # And finally just go into the GitHub release that you want to undo and press the delete button on the UI to remove it:\n","date":"8 July 2025","externalUrl":null,"permalink":"/posts/automated-semantic-releases/","section":"","summary":"Summary # Did you know you can rely on conventional commits, and a simple git branching workflow, to automate semantic releases for your Python projects, using GitHub Actions?","title":"Automated Semantic Releases on GitHub","type":"posts"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/conventional-commits/","section":"Tags","summary":"","title":"Conventional-Commits","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/git-branching/","section":"Tags","summary":"","title":"Git-Branching","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/github-actions/","section":"Tags","summary":"","title":"Github-Actions","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/semantic-releases/","section":"Tags","summary":"","title":"Semantic-Releases","type":"tags"},{"content":" Summary # Static sites, by definition, don\u0026rsquo;t have a backend, but you can still automate a lot of your workflow using GitHub Actions. Read below if you want to learn how to setup your GitHub repo for managing and deploying a Hugo static site. You\u0026rsquo;ll also learn how to schedule blog posts, so they go online at a later date without the need for any manual action. And you\u0026rsquo;ll learn how to use RSS and GitHub Actions to automate social media posting for Bluesky, Reddit, and Discord—you can easily add more options yourself, with a little Python coding.\n\u003e Deploying a Static Site on GitHub # A static website doesn\u0026rsquo;t have a backend, so, in theory, you wouldn\u0026rsquo;t be able to schedule posts. However, there is a way to circumvent this. Hosting a static site on GitHub can be done for free, if you create a repo named \u0026lt;username\u0026gt;.github.io. Once you do this, whatever you drop into the main branch of your repo will be published as a website on that page.\nLet\u0026rsquo;s use Hugo as an example for a static website generator. You have a few options, to manage the source code for Hugo.\nSource and Target Code: Repos vs Branches # You can either create a separate repository, where public/ is added to .gitignore, and then you just setup public/ as the \u0026lt;username\u0026gt;.github.io repo, by either keeping track of individual changes manually with proper commits, or by re-initializing and force pushing each time:\ncd public/ rm -rf .git/ git init -b main git config core.sshCommand \u0026#34;ssh -i ~/.ssh/\u0026lt;gh-key\u0026gt;\u0026#34; git config user.name \u0026#34;Your Name\u0026#34; git config user.email \u0026#34;your@email\u0026#34; git add . git commit -m \u0026#34;chore: local build deployment\u0026#34; git remote add origin \u0026lt;pages-repo-url\u0026gt; git push --force origin main:gh-pages Or, you can use the \u0026lt;username\u0026gt;.github.io repo for everything, by setting a separate branch for your compiled website (usually gh-pages), under Settings → Pages → Branch.\nPersonally, I use the second option, either deploying via a Makefile or, more often, via GitHub Actions (GHA). This is where you can also set a custom domain, if you have one.\nScheduling Hugo Blog Posts # When you create a blog post in Hugo, there are three relevant front matter flags you can set: draft, date, and expiryDate. If you set draft to true, use a date in the future, or an expiryDate in the past, then the post won\u0026rsquo;t be compiled when running hugo without arguments.\nYou still can compile drafts with the -D flag, future content with the -F flag, or expired content with the -E flag. In my Makefile, I\u0026rsquo;ve got a few rules so I don\u0026rsquo;t forget it:\nbuild: hugo --minify dev: hugo server -DF -b http://localhost:1313 future: hugo server -F -b http://localhost:1313 preview: hugo server -b http://localhost:1313 I use make dev when I\u0026rsquo;m working on a post, make future to confirm scheduled posts, and make preview to view the website as it will be published now.\nBuild and Deploy via GitHub Actions # For public repos, GitHub provides free and unlimited usage for a few runners, out of which ubuntu-latest for x64 is included, providing 4 CPU cores, 16 GB of RAM, and 14 GB of storage. This is perfect to help us manage a few simple tasks, such as compiling and deploying a Hugo static site.\nWe can use future dates to schedule posts, as long as we rebuild the website, after that date, but doing this manually is not that much of a scheduling—I\u0026rsquo;d rather just unflag from draft. This is where GitHub Actions come in:\nname: Hugo Rebuild Weekly on: schedule: - cron: \u0026#39;15 11 * * 2\u0026#39; workflow_dispatch: jobs: build-deploy: if: github.ref != \u0026#39;refs/heads/gh-pages\u0026#39; runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 with: submodules: true - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.111.3\u0026#39; - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_branch: gh-pages publish_dir: ./public Let\u0026rsquo;s break it down, step by step.\nWhen using GHAs, you can set it to activate manually, on push, and/or on a schedule. I avoid push, since I don\u0026rsquo;t want my website to be published as I\u0026rsquo;m working on a blog post, or changing the website design, before the content is ready. Personally, I set it to run on a schedule, a few minutes after my weekly YouTube video release, which is also when I publish a companion blog post for it:\nname: Hugo Rebuild Weekly on: schedule: - cron: \u0026#39;15 11 * * 2\u0026#39; workflow_dispatch: You can keep other branches besides main, and work on those, without ever worrying about that version being published automatically, as schedule only triggers for the default branch (usually main). You can, however, manually trigger the workflow for any branch. This is why we setup a single job that can be triggered for any branch that is not gh-pages:\njobs: build-deploy: if: github.ref != \u0026#39;refs/heads/gh-pages\u0026#39; runs-on: ubuntu-latest steps: ... Let\u0026rsquo;s now go through each step. First, we checkout the repo into the root of the runner, including submodules, which is required if you added your theme as a submodule, as is frequently the case:\n- uses: actions/checkout@v3 with: submodules: true Then, we install Hugo into the runner:\n- name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.111.3\u0026#39; And we compile the Hugo source code on the root of our repo:\n- name: Build run: hugo --minify Finally, we use peaceiris/actions-gh-pages to help us deploy the contents of public/ to our gh-pages branch:\n- name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_branch: gh-pages publish_dir: ./public In order for this last step to work, we are required to go into Settings → Actions → General → Workflow permissions and switch to \u0026ldquo;Read and write permissions\u0026rdquo;. This will let the actions for this repo write into the gh-pages branch, which is required for deploying a new version of the website.\nOnce this is setup, you can set your cron date:\non: schedule: - cron: \u0026#39;15 11 * * 2\u0026#39; And this action will run for the default branch (most likely main) whenever the date matches. You can use Crontab.guru to help you configure the date, which can be set with expressions like @weekly, @daily or @hourly as well.\nOn the set date, a new runner will be created, the repo\u0026rsquo;s main branch checked out, hugo will run, and the output produce in public/ will replace your gh-pages content. If a post scheduled for a future date is now in the past, it will be rendered automatically.\nAutomating Social Media Posts with RSS # Once you\u0026rsquo;ve got your static site running, it will usually provide an RSS feed with the latest posts. Hugo does this by default, even if you or your theme don\u0026rsquo;t provide a custom feed template. Each directory under content will contain its own index.xml.\nFor example, for my website, you\u0026rsquo;ll find one in the root, which includes all pages, even the privacy and cookie policies:\nhttps://datalabtechtv.com/index.xml Another one under /posts, which is the main RSS feed:\nhttps://datalabtechtv.com/posts/index.xml And even one per category (e.g., Data Engineering):\nhttps://datalabtechtv.com/categories/data-engineering/index.xml There are a few GitHub Actions on the Marketplace to handle RSS (e.g., Feed to Bluesky), but I decided to create my own repo and workflow, based on a custom Python script that publishes to Bluesky, Reddit and Discord at the same time.\nLet\u0026rsquo;s go through the workflow first, and then through the Python script.\nGitHub Actions \u0026amp; Secrets and Variables # The overall strategy for the GitHub Actions workflow consisted of:\nInstalling uv and creating a virtual environment with all Python dependencies. Running a Python script to read RSS from my static site and produce social media posts from it, posting to my socials (for now, Bluesky, Reddit, and Discord). Storing the last run dates (one per feed) directly on the repo, writing to the .last_runs.json file, committing and pushing. Step 3 will be used as persistence so that, when there are no new RSS articles, nothing will be published to social media.\nI also added the option to force post the latest n articles, when desired. I used this during development for testing, but it can also be used after you change your static site content and delete old social media posts, so that you can repost simultaneously to all your socials via manual trigger.\nThe action is setup to trigger weekly, 15 minutes after my static site is recompiled:\nname: Post from RSS env: LAST_RUNS_PATH: .last_runs.json on: schedule: - cron: \u0026#39;30 11 * * 2\u0026#39; workflow_dispatch: inputs: force_latest: description: force_latest type: number default: 0 You can customize the path where the last run dates will be stored via LAST_RUNS_PATH and, as you can see, we also provide a force_latest input, which defaults to zero (no forcing), that can optionally be set to a positive integer when manually running this workflow.\nOur workflow then has five steps. First, we checkout the repo, install uv and run uv sync to install required Python dependencies:\n- name: Checkout repo uses: actions/checkout@v4 - name: Set up uv and Python uses: astral-sh/setup-uv@v6 - name: Create venv and install dependencies run: uv sync Then we run our custom Python script, which is largely configured via environment variables that we set under Settings → Secrets and Variables → Actions.\nWe run the script and set all environment variables as follows:\n- name: Run RSS to Social env: FORCE_LATEST: ${{ github.event.inputs.force_latest }} RSS_FEED_URLS: ${{ vars.RSS_FEED_URLS }} ACTIVE_SOCIALS: ${{ vars.ACTIVE_SOCIALS }} BSKY_USERNAME: ${{ vars.BSKY_USERNAME }} BSKY_PASSWORD: ${{ secrets.BSKY_PASSWORD }} REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }} REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }} REDDIT_USERNAME: ${{ vars.REDDIT_USERNAME }} REDDIT_PASSWORD: ${{ secrets.REDDIT_PASSWORD }} REDDIT_SUBREDDIT: ${{ vars.REDDIT_SUBREDDIT }} DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }} run: uv run rss_to_social.py Any variables set under env will also be available to the script automatically (only LAST_RUNS_PATH in our case).\nHere\u0026rsquo;s a summary table for the required environment variables:\nEnvironment Variable Description Secret? FORCE_LATEST Number of RSS articles to force post to social media, even their date is older than the last run. ❌ LAST_RUNS_PATH Path for the JSON file that stores last runs dates per feed as an object. It must be relative to the repo\u0026rsquo;s root (e.g., .runs/last_runs.json. ❌ RSS_FEED_URLS New line separated feed URLs (both RSS and Atom are supported). We use new lines, because it\u0026rsquo;s easy to setup in GitHub vars and secrets, and it improves readability. ❌ ACTIVE_SOCIALS Only socials listed here will be posted to when the action runs. Values are new line separated and support: bluesky, reddit, and discord. ❌ BSKY_USERNAME The username for the Bluesky account you want to post to (e.g., datalabtechtv.bsky.social or datalabtechtv.com). ❌ BSKY_PASSWORD The password for the Bluesky account you want to post to. ✅ REDDIT_CLIENT_ID The client ID for your Reddit app (you can create one by vising https://www.reddit.com/prefs/apps/. ✅ REDDIT_CLIENT_SECRET The client secret for your Reddit app. ✅ REDDIT_USERNAME The username for the Reddit account that will be posting. We recommend using a dedicated bot account (e.g., we use DataLabTechBot). ❌ REDDIT_PASSWORD The password for the Reddit account that will be posting. ✅ REDDIT_SUBREDDIT The name of the subreddit where you\u0026rsquo;ll be posting to (e.g., we post to DataLabTechTV). ❌ DISCORD_WEBHOOK The Discord webhook URL for a given channel. This can be created by going into Edit Channel → Integrations → Webhooks → Create Webhook. ✅ The script also takes a --force-latest command line argument, which takes the value from the workflow input we set. Posts will be filtered based on the last run date and published to social media, and a new .last_runs.json file will be produced, committed and pushed:\n- name: Commit updated ${{ env.LAST_RUNS_PATH }} if changed run: | git config user.name \u0026#34;github-actions\u0026#34; git config user.email \u0026#34;github-actions@github.com\u0026#34; git add $LAST_RUNS_PATH git diff --cached --quiet \\ || git commit -m \u0026#34;Update $LAST_RUNS_PATH\u0026#34; continue-on-error: true - name: Push changes if: success() uses: ad-m/github-push-action@v0.8.0 with: github_token: ${{ secrets.GITHUB_TOKEN }} Make sure to set \u0026ldquo;Read and write permissions\u0026rdquo; under Settings → Actions → General → Workflow, so that the .last_runs.json file can be committed.\nPython Script: RSS to Socials # The following flowchart provides an overview of how the rss_to_social.py script works.\nflowchart TB LR[Last Runs] \u0026 FL[Force Latest] \u0026 FU[Feed URLs] \u0026 AS[Active Socials] --\u003e NE subgraph Each Feed NE[Determine New Entries] --\u003e CP[Create Post] subgraph Each New Entry CP --\u003e PB \u0026 PR \u0026 PD subgraph Post Socials PB[Bluesky] PR[Reddit] PD[Discord] end end end It first loads a dictionary of last run dates per feed from the file pointed by LAST_RUNS_PATH, as well as the values for the following environment variables:\nFORCE_LATEST – number of recent articles to force post (defaults to zero) RSS_FEED_URLS – one feed URL per line (usually a single feed per site). ACTIVE_SOCIALS – one social media platform name per line (can be bluesky, reddit, and/or discord). It then determines the new (or forced) entries to be posted. An entry will be considered when any of the following conditions apply:\nThere is no last run for the feed URL—all entries will be posted. The entry is one of the most recent, up to FORCE_LATEST. The entry\u0026rsquo;s date is older than the last run date. Each entry is then transformed into a Post, with a title, description (from the entry summary), link, and, if available, an image_path pointing to a temporary file with the downloaded image from the entry\u0026rsquo;s media content, along with an image_alt.\nFinally, for each of the ACTIVE_SOCIALS, we publish a post. For Bluesky, we use atproto, for Reddit, we use PRAW, and, for Discord, we just use requests.\nFinal Remarks # In the future, I might turn this into a reusable GitHub Action, but, for now, feel free to fork my repo at DataLabTechTV/rss-to-social and adapt it to your own needs. Remember that you\u0026rsquo;ll need to setup your own \u0026ldquo;Secrets and variables\u0026rdquo; and enable \u0026ldquo;Read and write permissions\u0026rdquo; for GHA on your repo.\nIf you like content about all things data, including DevOps and other tangents, such as this one, make sure to subscribe to my YouTube channel at @DataLabTechTV!\n","date":"1 July 2025","externalUrl":null,"permalink":"/posts/automate-blog-and-social-media/","section":"","summary":"Summary # Static sites, by definition, don\u0026rsquo;t have a backend, but you can still automate a lot of your workflow using GitHub Actions.","title":"Automating Hugo Blog and Social Media with GitHub Actions","type":"posts"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/rss/","section":"Tags","summary":"","title":"Rss","type":"tags"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/scheduling/","section":"Tags","summary":"","title":"Scheduling","type":"tags"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/static-site/","section":"Tags","summary":"","title":"Static-Site","type":"tags"},{"content":"","date":"24 June 2025","externalUrl":null,"permalink":"/categories/data-engineering/","section":"Categories","summary":"","title":"Data Engineering","type":"categories"},{"content":" Summary # Now that you can use DuckDB to power your data lakehouse through DuckLake, you\u0026rsquo;ll also save space on snapshots due to the ability to reference parts of parquet files (yes, you can keep all old versions, with little impact to storage), and you\u0026rsquo;ll get improved performance for small change operations due to data inlining, which lets data be stored directly within the metadata database (SQLite, PostgreSQL, etc.).\nWith a little help from dbt and an unreleased branch of dbt-duckdb adapter, we were able to design a data lakehouse strategy, covering data ingestion, transformation, and exporting, almost exclusively based on SQL!\nThis project is available as open source on GitHub, at DataLabTechTV/datalab, and the README will cover most of the details you need to understand it and get it running. In this blog post, we cover some of the most interesting components or issues, and provide a few comments about the whole process. You can also see data lab in action, and learn more about it, by watching the video below!\n\u003e Architecture # Here\u0026rsquo;s an overview of a Data Lab workflow to retrieve and organize a dataset (ingest), transform it into structured tables tracked by DuckLake (transform), and export them for external use (export):\nStorage Layout # Let\u0026rsquo;s begin with the storage layout. We use an S3 compatible object store (MinIO), but you could store your files locally as well (not supported by Data Lab, but easy to implement, as DuckLake supports it).\ns3://lakehouse/ ├── backups/ │ └── catalog/ │ ├── YYYY_MM_DD/ │ │ └── HH_mm_SS_sss/ │ │ ├── engine.duckdb │ │ ├── stage.sqlite │ │ └── marts/*.sqlite │ └── manifest.json ├── raw/ │ └── \u0026lt;dataset-name\u0026gt;/ │ ├── YYYY_MM_DD/ │ │ └── HH_mm_SS_sss/ │ │ ├── *.csv │ │ ├── *.json │ │ └── *.parquet │ └── manifest.json ├── stage/ │ └── ducklake-*.parquet ├── marts/ │ └── \u0026lt;domain\u0026gt;/ │ └── ducklake-*.parquet └── exports/ └── \u0026lt;domain\u0026gt;/ └── \u0026lt;dataset-name\u0026gt;/ ├── YYYY_MM_DD/ │ └── HH_mm_SS_sss/ │ ├── *.csv │ ├── *.json │ └── *.parquet └── manifest.json The directory structure above contains:\nraw/, which is where you drop your datasets, as they come (usually uncompressed, e.g., if it\u0026rsquo;s a zip file), and you can do this either manually or exclusively via the CLI. stage/ is where parquet files for DuckLake are stored for intermediate transformations. marts/ contains a subdirectory per data mart—I set up mine based on types of data (e.g., graphs) or relevant subjects I\u0026rsquo;m exploring (e.g., economics), but the classical setup is by company department (this shouldn\u0026rsquo;t be your case, as this is not production-ready, it\u0026rsquo;s a lab). exports/ contains exported datasets, usually in parquet format (the only one supported by Data Lab right now), ready to be used or loaded elsewhere. backups/ contains snapshots of the catalog databases, including the DuckDB engine DB and the SQLite DuckLake metadata DBs. In some cases, you\u0026rsquo;ll find a path containing a directory representing a date and a subdirectory representing a time—this is the timestamp from when the associated operation was started. We call these \u0026lsquo;dated directories\u0026rsquo;. Each dated directory contains a manifest.json with the dataset name (or snapshot name, for backups), along with the S3 path with the location of the latest version of that dataset.\nTech Stack # MinIO # We used MinIO, but you can use any S3-compatible object storage. If you\u0026rsquo;re using MinIO and are having trouble connecting, make sure that your S3_REGION matches the MinIO region. You might have to setup a custom region (e.g. eu-west-1) via the Web UI under Configurations → Region.\nUnfortunately, that feature, along with most features for the Community Edition of MinIO, was recently scraped—yes, sadly this includes users, groups, and policies as well.\nWe will soon research other viable S3-compatible object storage alternatives, perhaps in the neighborhood of SeaweedFS.\nDuckDB and DuckLake # While MinIO provides storage, DuckDB provides compute (we call it the \u0026ldquo;engine\u0026rdquo;), and DuckLake provides a catalog layer to manage metadata and external tables, tracking changes and offering snapshots, schema evolution, and time travel.\ndbt # Tools like dbt (data-build-tool) appear once in a lifetime—much like DuckDB.\n❓ Did you know that\u0026hellip;\ndbt is lower case—like sushi chefs that don\u0026rsquo;t like you to use a fork, the community also dislikes it when you wrongly spell dbt in upper case.\ndbt is for SQL templating and organization (models), and data documentation and testing (schemas). It provides a few configuration files to help you link up to external data sources as well, but, overall, this is it. Beautifully simple, yet extremely useful. Truly a gem!\nSince dbt provides multiple adapters, we were able to use the dbt-duckdb adapter and, through an unreleased branch, connect to our DuckLake catalogs via DuckDB. We then used pure DuckDB SQL queries to extract our data from the S3 ingestion bucket (raw/) and transform it into usable structured tables (stage/ and marts/).\nOperations # Below is an overview of the main lakehouse-related operations that Data Lab supports.\nWhen required, the dlctl CLI tool will read the manifests and export environment variables pointing to the S3 path with the latest version for each dataset. For example, RAW__DEEZER_SOCIAL_NETWORKS__HR__HR_EDGES will point to something like s3://lakehouse/raw/deezer_social_networks/2025_06_11/11_56_29_470/HR/hr_edges.csv, where the key point here is the date, 2025-06-11T11:56:29.470, which points to the most recent ingestion of this dataset.\nBoth ingestions (raw/) and exports (exports/) can be listed using the CLI:\ndlctl ingest ls dlctl ingest ls -a dlctl export ls dlctl export ls -a As well as pruned (i.e., all versions except the last one are deleted, per dataset):\ndlctl ingest prune dlctl export prune Other than that, catalog backups can be created:\n# Create a backup and update manifest.json accordingly dlctl backup create Listed:\n# List backup root directories dlctl backup ls # List all backed up files dlctl backup ls -a And restored:\n# Restore the latest catalog snapshot into local/ dlctl backup restore # Restore the latest catalog snapshot into /tmp/xpto # instead of local/ dlctl backup restore --target /tmp/xpto # Restore a specific snapshot into local/ dlctl backup restore --source 2025-06-17T16:24:31.349 Ingestion # The dlctl ingestion dataset command supports directory structure creation for manual uploads, as well as direct retrieval from Kaggle or Hugging Face.\nThis will create a dated directory for dataset snap_facebook_large (snake case is always used):\ndlctl ingest dataset --manual \u0026#34;SNAP Facebook Large\u0026#34; And the following commands will ingest two datasets, from Kaggle and Hugging Face, respectively:\ndlctl ingest dataset \u0026#34;https://www.kaggle.com/datasets/andreagarritano/deezer-social-networks\u0026#34; dlctl ingest dataset \u0026#34;https://huggingface.co/datasets/agusbegue/spotify-artists\u0026#34; Transformation # The dlctl transform, dlctl test, and dlctl docs commands are wrappers for dbt, although parametrization is specific to dlctl (at least for now).\nWe can run a specific model (and, therefore, its SQL transformations) as follows:\ndlctl transform -m stage.deezer_social_networks dlctl transform -m stage.million_song_dataset_spotify_lastfm dlctl transform -m marts.graphs.music_taste.nodes_genres Notice that the first two runs are for schemas (and all of their tables), while the last run is for a specific table, nodes_genres, within the graphs catalog and the music_taste schema, which is organized under the marts/ models.\nThis is the stage that produces DuckLake catalogs, storing DuckLake managed parquet files under the stage/ and marts/ S3 directories.\nDownstream/upstream triggering is also supported, using the regular dbt syntax (+):\ndlctl transform -m +marts.graphs.music_taste Finally, you can also run all data tests as follows:\ndlctl test And generate and serve model documentation as follows:\ndlctl docs generate dlctl docs serve Export # In order to be able to use a dataset externally, you first need to export it, from the DuckLake-specific parquet format into a usable format, like parquet (or CSV, or JSON).\nThis can be done by running:\ndlctl export dataset graphs music_taste Where graphs is a data mart catalog and music_taste is the schema. A few specific table exports, with names matching *nodes* and *edges*, will be stored in subdirectories—nodes/ and edges/ in this case. A similar logic can be added to the export process in the future, for other categorizable tables. Otherwise, files will live directly in the root directory, matching the schema name (e.g., music_taste).\nDuckDB Highlights # Most of our SQL code was boring, standard stuff, which is not unusual, but there were also a few interesting points that we cover next.\nHandling DuckDB MinIO Secret # Secrets in DuckDB are ephemeral, and exist only for the active session. As such, we store them in a .env file, which we automatically load via dlctl. We also offer a command to create an init.sql file under the local/ directory, directly generated from your .env configuration, once you set it up.\nAccordingly, you can access your Data Lakehouse locally by running:\n# Generate local/init.sql from your .env dlctl tools generate-init-sql # Connect to the data lakehouse duckdb -init local/init.sql local/engine.duckdb Useful List Functions # Datasets frequently contain string columns with comma-separated lists of items—in our case, it was tags—so having access to list functions was extremely useful. Here\u0026rsquo;s the transformation that we used:\nSELECT list_transform( string_split(tags, \u0026#39;, \u0026#39;), tag -\u0026gt; list_aggregate( list_transform( string_split(tag, \u0026#39;_\u0026#39;), tag_word -\u0026gt; ucase(substring(tag_word, 1, 1)) || lcase(substring(tag_word, 2)) ), \u0026#39;string_agg\u0026#39;, \u0026#39; \u0026#39; ) ) AS tags FROM ... Let\u0026rsquo;s go through it.\nlist_transform will apply a lambda to each tag, given by string_split (we split by comma and space). list_aggregate just applies string_agg to concatenate all words in a tag with spaces. Words are obtained from string_split on underscore, and list_transform is used to convert to title case. Title case was achieved by taking the first letter of a word via substring and converting to ucase (upper case). The remaining substring was converted to lcase (lower case)—if not already. For step 4, we could have used a Python UDF (user-defined function) which took a str and just returned input.title(), or we could have implemented it fully in Python, taking the original comma-separated string of tags as the argument. There would have been a slight overhead, since C++ is faster than Python, but it would have been perfectly viable for such tiny data.\nSetting up such a function using dbt-duckdb is done on top of the plugins API, and would look something like this:\n# Module: funcs from duckdb import DuckDBPyConnection from dbt.adapters.duckdb.plugins import BasePlugin # Title case conversion function def to_title(input: str) -\u0026gt; str: return input.title() # Data-specific tag parsing function def parse_tags(tags: str) -\u0026gt; list[str]: return [ t.replace(\u0026#34;_\u0026#34;, \u0026#34; \u0026#34;).title() for t in tags.split(\u0026#34;, \u0026#34;) ] class Plugin(BasePlugin): def configure_connection(self, conn: DuckDBPyConnection): # Register as UDFs, with the same name, in DuckDB conn.create_function(\u0026#34;to_title\u0026#34;, to_title) conn.create_function(\u0026#34;parse_tags\u0026#34;, parse_tags) A reference to this module would need to be added to the duckdb profile config:\ntransform: outputs: lakehouse: type: duckdb # ... plugins: - module: funcs Either way, I would avoid using this unless strictly necessary—if a working implementation exists in pure SQL, it\u0026rsquo;s usually more efficient than Python.\nAbnormally Slow JSON Parsing # One of the datasets we ingested and ran transformations for was andreagarritano/deezer-social-networks, which is found on Kaggle. It contains user data and friendship relationships for three subsets of Deezer users from Croatia (HR), Hungary (HU), and Romania (RO). For each country, there are two files: *_edges.csv and *_genres.json. The genres JSON looks something like this, but unformatted:\n{ \u0026#34;13357\u0026#34;: [\u0026#34;Pop\u0026#34;], \u0026#34;11543\u0026#34;: [\u0026#34;Dance\u0026#34;, \u0026#34;Pop\u0026#34;, \u0026#34;Rock\u0026#34;], \u0026#34;11540\u0026#34;: [\u0026#34;International Pop\u0026#34;, \u0026#34;Jazz\u0026#34;, \u0026#34;Pop\u0026#34;], \u0026#34;11541\u0026#34;: [\u0026#34;Rap/Hip Hop\u0026#34;], // ... } These files were extremely slow to parse in DuckDB. The largest genres JSON file is for Croatia, and is only 4.89 MiB. However, when we tried to load and transform this file using the following query, we got extremely high memory usage (hitting 14 GiB for DuckDB), and abnormally slow response time:\nCREATE TABLE users AS SELECT CAST(je.key AS INTEGER) AS user_id, CAST(je.value AS VARCHAR[]) AS genres FROM read_json(\u0026#39;HR/HR_genres.json\u0026#39;) j, json_each(j.json) je; Run Time (s): real 638.958 user 837.703249 sys 438.916651 That\u0026rsquo;s nearly 14m‼️\nSo, we tried to turn the JSON object into JSON lines, using:\njq \u0026#34;to_entries[] | {key: .key, value: .value}\u0026#34; \\ HR/HR_genres.json \u0026gt;HR/HR_genres.jsonl Which ran in sub-second time, as expected:\nExecuted in 481.52 millis fish external usr time 407.70 millis 596.00 micros 407.10 millis sys time 68.76 millis 914.00 micros 67.85 millis Reading HR/HR_genres.jsonl inside DuckDB was then instant and completely RAM-efficient, also as expected:\nCREATE TABLE users AS SELECT CAST(j.key AS INTEGER) AS user_id, CAST(j.value AS VARCHAR[]) AS genres FROM read_json(\u0026#39;HR/HR_genres.jsonl\u0026#39;) j; Run Time (s): real 0.082 user 0.106543 sys 0.032145 At first, since so much RAM was in use, we thought the query was actually a CROSS JOIN that replicated the whole JSON object for each produced line in the final table, but then we noticed that this is not the case, since the docs mentioned json_each as being a LATERAL JOIN (see here).\nWe also tried several other approaches, like first creating a table with the JSON object preloaded and querying over that, but this changed nothing. Besides parsing and transforming the file outside DuckDB before reading it in SQL, I don\u0026rsquo;t think there was much else we could do here, which was disappointing, as this would mean we\u0026rsquo;d have to add another layer to Data Lab, between ingestion and transformation, that would also do transformation but now in Python.\nI decided in favor of sticking with a pure SQL solution, so we posted a question in the GitHub Discussion for DuckDB, describing the issue, and one of the users was kind enough to debug the problem with us.\nFirst Proposed Solution # The first suggestion was to use one of the following approaches:\nFROM read_json(\u0026#39;HR/HR_genres.json\u0026#39;, records=false) SELECT unnest(json_keys(json)) AS user_id, unnest(list_value(unpack(columns(json.*)))) AS genres UNPIVOT (FROM read_json(\u0026#39;HR/HR_genres.json\u0026#39;)) ON * Let\u0026rsquo;s quickly unpack what is going on with these two queries.\nQuery #1 # Let\u0026rsquo;s start with the first one. First, we call read_json with records=false. By default, records is set to auto. For records=true, each key will become a column with its corresponding value. For records=false, it will either return a map(varchar, varchar[]) or a struct with each user ID identified as a field (the latter is what we want to happen here). You can read more about records here.\nAccording to the docs, COLUMNS is essentially an expansion on column selection, like j.*, col1, *, but where we can apply filtering by, or manipulate, the column name(s):\nCOLUMNS() expression can be used to execute the same expression on multiple columns:\nwith regular expressions with EXCLUDE and REPLACE with lambda functions For details on COLUMNS(), go here.\nThen, UNPACK works essentially like *lst would do in Python (i.e., it will expand the elements of a list into arguments for a function). In this example, the target function is list_value, which takes multiple arguments and returns a list with those arguments.\nFinally, for completion sake, unnest just unwinds a list or array into rows, and json_keys returns the keys in a JSON object.\nQuery #2 # For the second query, we\u0026rsquo;re just reading the JSON object using records=auto, which translates into records=true for the small sample JSON. Then, we are applying UNPIVOT on all columns (the user IDs), so that each becomes a row of user ID and list of genres.\nDocs about UNPIVOT here.\nBoth Query #1 and Query #2 worked fine for the small example that we provided (same as above), but failed for the larger original file.\nSecond Proposed (Working) Solution # We found out that the JSON object was parsed differently from 200 user records onward, and discovered there is a default of map_inference_threshold=200 for read_json. For long JSON objects like the one we have, this means that it will stop parsing object keys as structure fields from 200 keys onward, thus returning a map(varchar, varchar[]) instead of a struct, making the original query fail.\nKnowing this, the suggestion was to disable the threshold for map inference and run the query:\nFROM read_json( \u0026#39;HR/HR_genres.json\u0026#39;, records=false, map_inference_threshold=-1 ) SELECT unnest(json_keys(json)) AS user_id, unnest(list_value(unpack(columns(json.*)))) AS genres This brough down the run time from 14m to 24s, which is a significant speedup, but still extremely slow compared to the sub-second run time we got from first parsing the JSON object via jq to turn it into JSON lines.\nThis also made it possible to run the UNPIVOT query, which was even faster, taking only 6s to run:\nUNPIVOT ( FROM read_json( \u0026#39;HR/HR_genres.json\u0026#39;, records=true, map_inference_threshold=-1 ) ) ON *; Third Proposed (Creative) Solution # Finally, a third solution based on a variable to store the JSON object keys was also proposed:\nSET variable user_ids = ( FROM read_json_objects(\u0026#39;HR/HR_genres.json\u0026#39;) SELECT json_keys(json) ); FROM read_json_objects(\u0026#39;HR/HR_genres.json\u0026#39;) SELECT unnest( getvariable(\u0026#39;user_ids\u0026#39;) )::INTEGER AS user_id, unnest( json_extract( json, getvariable(\u0026#39;user_ids\u0026#39;) )::VARCHAR[] ) AS genres We find it interesting that such a solution design is possible in DuckDB. Regardless, this version was less efficient than the UNPIVOT query on the second solution, so we went with that.\nThe question remains. If the best solution takes 6s to run for a 5 MiB file, is this something that we might need to address in DuckDB, specially when the same process can run in milliseconds with a little command line magic?\nDuckDB and DuckLake Wishlist # While we were working with DuckDB and DuckLake, we created a bit of a wishlist, which we share with you below.\n1. Externally Loadable Parquet Files # Currently, using data from DuckLake tables externally requires exporting (e.g., to parquet). Maybe there isn\u0026rsquo;t a better solution, but it also defies the purpose of a data lakehouse, as data won\u0026rsquo;t be ready to use without a DuckLake adapter on target tools, which doesn\u0026rsquo;t seem reasonable to expect any time soon.\nIt\u0026rsquo;s not clear whether an DuckLake parquet file can be directly read by external processes, but it seems unlikely to be the case. Whether this is desirable, or a solid design choice, it is surely up for discussion, but, if we\u0026rsquo;re building on top of external storage, shouldn\u0026rsquo;t the stored files be ready to use directly?\nIf we follow that direction, then there is no clear way to match external files to the tables in DuckLake, as the current naming schema is not designed for identifying which parquet files are which.\n2. Hierarchical Schemas # Hierarchical schemas would be useful (e.g., marts.graphs.music.nodes), as this comes up a lot.\nIt is how dbt sets up its model logic—they use an _ for different levels—and it is also the way disk storage works (i.e., directories are hierarchical), and the natural way to organize data.\nSome teams are even looking into graph-based structures for data cataloging (e.g., Netflix). Maybe that could be an interesting as a feature for DuckDB and DuckLake, not to mention an additional distinguishing factor.\n3. Sequences in DuckLake # It would be nice to have access to sequences in DuckLake, if that makes sense technically as well. For example, while preparing nodes, generating node IDs could be done using sequences a opposed to something like:\nWITH other_nodes AS ( SELECT max(node_id) AS last_node_id FROM ... ) SELECT o.last_node_id + row_number() OVER () AS node_id FROM ..., other_nodes o ","date":"24 June 2025","externalUrl":null,"permalink":"/posts/data-lakehouse-dbt-ducklake/","section":"","summary":"Summary # Now that you can use DuckDB to power your data lakehouse through DuckLake, you\u0026rsquo;ll also save space on snapshots due to the ability to reference parts of parquet files (yes, you can keep all old versions, with little impact to storage), and you\u0026rsquo;ll get improved performance for small change operations due to data inlining, which lets data be stored directly within the metadata database (SQLite, PostgreSQL, etc.","title":"Data Lakehouse with dbt and DuckLake","type":"posts"},{"content":"","date":"24 June 2025","externalUrl":null,"permalink":"/tags/dbt/","section":"Tags","summary":"","title":"Dbt","type":"tags"},{"content":"","date":"24 June 2025","externalUrl":null,"permalink":"/tags/duckdb/","section":"Tags","summary":"","title":"Duckdb","type":"tags"},{"content":"","date":"24 June 2025","externalUrl":null,"permalink":"/tags/etl/","section":"Tags","summary":"","title":"Etl","type":"tags"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/databases/","section":"Tags","summary":"","title":"Databases","type":"tags"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/extensions/","section":"Tags","summary":"","title":"Extensions","type":"tags"},{"content":" Summary # Unedited research notes for my \u0026ldquo;PostgreSQL Maximalism\u0026rdquo; series. This is likely more than enough information, if you\u0026rsquo;re looking into extending Postgres for your storage and querying needs. For an easier-to-digest, follow-up version, check the video series. For your convenience, each extension category is properly annotated in the videos as chapters.\n\u003e Research # Document Store # Built-In Key-Value Support Built-In XML Support Built-In JSON Support Native types and functions for writing and reading JSON data. pg_jsonschema Adds JSON schema validation functions for robustness. Column Store and Analytics # pg_mooncake \u0026ldquo;Postgres-Native Data Warehouse\u0026rdquo; Provides column stores in PostgreSQL (Iceberg, Delta Lake). Uses DuckDB to query. Unlike pg_duckdb and pg_analytics, pg_mooncake can write out data to Iceberg or Delta Lake formats via transactional INSERT/UPDATE/DELETE. pg_duckdb Developed by Hydra and MotherDuck. MotherDuck integration. Maybe this will replace pg_mooncake when DuckDB extends integration with Iceberg or Delta Lake. pg_analytics Part of ParadeDB. Based on DuckDB. Recently archived and deprecated in favor of pg_search. pg_lakehouse Precursor to pg_analytics. Based on Apache DataFusion. Used to be a part of the ParadeDB codebase. columnar Developed by Hydra. Also used in pg_timeseries. Time Series Store and Real-Time # timescaledb A solution by Timescale. Provides a lot more functions to handle time series than pg_timeseries. Low latency makes it adequate for real-time analytics. Supports incremental views through continuous aggregates. Has some overlap with pg_mooncake, but can\u0026rsquo;t write to Iceberg or Delta Lake, using them directly as the storage layer. Supports tiered storage pg_timeseries A solution by Tembo. \u0026ldquo;The Timescale License would restrict our use of features such as compression, incremental materialized views, and bottomless storage.\u0026rdquo; Supports incremental materialized views. Vector Store # pgvector Vector database. Approximate indexing HNSW: Hierarchical Navigable Small World IVFFlat: Inverted File Flat Supported by GCP Cloud SQL and AWS RDS. How does it compare to pg_search? pgvectorscale A solution by Timescale. Learns from Microsoft\u0026rsquo;s DiskANN: \u0026ldquo;Graph-structured Indices for Scalable, Fast, Fresh and Filtered Approximate Nearest Neighbor Search\u0026rdquo; Efficiency layer over pgvector via: StreamingDiskANN indexing approach Statistical Binary Quantization Label-based filtered vector search Artificial Intelligence # pgai A solution by Timescale. \u0026ldquo;A suite of tools to develop RAG, semantic search, and other AI applications\u0026rdquo; Takes advantage of pgvectorscale for improved performance. Features Loading datasets from Hugging Face. Computing vector embeddings. Chunking text. Semantic search or RAG via OpenAI, Ollama, or Cohere. pg_vectorize A solution by Tembo (powers their VectorDB stack). Similar to pgai, supporting RAG and semantic search, but relies directly on pgvector. Supports Hugging Face\u0026rsquo;s Sentence-Transformers as well as OpenAI\u0026rsquo;s embeddings. Supports direct interaction with LLMs. pgrag Rust-based, experimental solution by Neon. Complete pipeline support from text extraction (PDF, DOCX) to chat completion based on ChatGPT\u0026rsquo;s API. Support for bge-small-en-v1.5 or OpenAI\u0026rsquo;s embeddings. Distance computation and ranking based on pgvector. Reranking based on jina-reranker-v1-tiny-en also available. Full-Text Search # Built-In Full-Text Search Support Generalized Inverted Index (GIN). tsvector and tsquery data types. Text preprocessing pipeline configurations usable by to_tsvector and to_tsquery. The english configuration runs the following operations: Tokenize text by spaces and punctuation; Convert to lower case; Remove stop words; Apply Porter stemmer. Example ranking/scoring functions ts_rank and ts_rank_cd. ParadeDB Search and Analytics. Search pg_search (ParadeDB\u0026rsquo;s rust-based version) Previously named pg_bm25. This is huge and it takes a long long time to compile! Adds a ton of Lucene-like features, based on Tantivy, a Rust-based Lucene alternative. Still doesn\u0026rsquo;t provide a text-based query parser. Analytics pg_analytics This has been deprecated, due to refocus on pg_search, even for analytics. Part of ParadeDB. pg_trgm Built-in extension. Character-based trigrams. Useful for fuzzy search based on string similarity (e.g., product name matching). Can be optimized via GIN and GiST indexes. pg_fuzzystrmatch Built-in extension. Provides functions to match and measure similar-sounding strings. pg_similarity Text similarity functions. Supported by GCP Cloud SQL and AWS RDS. Last commit over 5 years ago. Has forks to fix compilation with the latest versions of PostgreSQL. Graph Store # pgrouting Extension of PostGIS Graph algorithms AgensGraph PostgreSQL fork, not an extension. If it doesn\u0026rsquo;t integrate, why use this instead of a more specialized graph database, like Neo4j? Architecture diagram shows that it has its own separate graph storage layer. Query language SQL (ANSI) Cypher (openCypher) Visualization age Extension inspired by AgensGraph. Query language ANSI SQL Cypher (openCypher) No Gremlin support (yet) Visualization No graph algorithms. pggraph SQL implementations of Dijkstra and Kruskal. DOA (Dead On Arrival), this has been abandoned for 9 years. Message Queue # pgmq A solution by Tembo. Type: extension. Official libraries for Rust and Python. Community libraries for Dart, Go, Elixir, Java, Kotlin, JavaScript, TypeScript, .NET. Actively maintained. pgq A solution by Skype and a part of SkyTools. Type: extension. Official library for Python (last released for Python 3.8). Still maintained, but no meaningful changes over the last two years. Documentation pg_message_queue Type: extension. Abandoned for over 8 years. pgqueuer Type: Python library. Needs to be setup via pgq install, which creates required tables and indexes. pg-boss Type: JavaScript library. Setup is done in-code, by creating the queue. queue_classic Type: Ruby library. Postgres connection is setup via an environment variable. Description # Documents # Document formats, like JSON, XML, or YAML, model hierarchical data that follow a tree-like structure.\nThis kind of data is frequently stored in document databases like MongoDB (BSON), or simply using a key-value store, like RocksDB, where the value can only be deserialized and used in-code.\nPostgres natively supports JSON via its json and jsonb data types, as well as XML via its xml data type. It also supports key-value storage via the hstore extension, which is available by default. While the xml data type supports XML validation via the xmlschema_valid function, for JSON there is an extension called pg_jsonschema that adds support for validation based on a JSON Schema.\nAnalytics and Time Series # Transactional and analytics operations have different requirements. By default, Postgres is row-oriented, which is ideal for transactions (e.g., updating a user profile), but for analytics it\u0026rsquo;s usually more efficient to rely on column-oriented storage (e.g., averaging movie ratings, per age group). In this context, partitioning data is often a requirement as well, as to reduce complexity thus increasing performance.\nOn the data engineering community, formats like Apache Iceberg or Delta Lake, which add a metadata layer on top of Apache Parquet, are becoming a requirement for data lakehouse architectures. This layer tracks snapshots (data versioning), schema structure, partition information, and parquet file locations.\nAnother trend in the DE community is DuckDB, an in-process column-oriented database. Built for analytics, DuckDB is able to support medium scale data science tasks on a single laptop, and that\u0026rsquo;s why we love it! Think of it as a counterpart to SQLite, which is a well-liked row-oriented in-process database.\nColumn-oriented and analytics has been brought to Postgres via extensions like pg_mooncake, pg_duckdb, or pg_analytics.\nThere are also time series specific extensions that support real time and analytics by providing additional features, like incremental views, or functions like time_bucket_gapfill (add missing dates), or locf and interpolate to fill-in missing values.\nTime series specific extensions include the well-known timeseriesdb, or the more recent pg_timeseries.\nVectors and AI # One of the fundamental requirements of vector stores is that they provide efficient vector similarity calculations. This is usually achieved through specialized indexing that supports approximated similarity computations.\nExtensions like the well-known pgvector, or its complement pgvectorscale, both support querying nearest-neighbors, on pgvector via HNSW and IVFFlat indexes, and on pgvectorscale via a StreamDiskANN index. Nearest-neighbors can be computed based on multiple distance functions, such as Euclidean/L2, cosine, or Jaccard.\nRegardless of whether AI operations belongs in the database, extensions to facilitate text embedding and LLM integration still exist, integrating with pgvector.\nAI extensions include pgai and pg_vectorize, both supporting direct LLM querying, text embedding, and RAG and similarity search. In both extensions, text embedding is made possible either based on Hugging Face models, by querying OpenAI\u0026rsquo;s embedding API, or via Ollama\u0026rsquo;s API, which also powers the direct access to LLMs and RAG features. There is also pgrag, a more recent, experimental extension focused on delivering a complete pipeline for RAG, being the only one that supports text extraction from PDF or DOCX files, as well as a specialized reranking model to help improve the outcome before generating the text completion.\nAll this is made possible by accessing Python APIs under the hood, via PL/Python. While these features can be convenient at times, I tend to think that they do not belong in the database, but rather on its own Python codebase. The database should be exclusively concerned with storage and retrieval, so, unless there are performance reasons that justify integrating complex data processing features with the database, I believe this should be avoided. An example of this is pgvector and pgvectorscale, where indexing approaches were required to efficiently solve the vector distance computations — and indexing belongs in the database.\nSearch # Built-In # Full-Text Search # PostgreSQL provides basic full-text search features out-of-the-box with its Generalized Inverted Index (GIN), tsvector and tsquery data types, and corresponding functions and operators (e.g., @@ for matching a tsvector with a tsquery, or || for concatenating tsvector), supporting negation (!!), conjunction (\u0026amp;\u0026amp;), disjunction (||), and phrase queries (\u0026lt;-\u0026gt;). Documents and queries can be parsed using to_tsvector or to_tsquery, which default to the english configuration — tokenizes text by spaces and punctuation, normalizes to lower case, removes stop words, and applies Porter stemmer.\nSince PostgreSQL 11, there is the websearch_to_tsquery function, which gives us the ability to parse keyword queries directly, like we do on Google or with Apache Lucene, however there are differences.\nFor example, parsing the following keyword query:\n\u0026#34;data science\u0026#34; \u0026#34;state of the art\u0026#34; algorithms models Would result in the PostgreSQL equivalent:\n\u0026#39;data\u0026#39; \u0026lt;-\u0026gt; \u0026#39;scienc\u0026#39; \u0026amp; \u0026#39;state\u0026#39; \u0026lt;3\u0026gt; \u0026#39;art\u0026#39; \u0026amp; \u0026#39;algorithm\u0026#39; \u0026amp; \u0026#39;model\u0026#39; Which is essentially a conjunction (AND) of the two phrases and the two terms, along with stemming and stop word handling.\nHowever, search engines commonly default to disjunction (OR). Since results are often ranked, search engines just push results with a less and less matched tokens to the end of the results list.\nWe can rank the matched documents, either by using ts_rank, which is based on term frequency and proximity, or by using ts_rank_cd, which factors in cover density ranking (i.e., query term distance in the document). However, in order to retrieve documents that only partially match the query, we\u0026rsquo;d need to manually parse the query:\nphraseto_tsquery(\u0026#39;data science\u0026#39;) || phraseto_tsquery(\u0026#39;state of the art\u0026#39;) || to_tsquery(\u0026#39;algorithm | model\u0026#39;) Which is query-dependent and require us to build a query parser to handle this outside of SQL. If we do that, we can rank our documents by creating the GIN index:\nCREATE INDEX idx_doc_fulltext ON doc USING GIN (to_tsvector(\u0026#39;english\u0026#39;, content)); And using the following query:\nWITH search AS ( SELECT id, content, to_tsvector(\u0026#39;english\u0026#39;, content) AS d, phraseto_tsquery(\u0026#39;data science\u0026#39;) || phraseto_tsquery(\u0026#39;state of the art\u0026#39;) || to_tsquery(\u0026#39;algorithm | model\u0026#39;) AS q FROM doc ) SELECT ts_rank(d, q) AS score, id, content FROM search WHERE d @@ q ORDER BY score DESC; Note that Postgres uses the designation \u0026ldquo;rank\u0026rdquo; to refer to the score. For example, in the docs, when describing the weights for to_tsrank, they use phrases like \u0026ldquo;2 divides the rank by the document length\u0026rdquo;, where \u0026ldquo;rank\u0026rdquo; is really the returned score (no ranks are returned by ts_rank or ts_rank_cd).\nAlso note that these functions do not return the raw score, so the following won\u0026rsquo;t be equivalent:\nts_rank(d, q, 2) ts_rank(d, q) / length(d) Fuzzy String Matching # By default, Postgres also provides two extensions called pg_trgm and fuzzystrmatch. The first uses character-based trigrams to provide fuzzy string matching and compute string similarity. The second provides functions to match and measure similar-sounding strings.\nExtensions # There are other third-party extensions to compute string similarity, like pg_similarity, which provides several distance functions like L1/Manhattan, L2/Euclidean or Levenshtein, and other less commonly used methods like Monge-Elkan, Needleman-Wunsch or Smith-Waterman-Gotoh. While frequently offered in cloud services, including GCP and AWS, this extension appears to be unmaintained and incompatible with the latest versions of Postgres (forks exist to make it compilable).\nFinally, there is a fairly large and mature project, called ParadeDB, which provides a pg_search extension. Since the built-in support for full-text search on Postgres only provides two example ranking functions, the pg_search extension, initially called pg_bm25, was created to bring the BM25 ranking function to Postgres. It has since matured quite a lot, providing innumerous features supported by a new bm25 index. This index provides configurable segment sizes, as well as a separate text preprocessing configuration that can be set per field during indexing. A new operator @@@ is also introduced for matching, and field-based queries and boosting are supported. Several useful functions are provided to for checking term existence, fuzzy matching, range filtering, set matching, or phrase matching. JSON can also be indexed and queried, and \u0026ldquo;more like this\u0026rdquo; queries are also supported. Similarity search is supported via the pgvector extension.\nGraphs # This category is where Postgres does not shine. While graph storage can be done directly by creating a table for nodes and for relationships, this does not scale for real-world graph querying, particularly for demanding graph algorithms. A graph database usually relies on index-free adjacency to ensure efficiency, which is not supported by Postgres. The alternative is to index the ID columns of the relationships table, which means that complex graph queries, that require long walks or traversals, will need to query an index for each step it takes, without considering caching. For large graphs, this is highly inefficient. As far as I know, there is no Postgres extension that solves this problem at this moment.\nAlternatives for graph storage include AgensGraph, which is a Postgres fork rather than an extension, as well as Apache AGE (A Graph Extension), which was inspired by AgensGraph. Both support ANSI SQL as well as openCypher for querying, with AGE having an open issue on GitHub to implement Apache Gremlin support as well. AgensGraph has limited support for graph algorithms, while AGE has none at all, rather providing user defined functions. An project called pggraph implemented the Dijkstra and Kruskal graph algorithms using pure SQL, but has since been abandoned — it didn\u0026rsquo;t provide any specialized storage, but rather just functions to apply to your own relationships table via a SQL query parameter.\nFinally, perhaps the most interesting extension we can use for graph algorithms is pgrouting, which is built on top of postgis, as it is designed to add network analysis support for geospatial routing. While this still does not provide a custom storage layer for graphs, with index-free adjacency, it does provide a wide range of graph algorithms.\nMessage Queues # Message queueing software implements the producer-consumer pattern (one-to-one) and usually supports the publish-subscribe pattern as well (one-to-many / topics / events). Well-known software in this category includes Redis, ZeroMQ, RabbitMQ, or Apache Kafka, all of which provide interface libraries for several different languages — this is a requirement for message-oriented middleware, as different components are often written in different languages.\nWhile any of the previous options are likely more efficient than a Postgres-based implementation, for simple use cases there are a few extensions and libraries that implement message queues on top of Postgres. There is pgmq, from Tembo, the same authors of pg_timeseries and pg_vectorize. This integrates with over 10 languages via official (Rust and Python) and community libraries, providing a create queue function, as well as send and read functions, alongside other utilities, to support the producer-consumer pattern. For the publish-subscribe pattern, we only found the pgq extension from Skype, which similarly provides a create_queue function, as well as insert_event, register_consumer and get_batch_events functions.\nAll other extensions and libraries we found only implement the producer-consumer pattern. There is pg_message_queue which is an extension that provides functions pg_mq_create_queue, pg_mq_send_message, and pg_mq_get_msg_bin (bytes) or pg_mq_get_msg_text (plain text) — it also supports LISTEN for asynchronous notifications. There are also libraries supported on Postgres to help handle job queues: pgqueuer for Python, pg-boss for JavaScript, and queue_classic, que, good_job or delayed_job for Ruby.\nComparison # PGDG - PostgreSQL Global Development Group\nDocuments # Alternatives to: RocksDB, eXist-db, MongoDB\nExtension Author Created Description 🔴 hstore PGDG 2008 Bundled key-value type and functions. 🔴 xml PGDG 2008 Native XML type and functions. 🟢 json / jsonb PGDG 2012 / 2014 Native JSON types and functions. 🟢 pg_jsonschema Supabase 2022 JSON schema validation. Analytics and Time Series # Alternatives to: DuckDB, Apache Cassandra, Amazon RedShift, Google BigQuery, Snowflake, InfluxDB, Prometheus, Amazon Timestream\nExtension Author Created Description 🟢 pg_mooncake Mooncake Labs 2024 Column store based on Iceberg or Delta Lake, that transparently uses DuckDB vectorization for analytics queries, but also lets us extract Iceberg or Delta Lake for processing externally (e.g., using polars or duckdb). 🔴 pg_duckdb Hydra \u0026amp; MotherDuck 2024 Official extension for DuckDB that integrates with MotherDuck and cloud storage (e.g., AWS S3, Google GCS). 🔴 pg_analytics ParadeDB 2024 Similar to pg_duckdb. Added support for DuckDB as part of ParadeDB, but it was discontinued in favor of integrating analytics directly into pg_search instead. 🔴 pg_lakehouse ParadeDB 2024 Added support for Apache DataFusion to ParadeDB, but it was deprecated in favor of pg_analytics and a DuckDB backend. 🟡 columnar Hydra 2022 Columnar storage engine at the core of Hydra, a data warehouse replacement built on top of PostgreSQL. 🟢 timescaledb Timescale 2017 Well-known time series storage solution based on the hypertable, a temporally partitioned table. Adequate for real-time solutions due to its low latency and incremental materialized views. Provides a wide range of useful analytics functions. 🔴 pg_timeseries Tembo 2024 Similar to timescaledb, but extremely lacking in analytics functions. Built to compete with the limiting Timescale License. Vectors and AI # Alternatives to: Pinecone, Weaviate, Milvus, Azure AI Search\nExtension Author Created Description 🟢 pgvector Andrew Kane et al. 2021 Provides a vector type, as well as several similarity functions that power kNN. Efficiency is reached by implementing the HNSW and IVFFlat approximate indexing strategies. 🔴 pg_vectorscale Timescale 2023 Extends pgvector with the StreamingDiskANN index (inspired by Microsoft\u0026rsquo;s DiskANN), and adds Statistical Binary Quantization for compression, and label-based filtered vector search for vector operations with added filtering over categories. 🟢 pgai Timescale 2024 Relies on pg_vectorscale to provide semantic search, RAG via OpenAI, Ollama or Cohere, text chunking, computing text embeddings, or loading Hugging Face datasets. 🔴 pg_vectorize Tembo 2023 Similar to pgai, but relies directly on pgvector to provide semantic search, and RAG via Hugging Face\u0026rsquo;s Sentence-Transformers, OpenAI\u0026rsquo;s embeddings or Ollama. It also supports direct interactions with LLMs. 🔴 pgrag Neon 2024 Focused on providing a complete RAG pipeline, provides text extraction from PDF or DOCX, as well as support for reranking via jinaai/jina-reranker-v1-tiny-en. Embeddings are either based on BAAI/bge-small-en-v1.5 or OpenAI, and it only supports ChatGPT for generation. Search # Alternatives to: Elasticsearch, Apache Solr\nExtension Author Created Description 🟢 tsvector / tsquery PGDG 2008 Native text preprocessing, document/query vector representation and matching, basic ranking functions, and GIN index to support efficient full-text search. 🟢 pg_search ParadeDB 2023 Historically introduced as pg_bm25, as it focused on bringing BM25 into Postgres, it now also provides several Lucene-like features, supported on Tantivy, a Rust-based Lucene alternative. It also provides its own bm25 index with several text preprocessing settings (e.g. support for n-grams). It supports field-based and range queries, as well as set filtering, and boosting. 🟢 pg_trgm PGDG 2011 Bundled character-based trigram matching, useful for string similarity and autocompletion. 🔴 fuzzystrmatch PGDG 2005 Bundled string similarity functions, with support for matching similar-sounding names via Daitch-Mokotoff Soundex. 🔴 pg_similarity Euler Taveira et al. 2011 Large collection of text-similarity functions, like L1/Manhattan, L2/Euclidean or Levenshtein, and other less known approaches like Monge-Elkan, Needleman-Wunsch or Smith-Waterman-Gotoh. Graphs # Alternatives to: Neo4j, OrientDB, KuzuDB\nExtension Author Created Description 🟢 pgrouting pgRouting community 2010 Built on top of postgis, it was designed to add network analysis support for geospatial routing. Despite its focus, this is likely the most complete graph extension for Postgres, supporting multiple graph algorithms, although none of the state-of-the-art approaches (e.g., embeddings). 🔴 AgensGraph SKAI Worldwide 2016 Technically a Postgres fork, supporting ANSI SQL and openCypher, with few graph algorithms. 🔴 age Apache 2020 Apache AGE (A Graph Extension) supports ANSI SQL and openCypher, and might come to support Apache Gremlin. Unfortunately, no graph algorithms are provided. 🔴 pggraph Rait Raidma 2016 Meant as a collection of graph algorithms for Postgres, it only implemented Dijkstra and Kruskal, but the project has been abandoned. Message Queues # Alternatives to: Redis (Queue, Pub/Sub), ZeroMQ, RabbitMQ, Apache Kafka, Amazon Simple Queue Service, Google Cloud Pub/Sub\nTwo main categories: producer-consumer (one-to-one), and publish-subscribe (one-to-many, event-driven).\nLibraries are focused on job queues and support scheduling as well.\nExtension Author Created Description 🟢 pgmq Tembo 2023 Provides a create queue function, as well as send and read functions, alongside other utilities, to support the producer-consumer pattern. Integrates with over 10 languages via official (Rust and Python) and community libraries. 🔴 pgq Skype 2016 Provides a create_queue function, as well as insert_event, register_consumer and get_batch_events functions. It supports the publish-subscribe pattern. 🔴 pg_message_queue Chris Travers 2013 Provided the functions pg_mq_create_queue, pg_mq_send_message, and pg_mq_get_msg_bin (bytes) or pg_mq_get_msg_text (plain text), and also supported LISTEN for asynchronous notifications. Originally published via an SVN repository and later migrated to Google Code, the code by the original creator is no longer available or maintained. While a fork exists on GitHub, the project has been abandoned. 🔴 pgqueuer Jan Bjørge Løvland et al. 2024 Python library (pgqueuer) and CLI tool (pgq) that relies on asyncpg instead of psycopg2 (like pgmq). It can be configured using default Postgres environment variables, but there is no default env var to set the connection string. Queues are managed programmatically and via the CLI and only one queue exists per database, stored in the pgqueuer_jobs table. 🔴 pg-boss Tim Jones et al. 2016 Node.js library that provides the PgBoss object, instantiated with a connection string. This creates the pgboss schema where the queues are named and managed. 🔴 delayed_job Shopify 2008 Ruby library extracted from Shopify. It supports Active Job and it is not specific to Postgres. It provides multiple features to handle diverse tasks at Shopify and one of the features is named queues. Not the best option for a general purpose message queue library on top of Postgres. 🔴 que Chris Hanks et al. 2013 Ruby library that focuses on reliability and performance, taking advantage of PostgreSQL\u0026rsquo;s advisory locks, which are application-specific locks that can be set at session-level or transaction-level. These fail immediately when locked instead of blocking like row-level locks do, so workers can try another job. 🔴 good_job Ben Sheldon et al. 2020 Ruby library. Inspired by delayed_job and que, it also uses advisory locks, but provides Active Job and Rails support. 🔴 queue_classic Ryan Smith et al. 2011 Ruby library specialized in concurrent locking and supporting multiple queues and workers that can handle any of those named queues. Bits # Before PostgreSQL there was Postgres, which didn\u0026rsquo;t support SQL but an implementation of QUEL (POSTQUEL). QUEL was inspired by relational algebra and created as a part of the Ingres Database. pgcli is a useful pgsql alternative that adds syntax highlighting, autocompletion, multiline editing, and external editor support. Harlequin is a SQL IDE for the command line, supporting DuckDB natively, but also SQLite, PostgreSQL, or MariaDB, via plugins. - It can be installed via uv by running: uv tool install harlequin[postgres] - Create a Postgres profile via: uvx harlequin --config - And then connect using: uvx harlequin --profile \u0026lt;profile\u0026gt; - If it\u0026rsquo;s the default profile, you can just run: uvx harlequin WhoDB is as web client with support for PostgreSQL, MongoDB, Redis, SQLite, etc. that can be deployed as a docker image and connect to our postgresql-maximalism via host.docker.internal. It also supports conversational querying via an Ollama supported LLM — must have Ollama installed, along with the required models, and run ollama serve. When looking for Postgres extensions, there are two registries we can search: PGXN, the PostgreSQL eXtension Network. pgxn can be installed using pip install pgxnclient. Install extension: pgxn install pgmq Load extension: pgxn load -d dbname pgmq Trunk, a Postgres extension registry. trunk can be installed using cargo install pg-trunk. Install extension: trunk install pgmq will install the pgmq extension. Load extension: psql -d dbname -c \u0026quot;CREATE EXTENSION pgmq;\u0026quot; On Postgres, temporary tables are scoped to a session defaulting to ON COMMIT PRESERVE ROWS, however SQL clients often timeout sessions, so be aware of this if you\u0026rsquo;re working interactively. For example, VSCode\u0026rsquo;s SQLTools requires idleTimeoutMillis to be set per connection, or else it will default to 10s before closing idle sessions. I set mine to 1h (3,600,000ms). Make sure to save the connection and reconnect, when changing this. You can also set it to 0, in which case only manually disconnecting and reconnecting will force the session to be closed. Did you know that $$ ... $$ blocks are just a different way to quote strings? And did you know that these blocks can be nested by using a quote identifier like $myblock$ ... $myblock$? PIGSTY (PostgreSQL In Great STYle) is a PostgreSQL local-first RDS alternative that supports nearly all extensions we tested, excluding pgai, but it does support it\u0026rsquo;s competitor, pg_vectorize, from Tembo. Resources # PGXN - PostgreSQL Extension Network Trunk - A Postgres Extension Registry Just use Postgres Postgres as a Graph Database: (Ab)using pgRouting GCP: Configure PostgreSQL extensions AWS: Extension versions for Amazon RDS for PostgreSQL Elasticsearch as a column store pg_mooncake: Fast Analytics in Postgres with Columnstore Tables and DuckDB Anomaly Detection in Time Series Using Statistical Analysis PostgreSQL Wiki: Incremental View Maintenance Everything You Need to Know About Incremental View Maintenance What Goes Around Comes Around\u0026hellip; And Around\u0026hellip; Faster JSON Generation with PostgreSQL PIGSTY (PostgreSQL In Great STYle) TimescaleDB: Best Practices for Time Partitioning TimescaleDB: Compression policy TimeScale Forum: Tuple decompression limit exceeded by operation Hugging Face Datasets: jettisonthenet/timeseries_trending_youtube_videos_2019-04-15_to_2020-04-15 Hugging Face Datasets: wykonos/movies Ollama: nomic-embed-text timescale/pgai : Migrating from the extension to the python library Timescale: SQL interface for pgvector and pgvectorscale ","date":"27 May 2025","externalUrl":null,"permalink":"/posts/postgresql-maximalism/","section":"","summary":"Summary # Unedited research notes for my \u0026ldquo;PostgreSQL Maximalism\u0026rdquo; series.","title":"PostgreSQL Maximalism","type":"posts"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/research-notes/","section":"Tags","summary":"","title":"Research-Notes","type":"tags"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/series/","section":"Tags","summary":"","title":"Series","type":"tags"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/unedited/","section":"Tags","summary":"","title":"Unedited","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"Last updated: May 25, 2025\nThis website uses cookies to improve your browsing experience and to analyze how visitors use our site. We use only Google Analytics cookies for this purpose.\nWhat Are Cookies? # Cookies are small text files placed on your device by websites you visit. They help websites remember information about your visit, such as language preferences or how you interact with the site.\nWhich Cookies Do We Use? # We use Google Analytics cookies to collect anonymous information about website usage, including:\nNumber of visitors Pages visited Time spent on the site Referring websites Browser and device type These cookies help us understand and improve how visitors use our website.\nHow We Use Google Analytics Cookies # IP addresses are anonymized to protect your privacy. Data collected is aggregated and does not identify individual users. We do not use Google Analytics cookies for advertising or personal profiling. Data is retained for a maximum of 14 months. Your Consent and Control # We will only set Google Analytics cookies after you provide explicit consent through our cookie banner.\nYou can manage or withdraw your consent at any time by:\nChanging your cookie preferences in the cookie settings link available on the website. Adjusting your browser settings to block or delete cookies. More Information # For more details about Google Analytics and how your data is handled, please visit Google\u0026rsquo;s Privacy Policy.\n","externalUrl":null,"permalink":"/cookies/","section":"Data Lab Tech TV","summary":"Last updated: May 25, 2025","title":"Cookie Policy","type":"page"},{"content":"Last updated: May 25, 2025\nWe respect your privacy and are committed to protecting your personal data. This privacy policy explains how we collect, use, and protect information when you visit our website.\n1. Who We Are # Data Lab Tech TV\nEmail: mail@datalabtechtv.com\nWebsite: https://datalabtechtv.com\nWe act as the Data Controller for your personal data as defined under the General Data Protection Regulation (GDPR).\n2. What Data We Collect # We do not collect personal data unless you voluntarily provide it (e.g., by contacting us). However, we use Google Analytics to collect anonymized usage data, such as:\nPages viewed Browser type and version Operating system Referring website Approximate geographic location (based on anonymized IP) Time and duration of visit ⚠️ We do not collect names, email addresses, or other personally identifiable information (PII) via analytics.\n3. Legal Basis for Processing # We process analytics data based on your explicit consent through our cookie banner. You may withdraw consent at any time.\n4. Use of Data # We use Google Analytics data to:\nUnderstand how our website is used Improve website performance and user experience We do not:\nUse your data for advertising or marketing Share your data with third parties for commercial purposes 5. Cookies and Analytics # We use only Google Analytics cookies with:\nIP anonymization enabled Data sharing with Google disabled Data retention set to 14 months For more details, see our Cookie Policy.\n6. International Data Transfers # Google may transfer data to servers in the United States. We rely on appropriate safeguards including:\nA Data Processing Agreement with Google Standard Contractual Clauses (SCCs), as approved by the European Commission 7. Your Rights Under GDPR # You have the right to:\nAccess your personal data Request correction or deletion Withdraw your consent Object to data processing Lodge a complaint with a supervisory authority To exercise any of these rights, contact us at mail@datalabtechtv.com.\n8. Data Security # We take appropriate technical and organizational measures to ensure a level of security appropriate to the risk.\n9. Retention # We retain anonymized analytics data for a maximum of 14 months.\n10. Changes to This Policy # We may update this policy from time to time. Any changes will be posted on this page with an updated revision date.\n11. Contact # If you have questions or concerns about this privacy policy or your personal data, contact us at:\nData Lab Tech TV\nEmail: mail@datalabtechtv.com\n","externalUrl":null,"permalink":"/privacy/","section":"Data Lab Tech TV","summary":"Last updated: May 25, 2025","title":"Privacy Policy","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]