[{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"Aws","type":"tags"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/tags/cli/","section":"Tags","summary":"","title":"Cli","type":"tags"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/tags/client/","section":"Tags","summary":"","title":"Client","type":"tags"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/","section":"Data Lab Tech TV","summary":"","title":"Data Lab Tech TV","type":"page"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/tags/go/","section":"Tags","summary":"","title":"Go","type":"tags"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/tags/iam/","section":"Tags","summary":"","title":"Iam","type":"tags"},{"content":" Summary # Learn how to build a CLI for your monorepo, with cobra and the charm stack. With this blog post and video, you will easily learn the basics‚Äîand beyond‚Äîon how to use cobra for building a CLI, from persistent pre runs that propagate, to command annotations for custom control, or handling your own errors manually. We\u0026rsquo;ll also teach you how to manage multiple services, on separate ports, within a single serve command, how to manage client-side credentials easily, and how to design a basic TUI, including color palettes and styling with lipgloss, as well as custom components with bubbletea. Finally, you\u0026rsquo;ll learn how easy it is to embed a static site into a Go binary so that you can serve your own web UI. One binary to rule them all!\nFollow this series with IllumiKnow Labs, and let\u0026rsquo;s see where this journey takes us. Hopefully you\u0026rsquo;ll learn a lot along the way!\n\u003e Taming the CLI # Creating a command line interface is not, by itself, a hard task. And it\u0026rsquo;s not any different with Go. In fact, this is quite straightforward when using Cobra.\nIn this blog post, we describe our approach to accommodating the CLI for LabStore:\nChanges to the codebase, including to the CI workflow; Basics of Cobra for CLI design; How to cleanly manage multiple HTTP services in a single serve command; How generics and type constraints helped simplify a few CLI display tasks; The new credentials.yaml file, meant for direct user editing; A light introduction to TUI design, focusing on styling and the progress bar component; File embedding in Go, and how to serve a web UI as a static site, directly from Go, via http.FileServer. Changes to the Codebase # Centralizing Commands # We started from two main Go projects, backend and cli, we well as a secondary shared directory to drop any additional subprojects, required by other projects in our monorepo. Our backend project already provided its own cmd/labstore-server implemented in Cobra, but it quickly became clear that this should be centralized in cli. Initially, we were considering importing cmd/labstore-server/serve.go directly from cli, but this would essentially make the labstore-server binary irrelevant, so we opted to move all commands into cli.\nWe dropped backend/cmd/labstore-server and consolidated its commands into cli/cmd/labstore instead, thus turning backend into a library, which we renamed to server, for clarity.\nWe also started a new client project, a library to handle S3 and IAM requests. At first, we dropped it into shared/client, but we quickly realized this should be a top-level project, so it now lives in the root of the repo as client.\nFinally, because we had to reuse a lot of code from server (or, previously, backend), we ended up having to do some refactoring, so that this could be exposed under server/pkg. This included types and errors, as well as most of the authentication logic and other common components, like logging or helpers. In the future, we\u0026rsquo;ll most likely refactor some of these into subprojects, under shared, as well as split the helpers into semantically clearer packages.\nFor now, this is what we expose from server:\nserver/pkg/ ‚îú‚îÄ‚îÄ auth ‚îú‚îÄ‚îÄ config ‚îú‚îÄ‚îÄ constants ‚îú‚îÄ‚îÄ errs ‚îú‚îÄ‚îÄ helper ‚îú‚îÄ‚îÄ iam ‚îú‚îÄ‚îÄ logger ‚îú‚îÄ‚îÄ profiler ‚îú‚îÄ‚îÄ router ‚îú‚îÄ‚îÄ security ‚îî‚îÄ‚îÄ types Using go work for Development # Our individual projects‚Äîserver, client, and cli‚Äîall have their own go.mod‚Äîbut we don\u0026rsquo;t use a common go.mod at the repo root, and we also do not release each project individually. This is perhaps adding needless complexity, and something to revise later. Regardless, we needed a way for projects to import from each other.\nThe legacy approach was to use replace inside each go.mod file, like so:\nmodule github.com/IllumiKnowLabs/labstore/cli go 1.25.5 require ( github.com/IllumiKnowLabs/labstore/server v0.0.0 ) replace github.com/IllumiKnowLabs/labstore/server =\u0026gt; ../server However, there is a cleaner, more modern approach, using go.work. In the repo root, run:\ngo work init ./server ./client ./cli go work sync If you need to add another project later, you can use something like:\ngo work use ./share/logging go work sync Running go work sync will produce a go.work.sum, ensuring that workspace dependencies are consistent across all projects. Both files can be committed to your repo to ensure reproducibility, as well as for CI workflows.\nBuilding and CI # Linting with Pre-Commit # Since we moved to a monorepo with cross-project dependencies, we also had to update our CI workflows, making sure that our GitHub Actions for linting and testing still worked as expected.\nSince we now have three projects for linting, we updated our .pre-commit-config.yaml as follows:\nrepos: - repo: https://github.com/golangci/golangci-lint rev: v2.6.2 hooks: - id: golangci-lint name: golangci-lint (server) files: ^server/ entry: bash -c \u0026#39;golangci-lint run ./server/...\u0026#39; language: system pass_filenames: false - id: golangci-lint name: golangci-lint (client) files: ^client/ entry: bash -c \u0026#39;golangci-lint run ./client/...\u0026#39; language: system pass_filenames: false - id: golangci-lint name: golangci-lint (cli) files: ^cli/ entry: bash -c \u0026#39;golangci-lint run ./cli/...\u0026#39; language: system pass_filenames: false We use three similar golangci-lint entries (one per project), where we dropped types: [go], replacing it with pass_filenames: false, as we explicitly pass the file arguments to the linter. Here, we simply use the same prefixes as the ones defined for go.work. For example, ./server/... will cover all Go files under the server project.\nIf we, instead, tried to use a single entry with ./..., then we would get the following error:\nERRO [linters_context] typechecking error: pattern ./...: directory prefix . does not contain modules listed in go.work or their selected dependencies 0 issues. When using go.work, make sure that linting is done for each individual module directory.\nGitHub Actions # We defined two workflows, one for linting, where we call pre-commit, and another one for testing, where we call go test -v for each module.\nBoth lint.yml and test.yml, which exist under .github/workflows, call a common composite action defined under .github/actions/setup, which will setup go and node, and build the web project, copying web/build to server/pkg/router/assets, where it lives, to be embedded during server building.\nThis is how the jobs entry for lint.yml looks like:\njobs: pre-commit: runs-on: ubuntu-latest steps: - uses: actions/checkout@v6 - name: setup uses: ./.github/actions/setup - name: golangci-lint uses: golangci/golangci-lint-action@v9 with: version: v2.8 install-only: true - name: run pre-commit uses: pre-commit/action@v3.0.1 with: extra_args: --all-files We install golangci-lint without running it (install-only: true) and then we call pre-commit to run it. This way, we share the same linting code for CI and git commit.\nThis is how the jobs entry for test.yml looks like:\njobs: go-tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v6 - name: setup uses: ./.github/actions/setup - name: go test (server) run: go test -v ./server/... - name: go test (client) run: go test -v ./client/... - name: go test (cli) run: go test -v ./cli/... In here, we have to make sure we reference the modules defined in go.work, so we wouldn\u0026rsquo;t be able to run go test -v ./... from the repo root.\nMakefile # Finally, we had to adapt our Makefile to make sure our cli project was the only Go project being built, and that it depended on web. We also had to sync the static site files built for web with the assets directory on our server module.\nThese are the relevant targets to achieve what we just described:\n$(BIN_DIR): mkdir -p $(BIN_DIR) WEB_SRCS := $(shell find $(WEB_SRC_DIRS) -type f) $(WEB_BUILD_DIR): $(WEB_SRCS) cd $(WEB_DIR) \u0026amp;\u0026amp; npm ci cd $(WEB_DIR) \u0026amp;\u0026amp; npm run build ASSETS_SRCS := $(shell find $(WEB_BUILD_DIR) -type f) $(ASSETS_DIR): $(ASSETS_SRCS) rsync -a --delete web/build/ $(ASSETS_DIR)/ CLI_SRCS = $(shell find $(CLI_DIR) $(SERVER_DIR) $(CLIENT_DIR) -name \u0026#39;*.go\u0026#39;) $(CLI_CMD): $(CLI_SRCS) | $(BIN_DIR) cd $(CLI_DIR) \u0026amp;\u0026amp; go build -o ../$(CLI_CMD) ./cmd/labstore assets: web $(ASSETS_DIR) cli: assets $(CLI_CMD) web: $(WEB_BUILD_DIR) build: cli Running make will result in the targets web, assets, and cli being run, building the node project, rsyncing the output to the assets directory under server, and building cli/cmd/labstore .\nUsing Cobra to Produce Commands # The entry point for the Cobra library is a root command. Commands can receive a context, and handle a return error. We take advantage of both options. This how our main package looks like under cli/cmd/labstore.\nFirst, we enable the option to run all persistent pre-run and post-run from parents, as opposed to simply overriding them:\nfunc init() { cobra.EnableTraverseRunHooks = true } This means that anything defined on a persistent pre-run will always be run downstream.\nThen, we create a root command using our own NewRootCmd(), and pass it a context that captures interrupt signals. This will help us control user interruptions, i.e., ctrl+c or SIGINT:\nfunc main() { rootCmd := NewRootCmd() ctx, stop := signal.NotifyContext( context.Background(), os.Interrupt, ) defer stop() if err := rootCmd.ExecuteContext(ctx); err != nil { // ... } } We handle errors manually, so we set the following options for the root command:\ncmd.SilenceErrors = true cmd.SilenceUsage = true We then detect errs.RuntimeError, which we use to signal that an error has already been handled upstream, and only print the error and usage for any other errors (default behavior):\nfunc main() { // ... if err := rootCmd.ExecuteContext(ctx); err != nil { var runtimeError *errs.RuntimeError if errors.As(err, \u0026amp;runtimeError) { os.Exit(2) } cmd, _, cmdErr := rootCmd.Find(os.Args[1:]) if cmdErr == nil { cmd.PrintErrln(err) helper.CheckFatal(cmd.Usage()) os.Exit(1) } } } Root Command # Our root command is defined in the NewRootCmd() constructor. First, we create a *cobra.Command instance:\nvar cmd = \u0026amp;cobra.Command{ Use: strings.ToLower(constants.Name), Short: fmt.Sprintf(\u0026#34;%s, by %s\u0026#34;, constants.Name, constants.Author), Long: fmt.Sprintf(\u0026#34;%s - %s, by %s\u0026#34;, constants.Name, constants.Description, constants.Author), PersistentPreRun: func(cmd *cobra.Command, args []string) { baseCmd := topLevelCommand(cmd) if baseCmd.Name() == \u0026#34;completion\u0026#34; { return } if cmd.Annotations[\u0026#34;show-default-secret\u0026#34;] == \u0026#34;yes\u0026#34; { config.DisplayDefaultAdminSecretKey = true } if cmd.Annotations[\u0026#34;mode\u0026#34;] == \u0026#34;daemon\u0026#34; { slog.SetDefault(slog.New( slog.NewTextHandler(io.Discard, nil))) config.Load(cmd) return } bootstrap(cmd) }, } Here, Use will represent the command name (labstore for the root command, same as the binary name).\nPositional Arguments # For subcommands, Use can also contain the expected positional arguments, e.g. labstore iam users create requires a USERNAME:\nfunc NewUsersCreateCmd() *cobra.Command { var cmd = \u0026amp;cobra.Command{ Use: \u0026#34;create USERNAME\u0026#34;, Short: \u0026#34;Create an IAM user\u0026#34;, Args: cobra.MinimumNArgs(1), RunE: func(cmd *cobra.Command, args []string) error { handler := cmd.Context().Value(handlerKeyCtx).(*handlers.IAMHandler) return handler.CreateUser(args[0]) }, } return cmd } Notice that Args is set to cobra.MinimumNArgs(1), to ensure USERNAME will be set.\nWe also set a Short description, displayed when listing commands, and we can set an optional Long description, if we want to display further details when showing the help for that specific command.\nRun Hooks # Then, we can make use of several run methods:\nPersistentPreRun / PersistentPreRunE PreRun / PreRunE Run / RunE PostRun / PostRunE PersistentPostRun / PersistentPostRunE For the persistent variants, depending on whether cobra.EnableTraverseRunHooks is true or false, we\u0026rsquo;ll get different behaviors. For true, persistent hooks will trigger in-order from outer to inner level (pre) or from inner to outer level (post). For false, the inner-most definition will take precedence, in overridable fashion.\nFor the E suffix variants, our function must return error. This is automatically handled by Cobra, although, like we stated above, we disabled this default behavior and handled it ourselves.\nAnnotations # We can also find a few annotation checks. Each command can store a map[string]string containing app-specific metadata to help handle the flow. For example, our serve command is configured as follows, with the show-default-secret annotation:\nvar cmd = \u0026amp;cobra.Command{ Use: \u0026#34;serve\u0026#34;, Short: \u0026#34;Run server\u0026#34;, Long: \u0026#34;Run server for S3, IAM, and admin services\u0026#34;, Run: func(cmd *cobra.Command, args []string) { adminPass := cmd.Flags().Lookup(\u0026#34;admin-pass\u0026#34;) if adminPass != nil \u0026amp;\u0026amp; adminPass.Changed { slog.Warn(\u0026#34;setting admin pass via the command line is insecure\u0026#34;) } router.Start() }, Annotations: map[string]string{ \u0026#34;show-default-secret\u0026#34;: \u0026#34;yes\u0026#34;, }, } This annotation is then handled on our root command, determining whether to display a default admin secret key, when it hasn\u0026rsquo;t been specified by the user elsewhere on the config (this will be handled internally by the config package):\nif cmd.Annotations[\u0026#34;show-default-secret\u0026#34;] == \u0026#34;yes\u0026#34; { config.DisplayDefaultAdminSecretKey = true } Flags and Subcommands # Once the cmd is instanced in the NewRootCmd() constructor, then we can add flags or persistent flags (i.e., common to all subcommands), as well as the subcommands, each having a similar constructor:\ncmd.SilenceErrors = true cmd.SilenceUsage = true cmd.PersistentFlags().Bool(\u0026#34;debug\u0026#34;, false, \u0026#34;Set debug level for logging\u0026#34;) cmd.PersistentFlags().Bool(\u0026#34;pprof\u0026#34;, false, \u0026#34;Enable profiler\u0026#34;) cmd.PersistentFlags().String(\u0026#34;pprof-host\u0026#34;, \u0026#34;localhost\u0026#34;, \u0026#34;Profiler host\u0026#34;) cmd.PersistentFlags().Int(\u0026#34;pprof-port\u0026#34;, 6060, \u0026#34;Profiler port\u0026#34;) cmd.AddCommand(NewServeCmd()) cmd.AddCommand(NewS3Cmd()) cmd.AddCommand(NewIAMCmd()) cmd.AddCommand(NewAdminCmd()) cmd.AddCommand(NewTUICmd()) AddDaemonCommands(cmd) Help Message # This is what running the labstore command without any options will print:\nLabStore - An S3-Compatible Object Store, by IllumiKnow Labs Usage: labstore [command] Available Commands: admin Admin client completion Generate the autocompletion script for the specified shell help Help about any command iam IAM client, designed for learning restart Restart LabStore server s3 S3 client, designed for learning serve Run server start Start LabStore server status Check if LabStore server is running stop Stop LabStore server tui TUI and helper commands Flags: --debug Set debug level for logging -h, --help help for labstore --pprof Enable profiler --pprof-host string Profiler host (default \u0026#34;localhost\u0026#34;) --pprof-port int Profiler port (default 6060) Use \u0026#34;labstore [command] --help\u0026#34; for more information about a command. Here\u0026rsquo;s a semantically organized listing of the supported commands as well‚Äîsomething that would be nice do in Cobra as well, without having to create subcommands, for improved readability, like just does with groups:\nServer server ‚Äì used to start the services for the LabStore server (admin, S3, IAM, and web UI). Clients admin ‚Äì admin service client (does nothing at this time). iam ‚Äì IAM client designed for learning, not usability (maps one-to-one to the IAM service implementation). s3 ‚Äì S3 client designed for learning, not usability (maps one-to-one to the S3 service implementation). Daemon ‚Äì convenience commands for a better UX when testing LabStore, so you can manage the server without having to deploy our Docker container, or setup your own systemd service. start stop restart status Helpers tui palette ‚Äì as of now, it displays the color scheme for our palette and, in the future, its parent command will be the home for our TUI as well. completion ‚Äì automatic shell completion scripts provided by Cobra (supports bash, zsh, fish, and PowerShell). help ‚Äì go-style help and usage messages. Shell Autocompletion # As shown in the help, in the future, after you go install LabStore, you\u0026rsquo;ll also be able to setup autocompletion for your shell. For example, for fish, you just add this to your ~/.config/fish/config.fish:\nlabstore completion fish | source - With cobra, you get autocompletion for your CLI, out-of-the-box, at a zero-cost.\nManaging Multiple HTTP Services # When we run labstore serve, we start multiple http.Server instances concurrently using goroutines. In order to manage this, we define the following type:\ntype ServerDescriptor struct { Name string Server *http.Server Healthy atomic.Bool } The Name (e.g., S3-Compatible API, or IAM) will be used to display a message letting the user know where to connect for each service:\nüåê S3-Compatible API listening on http://0.0.0.0:6789 üåê IAM listening on http://0.0.0.0:6788 üåê Web UI listening on http://0.0.0.0:6790 üåê Admin API listening on http://0.0.0.0:6787 The URL will be extracted from Server.Addr. Once the server is started, its Healthy status will be set to true. This will be monitored by our admin service, which will return HTTP 200 (OK) when all services are healthy, or HTTP 503 (Service Unavailable) when any of the S3, IAM, or Web UI services are down.\nWe instance each service using values from config, e.g.:\nwebServerDescriptor := NewWebServerDescriptor( config.App.Web.Address.Host, config.App.Web.Address.Port, ) We then handle the lifecycle under router.Start() as follows:\nvar wg sync.WaitGroup errCh := make(chan error, len(serverDescriptors)) for _, sd := range serverDescriptors { wg.Add(1) go runServer(sd, \u0026amp;wg, errCh) sd.Healthy.Store(true) } go shutdownServers(ctx, serverDescriptors) go waitAndClose(errCh, \u0026amp;wg) for err := range errCh { if err != nil { slog.Error(\u0026#34;server error\u0026#34;, \u0026#34;err\u0026#34;, err) } } slog.Info(\u0026#34;all servers shut down cleanly\u0026#34;) We monitor the error channel errCh, shared among all services. Each service is started in a goroutine using go runServer(...) and setting its healthy status to true‚Äîif an error occurs or it terminates, it will be set to false. Errors are sent to errCh, which also blocks the flow until completion:\nfor err := range errCh { if err != nil { slog.Error(\u0026#34;server error\u0026#34;, \u0026#34;err\u0026#34;, err) } } The shutdownServers goroutine will immediately block until the context is done:\nfunc shutdownServers( ctx context.Context, serverDescriptors []*ServerDescriptor, ) { \u0026lt;-ctx.Done() slog.Info(\u0026#34;shutdown signal received\u0026#34;) shutdownCtx, cancel := context.WithTimeout( context.Background(), 5*time.Second, ) defer cancel() for i := len(serverDescriptors) - 1; i \u0026gt;= 0; i-- { s := serverDescriptors[i] slog.Info(\u0026#34;shutting down server\u0026#34;, \u0026#34;addr\u0026#34;, s.Server.Addr) _ = s.Server.Shutdown(shutdownCtx) } } And the context is instanced in router.Start(), as follows, to listen for SIGINT or SIGTERM:\nctx, stop := signal.NotifyContext( context.Background(), os.Interrupt, syscall.SIGTERM, ) defer stop() Again, notice that runServer, shutdownServers, and waitAndClose are all started concurrently, but shutdownServers will immediately block due to \u0026lt;- ctx.Done(), which only resumes on interruption (SIGINT or SIGTERM). This is how simple and powerful channels are in Go!\nOnce each server is shutdown, runServer will return and call wg.Done():\nfunc runServer( sd *ServerDescriptor, wg *sync.WaitGroup, errCh chan\u0026lt;- error, ) { defer wg.Done() fmt.Printf( \u0026#34;üåê %s listening on http://%s\\n\u0026#34;, sd.Name, sd.Server.Addr, ) err := sd.Server.ListenAndServe() if err != nil \u0026amp;\u0026amp; err != http.ErrServerClosed { sd.Healthy.Store(false) errCh \u0026lt;- err } } Finally, we wait for all goroutines to be done, and close errCh to exit:\nfunc waitAndClose(errCh chan error, wg *sync.WaitGroup) { wg.Wait() close(errCh) } Server errors are printed as they arrive, but the whole program is blocked until the errCh is closed, at which time it prints a log message:\nJan 23 17:09:46.263 INF all servers shut down cleanly Managing Client-Side Credentials # Since we\u0026rsquo;re implementing an S3 client, we\u0026rsquo;ll need a way to manage client-side credentials. This is usually done using some kind of config file. As we know, the aws CLI tool uses the concept of profile. We adopted a lightweight approach of this same idea.\nWe questioned whether we should:\nExtend our config file format to include client configs (i.e., a top-level client entry), with overrides via CLI args and env vars. Same as previous, with a config file, but without overrides. Create a simpler YAML file for credentials. We ended up going with option 3, creating a ~/.config/labstore/credentials.yml, automatically initialized when not found, and based on profiles. Something like this, but empty by default:\ndefault_profile: default profiles: default: access_key: admin secret_key: adminadmin After editing this file, the user can then call authenticated commands without specifying a profile, in which case the profile set in default_profile will be used, unless overridden by --profile. Since credentials are prone to fail due to server or client side config issues, we decided that having a single source of truth would reduce any potential debugging there.\nFirst Look at TUI Design # Our next release will focus on the TUI, but, for the CLI, we already used a few TUI tricks based on lipgloss and bubbletea, from the charm stack.\nWe used lipgloss to create CLI styles‚Äîcolor scheme, width, alignment, margin, padding,setting bold, etc.‚Äî and we used bubbletea for its progress bar component, that we needed to track uploads (PutObject) and downloads (GetObject).\nColor Palette # Our color palette was defined using a custom type containing multiple lipgloss.Color entries:\ntype Palette struct { TextPrimary lipgloss.Color TextMuted lipgloss.Color TextInverted lipgloss.Color Surface lipgloss.Color SurfaceAlt lipgloss.Color SurfaceHover lipgloss.Color Accent lipgloss.Color AccentMuted lipgloss.Color Success lipgloss.Color Warning lipgloss.Color Error lipgloss.Color Border lipgloss.Color } The actual final color will depend on the terminal you\u0026rsquo;re using, and whether it supports 16-colors, 256-colors, or true color, but lipgloss will handle the fallback for you transparently‚Äîit might not look good, if you don\u0026rsquo;t test it thoroughly, but it will work.\nWe then define two variables of type Palette, named DefaultPalette and ActivePalette. At this time, both variables are the same, but, in the future, we can easily extend this to support external color scheme loading (e.g., load an external file format to override DefaultPalette entries).\nStyling # A common styling strategy with lipgloss is to instance lipgloss.NewStyle() and, using method chaining to set all the required display properties. It looks something like this:\nmetaLabelStyle := lipgloss.NewStyle(). Width(20). Bold(true). Align(lipgloss.Right). PaddingRight(1). MarginRight(2). Background(ActivePalette.Surface). Foreground(ActivePalette.TextPrimary). Render If you\u0026rsquo;re defining the final style, you can simply return Render, like we do, which is the method you call to render a string with the given display options. A few examples in the lipgloss repo use this same strategy, so we generically adopted it. Notice that padding and margin is defined as multiples or columns or lines‚Äîif you\u0026rsquo;re coming from web design, remember that designing for a TUI is more limited due to the constraints of the display support‚Äîthe terminal, rather than the browser.\nGenerics and Type Constraints in Go # In Go, we often do not need generics (or at least I haven\u0026rsquo;t), so it\u0026rsquo;s worth mentioning a use case where generics were helpful. Since we often had to display metadata (i.e., key-value style information), we ended up creating a custom Metadata type, with its own Render method:\ntype Metadata map[string]Meta type Meta struct { Value any Format func() string } We wanted to be able to customize the display of generic strings and numbers, as well as dates (time.Time) and sizes (int64). In particular for numbers, we wanted to accept any of the supported types in Go, which also meant distinguishing from sizes.\nWe approached this by using generics and the following type constraint:\ntype Number interface { ~float32 | ~float64 | ~int | ~int8 | ~int16 | ~int32 | ~int64 | ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64 } Such an interface can only be used with generics to limit the acceptable types. The tilde means that any type with the same underlying type will match (e.g., based on ~int64, it would accept type MyInt int64).\nThen we defined a constructor per type. For example, we defined NewNumber as follows:\nfunc NewNumber[T Number](value T) Meta { return Meta{ Value: value, Format: func() string { p := message.NewPrinter(language.English) switch val := any(value).(type) { case float32, float64: return p.Sprintf(\u0026#34;%.6f\u0026#34;, val) default: return p.Sprintf(\u0026#34;%d\u0026#34;, val) } }, } This will format numbers with thousands commas, with floats having 6-decimals‚Äîanything else will be an integer.\nIn Metadata.Render we define the styles to use and then iterate over the keys in the map, after sorting them (labels):\nfor _, label := range labels { metaRow := lipgloss.JoinHorizontal( lipgloss.Top, metaLabelStyle(label), MetaValueStyle(metadata[label].Format()), ) rows = append(rows, metaRow) } metaView := lipgloss.JoinVertical( lipgloss.Left, rows..., ) return metaView + \u0026#34;\\n\u0026#34; As you can see, lipgloss provides a JoinHorizontal, as well as a JoinVertical, that can be quite useful in designing our TUI.\nThis will render to something like this:\nDesigning a Progress Bar Component # We also implemented a progress bar, in order to track uploads (PutObject) and downloads (GetObject). This was based on the progress-animated example straight from the bubbletea repo, with an optional console output component, to divert the logger to, while bubbletea takes over the CLI.\nSince we do not have an actual TUI, and we do not relinquish control to bubbletea completely (i.e., the main program loop is not exclusively running a tea.Program), we needed to take extra care in designing our architecture.\nIn order to display a bubbletea component, we must first define a type that implements the tea.Model interface, providing Init(), Update(), and View() methods.\nThis is how our progress bar model looks like:\ntype ProgressBarModel struct { Ctx context.Context Bar progress.Model MaxConsoleSize int Debug bool Progress chan client.Progress Message chan string done chan struct{} cancel context.CancelFunc program *tea.Program width int height int console []string In Go, as a rule of thumb, always pass the context. Everything, from Cobra to the standard HTTP library, uses a context. Simply passing the context will make it easier to receive interruption or cancellation signals. This is our Ctx.\nWe define a Bar that uses the bubbles progress.Model component.\nWe define MaxConsoleSize to determine the maximum number of lines to display at once for our console output‚Äîif we hadn\u0026rsquo;t limited this, the program would progressively slow down, as it had to keep in memory the whole console output and update it every time it called View().\nThen we pass Debug, based on the --debug flag from the CLI.\nOn the next segment, we define channels‚ÄîProgress will serve to update the current value in the progress bar, while Message will write a message to the console component; done just signals that the progress bar terminated.\nThe remaining variables are helpers‚Äîcancel ensures the context is cancellable, program runs the TUI component, width and height track current terminal dimensions, and console keeps the last MaxConsoleSize messages being displayed.\nThe two most relevant methods in our progress bar are Update() and Run()‚Äîthis last one replaces the usual TUI starter on main(). The Init() method only calls Bar.Init(), while the View() method is just regular lipgloss styling, like we\u0026rsquo;ve seen in the previous sections.\nProcessing Update Events # The Update() method looks like this:\nfunc (m *ProgressBarModel) Update(msg tea.Msg) (tea.Model, tea.Cmd) { switch msg := msg.(type) { case SomeEvent: // handle and produce a tea.Cmd return m, cmd // ... default: return m, nil } } This is our event loop, where we process each event (e.g., SomeEvent, which is not real), and return the model and a tea.Cmd to execute.\nLet\u0026rsquo;s go through each event. First, let\u0026rsquo;s take a look at how we handle updating the progress bar\u0026rsquo;s current value:\ntype progressMsg struct { current int total int } case progressMsg: pct := float64(msg.current) / float64(msg.total) if pct \u0026gt;= 1.0 { cmd := m.Bar.SetPercent(1.0) return m, tea.Sequence(cmd, tea.Quit) } cmd := m.Bar.SetPercent(pct) return m, cmd This receives the current and total values as integers, and computes the percentage to set the progress bar to. If we reach 100%, then we trigger tea.Quit, as a part of a tea.Sequence of commands, where the first command sets the percentage to 100% before quitting. Otherwise, we just set the percentage.\nSecondly, we handle our console messages:\ntype consoleMsg string case consoleMsg: m.console = append(m.console, string(msg)) if len(m.console) \u0026gt; m.MaxConsoleSize { m.console = m.console[len(m.console)-m.MaxConsoleSize:] } return m, nil This is essentially a circular or ring buffer‚Äîwe delete older messages as we hit the limit set by MaxConsoleSize.\nFinally, we handle bubbletea and bubble specific events:\ncase tea.KeyMsg: if msg.Type == tea.KeyCtrlC { fmt.Println(\u0026#34;SIGINT caught, quitting...\u0026#34;) return m, tea.Quit } return m, nil case progress.FrameMsg: progressModel, cmd := m.Bar.Update(msg) m.Bar = progressModel.(progress.Model) return m, cmd case tea.WindowSizeMsg: m.width, m.height = msg.Width, msg.Height m.Bar.Width = min(msg.Width, 80) return m, nil Here, tea.KeyMsg handles key presses‚Äîwe capture ctrl+c and exit. Then, progress.FrameMsg will handle the animation, triggering at a given frame rate, as opposed to instantly updating, and tea.WindowSizeMsg signals terminal size changes, so we can handle it properly.\nRunning and Consuming Channels # Running our program means starting the tea.Program by calling program.Run(), but also handling channel consumption and program message sending, as well as context cancelling.\nThe overall structure of Run() looks like this:\nfunc (m *ProgressBarModel) Run() { defer close(m.done) var wg sync.WaitGroup wg.Add(2) output := \u0026amp;consoleWriter{ctx: m.Ctx, ch: m.Message} revert := logger.Swap(output, logger.WithDebugFlag(m.Debug)) go func() { // consume Progress channel }() go func() { // consume Message channel }() _, err := m.program.Run() m.cancel() wg.Wait() revert() if err != nil { slog.Error(\u0026#34;progress bar\u0026#34;, \u0026#34;err\u0026#34;, err) return } slog.Debug(\u0026#34;progress bar done\u0026#34;) } As you can see, we defer the closing of the done channel, so that it is called when the function returns and unblocks any termination routine (like we did for the multiple HTTP services, described in a previous section). We also define a wait group with two tasks, one for each channel consumer (defined in the go func() calls).\nLet\u0026rsquo;s take note of the logger.Swap logic:\noutput := \u0026amp;consoleWriter{ctx: m.Ctx, ch: m.Message} revert := logger.Swap(output, logger.WithDebugFlag(m.Debug)) Our logger.Swap method is simple. It will create a new logger, with the given output, and set it as the default, while saving the previous logger, that can be restored by calling revert(). What we\u0026rsquo;re doing here is diverting the logger into a consoleWriter, which takes a channel‚Äîthe Message channel is used‚Äîand writes messages to that channel instead of stdout or stderr. This is how we produce the console messages you\u0026rsquo;ve seen us handle in the Update() method above.\nWe then start a consumer in a goroutine for the Progress and Message channels, and we run the program. Once the program exits, we cancel the context, we wait for each consumer to finish, and we revert the logger.\nChannel consumers look similar, but, for the Progress channel, we ensure we consume all remaining messages before releasing:\ngo func() { defer wg.Done() for { select { case \u0026lt;-m.Ctx.Done(): for { // Consume remaining select { case msg, ok := \u0026lt;-m.Progress: if !ok { return } m.program.Send(progressMsg{ current: msg.Current, total: msg.Total, }) default: return } } case msg, ok := \u0026lt;-m.Progress: if !ok { return } m.program.Send(progressMsg{ current: msg.Current, total: msg.Total, }) } } }() As you can see, we have an infinite for loop, where we call select. This is a reserved keyword used to randomly poll channels and handle the first that isn\u0026rsquo;t blocked‚Äîdefault will be triggered when all channels are blocked.\nWe separate two cases:\nEither the context is done, and we must finish up; Or we\u0026rsquo;re reading a Progress message. We\u0026rsquo;ll block otherwise, as we do not have a default case.\nWhen we receive a Progress message, we send progressMsg to our program, synchronously. When the context is done, we process all remaining Progress messages and then unblock and return with the default case. Once the goroutine returns, the wait group will signal that this task is done.\nDue to the complexity of this workflow, we still have some issues where the progress bar terminates without updating completely, despite having reached 100%‚Äîuse --debug if you need to confirm this is happening. It will be fixed in the future, perhaps for release v0.2.0! üòÖ\nOne Binary to Rule Them All # Finally, let\u0026rsquo;s close with something interesting about Go programs.\nWe often install Go apps using go install, and surely you have noticed that there is no go uninstall. This is because go install will download, build, and install a single command, usually copying the binary to ~/go/bin‚Äîwhich should be a part of your PATH. Uninstall can be done simply by deleting the binary, with rm ~/go/bin/\u0026lt;command\u0026gt;.\nThat said, in Go, we are able to embed files directly into our binary, which is perfect if we want to keep it in line with the installation approach I just described.\nEmbedding and Serving the Web UI # Our web UI is planned as a SvelteKit app, to be deployed as a static site. Our original approach to deploying this to production was to produce a Dockerfile.web that built the static site and deployed it using Nginx. But why do this when we can just embed the static site directly into our server library and serve it with http.FileServer? How hard can it be, right?\nWell, as it turns out, it\u0026rsquo;s not hard at all:\n//go:embed assets/** var frontendFiles embed.FS func NewWebServerDescriptor(host string, port uint16) *ServerDescriptor { slog.Info(\u0026#34;web ui server\u0026#34;, \u0026#34;host\u0026#34;, host, \u0026#34;port\u0026#34;, port) addr := fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, host, port) contentFS := helper.Must(fs.Sub(frontendFiles, \u0026#34;assets\u0026#34;)) httpFS := http.FS(contentFS) handler := http.FileServer(httpFS) server := http.Server{ Addr: addr, Handler: handler, } serverDescriptor := \u0026amp;ServerDescriptor{ Name: \u0026#34;Web UI\u0026#34;, Server: \u0026amp;server, } return serverDescriptor } The previous code is from server/pkg/router/web.go. First, we need to make sure that the web/build directory is synced to server/pkg/router/assets/, which we have done using our Makefile, as described in a previous section, although this will have to change in the future, if we want to use go install.\nThen, we simply define a variable with a special go:embed comment pointing to the assets directory. This comment is the magic sauce:\n//go:embed assets/** var frontendFiles embed.FS Once set, we can access our files using fs.Sub, which returns an fs.FS instance, and serve them using http.FS, which returns an http.FileSystem instance. We pass this to an http.FileServer, which provides an HTTP handler that we can serve as usual.\nThat\u0026rsquo;s it! Pretty sweet, right?\nWe end up with a single 21 MiB binary that has it all‚Äîone binary to rule them all! üòé\n‚ùØ du -h bin/labstore 21M bin/labstore This binary provides the S3 and IAM server, the web UI server (no Nginx required), and the clients for S3 and IAM. And, next time, it will also provide the TUI for S3 and IAM! üöÄ\n","date":"27 January 2026","externalUrl":null,"permalink":"/posts/labstore-part-5-cli-tooling/","section":"","summary":"Summary # Learn how to build a CLI for your monorepo, with cobra and the charm stack.","title":"LabStore - Part 5 - Building an Object Store in Go: CLI - Command Line Interface","type":"posts"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/tags/object-store/","section":"Tags","summary":"","title":"Object-Store","type":"tags"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/tags/s3/","section":"Tags","summary":"","title":"S3","type":"tags"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/categories/software-engineering/","section":"Categories","summary":"","title":"Software Engineering","type":"categories"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"27 January 2026","externalUrl":null,"permalink":"/tags/video/","section":"Tags","summary":"","title":"Video","type":"tags"},{"content":"","date":"23 December 2025","externalUrl":null,"permalink":"/tags/authentication/","section":"Tags","summary":"","title":"Authentication","type":"tags"},{"content":" Summary # Learn how to build an IAM service on top of SQLite in Go, from config handling to proper secret encryption. We\u0026rsquo;ll cover the required actions to implement an MVP, and SQLite pragmas and patterns to ensure performance. You\u0026rsquo;ll learn how to design the database schema for IAM, and how to implement SQLite CRUD in Go, with concurrency, using buffered channels. We\u0026rsquo;ll also touch on symmetric encryption, based on 256-bit AES-GCM, for secure secret storage, and describe our approach to organizing business logic and HTTP handlers, while also touching on interesting JSON marshaling details that arose during development.\nFollow this series with IllumiKnow Labs, and let\u0026rsquo;s see where this journey takes us. Hopefully you\u0026rsquo;ll learn a lot along the way!\n\u003e Introducing IAM # An IAM API essentially handles CRUD operations for users, groups, and policies. Users can have access keys to access the S3 service, and they can also belong to groups. Policies are used for access control, and they can be attached to users or groups. IAM also supports inline or embedded policies, directly added to users or groups, but we do not support this option for now.\nMinimum Viable Actions # After a few iterations, we ended up with the follow list of IAM actions, for which we provide working but partial implementations:\nWe only provide a single access key per user, matching the username, and providing a randomly generated secret key (overridden on subsequent creation requests); We do not support all response fields or errors; The IAM service is unprotected (i.e., there is no authentication, so it needs to be secured, for example behind a reverse proxy with TLS and HTTP auth). Regardless, the existing IAM implementation is enough to do most tasks, and we\u0026rsquo;re quite happy with it, given that our target was to reach an MVP.\nBelow is an overview of the IAM actions that we support in LabStore, with links to the respective pages in the official AWS IAM documentation:\nUsers CreateUser CreateAccessKey GetUser ListAccessKeys DeleteUser DeleteAccessKey Groups CreateGroup AddUserToGroup GetGroup DeleteGroup RemoveUserFromGroup Policies CreatePolicy AttachUserPolicy AttachGroupPolicy GetPolicy ListAttachedUserPolicies ListAttachedGroupPolicies DeletePolicy DetachUserPolicy DetachGroupPolicy Policy Document # A policy is defined by a JSON document with the following format:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:ListAllMyBuckets\u0026#34;], \u0026#34;Resource\u0026#34;: [\u0026#34;*\u0026#34;] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:Get*\u0026#34;, \u0026#34;s3:List*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::test-bucket\u0026#34;, \u0026#34;arn:aws:s3:::test-bucket/*\u0026#34; ] } ] } We support both Allow and Deny effects, AWS-compatible ARN identifiers for resource names, and the following S3 actions:\n* ‚Äì matches all actions s3:ListAllMyBuckets s3:CreateBucket s3:DeleteBucket s3:ListBucket s3:PutObject s3:GetObject s3:DeleteObject We also support globbing for resources (e.g., arn:aws:s3:::test-bucket/*) and actions (e.g., s3:Get*).\nBackend Configuration # We implemented the LabStore configuration for IAM via spf13/viper, with lower priority for defaults, followed by the labstore.yml config file, environment variables, and, with the highest priority, command line arguments.\nIn order to keep it simple, we use a consistent naming convention across config sources. For example, if we\u0026rsquo;re setting backend configs, which are used by the labstore-server command, then a setting like backend.storage.data_dir in the YAML file can be overridden by LABSTORE_BACKEND_STORAGE_DATA_DIR, where LABSTORE_ is the prefix for all LabStore configs set via env vars. We can also override the env var using the CLI, but this time, since the context is not ambiguous (e.g., for labstore-server we know we\u0026rsquo;re handling LabStore backend configs), we only use --storage-data-dir without any prefix . This behavior is consistent across the application‚Äîin summary, for env vars we add a prefix to avoid potential collisions with other program environments, and for the CLI we remove the unrequired prefix to make arguments more manageable, but, regardless, we always name configs consistently across all sources.\nBelow is an example for the backend config. We also have three other sections, which we do not cover here, named web, shared, and benchmark (covered on the previous video).\nbackend: storage: data_dir: ./data keys_dir: ./keys admin: server: host: 0.0.0.0 port: 6787 auth: access_key: admin secret_key: adminadmin iam: server: host: 0.0.0.0 port: 6788 db: max_open_conns: 3 max_idle_conns: 3 write_chan_cap: 32 timeout_ms: 5000 read_cache_size_kib: 65536 writer_cache_size_kib: 16384 s3: server: host: 0.0.0.0 port: 6789 paging: max_keys: 1000 io: buffer_size: 262144 Notice that we moved the storage configs into its own namespace, and created separate namespaces for the admin, iam, and s3 servers. The admin server only provides a health check endpoint currently, but we plan to use it in the future for other LabStore-specific tasks. The S3 server exposes our object store through an S3-compatible API. Finally, the IAM server, that we introduce here today, provides a subset of AWS IAM compatible endpoints to help manage users, groups, and policies.\nStorage Changes # The directory structure has also evolved to make room for the IAM requirements. We now configure a data_dir and a keys_dir.\nWhile objects were previously stored directly on the data_dir, they are now stored in an objects/ subdirectory inside the data_dir. We added a new metadata/ subdirectory to the data_dir, which will contain the iam.db SQLite database. We also set a separate keys_dir to store cryptographic keys‚Äîthese should be backed up separately. More on this topic below, under the Security section. IAM DB Configs # Notice that there are a few settings under backend.iam.db that let you customize your SQLite IAM database:\nmax_open_conns: 3 max_idle_conns: 3 write_chan_cap: 32 timeout_ms: 5000 read_cache_size_kib: 65536 writer_cache_size_kib: 16384 max_open_conns / max_idle_conns ‚Äì control the SQLite reader connections‚Äîhow many can be open or remain idle at a time, before reads are blocked or idle connections are closed. write_chan_cap ‚Äì determines the size of the buffered channel that consumes write queries‚Äîonce the limit is reached, requests will block; we do not have a channel timeout yet. timeout_ms ‚Äì used to timeout a SQLite connection when there are no connections available to handle the request‚Äîeither max_open_conns is reached for readers, or the channel is full for writers. read_cache_size_kib / writer_cache_size_kib ‚Äì SQLite cache size for readers and for the writer. Implementing IAM # We implement IAM as separate service in LabStore, and we integrate it in the S3-compatible service through a custom middleware for which we had already created a placeholder. Storage is completely supported on SQLite, which is embedded into the server‚Äîwe might need a different solution, if we later expand LabStore to support multiple nodes, which will largely depend on the interest the project generates within the community, to justify expanding into distributed systems.\nNext, we cover the database schema, as well as pragmas and the strategy we used to handle connections for reading an writing. Since we\u0026rsquo;re working with credentials, we also cover our security approach openly, so that it has a change to be scrutinized. Finally, we cover the business logic and HTTP requests, focusing on the data types and errors that we implemented in Go. And we close with our manual testing strategy, an area that will need some attention in the future, to make the project more accessible to external contributions.\nDatabase # When the server is first started, it will ensure that the database exists under /data/metadata/iam.db (assuming that /data is our backend.storage.data_dir).\nSchema # Our database schema is relatively straightforward, as we can see in the ERD below:\n--- title: IAM SQLite Database Schema --- erDiagram users { TEXT user_id PK TEXT name UK TEXT arn UK TEXT access_key BLOB secret_key } groups { TEXT group_id PK TEXT name UK TEXT arn UK } policies { TEXT policy_id PK TEXT name UK TEXT arn UK JSON document \"NOT NULL\" DATETIME created_at \"NOT NULL DEFAULT (CURRENT_TIMESTAMP)\" DATETIME updated_at \"NOT NULL DEFAULT (CURRENT_TIMESTAMP)\" } users }o--o{ groups : member users }o--o{ policies : attaches groups }o--o{ policies : attaches The main entity tables are created as follows, each with a unique identifier, unique name, and unique ARN.\nCREATE TABLE IF NOT EXISTS users ( user_id TEXT PRIMARY KEY, name TEXT UNIQUE, arn TEXT UNIQUE, access_key TEXT, secret_key BLOB ); CREATE TABLE IF NOT EXISTS groups ( group_id TEXT PRIMARY KEY, name TEXT UNIQUE, arn TEXT UNIQUE ); CREATE TABLE IF NOT EXISTS policies ( policy_id TEXT PRIMARY KEY, name TEXT UNIQUE, arn TEXT UNIQUE, document JSON NOT NULL, created_at DATETIME NOT NULL DEFAULT (CURRENT_TIMESTAMP), updated_at DATETIME NOT NULL DEFAULT (CURRENT_TIMESTAMP) ); For policies, we also store the policy document which is JSON / TEXT, alongside the created_at and updated_at DATETIME fields. Both of the DATETIME fields are set via defaults or through the following trigger:\nCREATE TRIGGER IF NOT EXISTS policies_update_trigger AFTER UPDATE ON policies FOR EACH ROW BEGIN UPDATE policies SET updated_at = CURRENT_TIMESTAMP WHERE policy_id = OLD.policy_id; END; Finally, as shown below, we also define three junction tables for the many-to-many relationships defining group_users, user_policies, and group_policies. Since we use foreign key checks, we also define those alongside cascading rules.\nCREATE TABLE IF NOT EXISTS group_users ( group_id TEXT, user_id TEXT, PRIMARY KEY (group_id, user_id), FOREIGN KEY(group_id) REFERENCES groups(group_id) ON DELETE CASCADE ON UPDATE CASCADE, FOREIGN KEY(user_id) REFERENCES users(user_id) ON DELETE CASCADE ON UPDATE CASCADE ); CREATE TABLE IF NOT EXISTS user_policies ( user_id TEXT, policy_id TEXT, PRIMARY KEY (user_id, policy_id), FOREIGN KEY(user_id) REFERENCES users(user_id) ON DELETE CASCADE ON UPDATE CASCADE, FOREIGN KEY(policy_id) REFERENCES policies(policy_id) ON DELETE CASCADE ON UPDATE CASCADE ); CREATE TABLE IF NOT EXISTS group_policies ( group_id TEXT, policy_id TEXT, PRIMARY KEY (group_id, policy_id), FOREIGN KEY(group_id) REFERENCES groups(group_id) ON DELETE CASCADE ON UPDATE CASCADE, FOREIGN KEY(policy_id) REFERENCES policies(policy_id) ON DELETE CASCADE ON UPDATE CASCADE ); PRAGMAs # Database connections are opened with the following pragmas, with a slight difference in cache_size between the reader and writer:\nPRAGMA journal_mode = WAL; PRAGMA synchronous = NORMAL; PRAGMA temp_store = MEMORY; PRAGMA cache_size = -65536; -- 64 MiB; or 16MiB for writer PRAGMA locking_mode = NORMAL; PRAGMA foreign_keys = ON; PRAGMA busy_timeout = 5000; -- 5s We use WAL mode to enable a write-ahead log, making it possible to use multiple readers and a single writer concurrently, without blocking.\nWe set synchronous to NORMAL, which controls how often fsync. The NORMAL mode ensures high performance and safety for the WAL mode, and an IAM service is also not write-intensive, but rather read-intensive.\nTemporary tables and indices will be stored in MEMORY, according to the temp_store pragma.\nBy default, we use 64 MiB of cache for a default pool of three readers, or 16 MiB of cache for the single writer. These can, however, be set through the config.\nWe connect in NORMAL locking_mode‚Äîthe only other option would be EXCLUSIVE, but we need concurrency here.\nAnd, finally, we enable foreign key checks, and a timeout of 5 seconds when no connection is available.\nConnections # We had to make a few decision regarding how to handle the connection to SQLite in Go:\nLibrary to handle querying (e.g., sqlc, sqlx, etc.); SQLite driver‚ÄîC-based mattn/go-sqlite3, or pure Go modernc.org/sqlite; Inline SQL queries or embedded files. Regarding decision 1, sqlc is a Go code generator that takes SQL files as input, producing methods that we can call directly for simple CRUD, while also supporting context. Since, in general, we do not like this kind of black box approach, so we ended up going with sqlx, which is a SQL query executor with a few helpers to integrate with data types, also supporting context‚Äîthis is relevant to ensure that, when HTTP requests are cancelled, the SQL query is also aborted.\nRegarding decision 2, while the C-based driver is considered the preferred approach, we didn\u0026rsquo;t want to deal with the hassle of cross-compilation issues (we\u0026rsquo;d need a C compiler to run in the target platform), so we went with the pure Go driver (this let\u0026rsquo;s us compile for any platform locally, which is easier to manage; while the driver is slower, the IAM service won\u0026rsquo;t be under immense load, specially given the self-hosting target audience for LabStore).\nFinally, regarding decision 3, while we do appreciate the option to separate SQL in their own files, we went with inline SQL queries, since most of our queries are quite simple and small, with the exception of the schema creation SQL. In the future, we might rethink this decision, but the cost of migration is not too high‚Äîjust move queries to their own SQL files and add a comment on top of a query string variable that points to the file:\n//go:embed sql/file.sql var query string Store # We define a Store data type, where we handle all of our IAM-related queries. For reading concurrently, we define a Store.readDB variable with a connection pool that defaults to only three connections, but can be configured with a higher value for systems with available resources. For writing, we use a separate strategy based on a buffered channel‚ÄîStore.writeCh‚Äîto mitigate potencial write blocks, given we are required to use a single writer. By default the buffer can hold 32 SQL tasks, but this can be configured as well.\nThe reader is accessed directly via sqlx‚Äîe.g., store.readDB.SelectContext. On the other hand, the writer is accessed either through specific unexported store methods‚Äîstore.sqlExecContext and store.sqlNamedExecContext‚Äîor by sending a sqlTask into the store.writeCh task channel, containing a response channel. This is implemented as follows.\nFirst, we define a sqlTask data type to wrap a sqlFn, containing the actual sqlx querying code, alongside a uuid to identify the request, the context, usually passed from the HTTP request, and the result channel, where we\u0026rsquo;ll send a sqlTaskResult to.\ntype sqlFn func(ctx context.Context, db *sqlx.DB) sqlTaskResult type sqlTask struct { uuid string ctx context.Context resCh chan\u0026lt;- sqlTaskResult fn sqlFn } The sqlTaskResult data type will hold the tuple that is usually returned when running a query through sqlx methods:\ntype sqlTaskResult struct { sqlRes sql.Result err error } We also provide a constructor for sqlTask instances, which takes care of the UUID for us:\nfunc newSQLTask( ctx context.Context, resCh chan\u0026lt;- sqlTaskResult, fn sqlFn ) sqlTask { return sqlTask{ uuid: uuid.NewString(), ctx: ctx, resCh: resCh, fn: fn, } } And, finally, we construct our writer worker by calling the following function only once:\nfunc newWriterWorker(db *sqlx.DB) chan\u0026lt;- sqlTask { slog.Debug( \u0026#34;new writer worker\u0026#34;, \u0026#34;writeChanCap\u0026#34;, config.IAM.DB.WriteChanCap ) taskCh := make(chan sqlTask, config.IAM.DB.WriteChanCap) go func() { for task := range taskCh { slog.Debug(\u0026#34;sql write task\u0026#34;, \u0026#34;uuid\u0026#34;, task.uuid) sqlResp := task.fn(task.ctx, db) task.resCh \u0026lt;- sqlResp } }() return taskCh } Notice that the goroutine wraps the for loop, ensuring that all tasks will be received concurrently, but handled sequentially by the writer (i.e., this is merely so that we can return from the function).\nAny SQL tasks will then be sent to the taskCh, which is available as Store.writeCh. For example:\nfunc (store *Store) sqlExecContext( ctx context.Context, query string, args ...any, ) (sql.Result, error) { resCh := make(chan sqlTaskResult, 1) store.writeCh \u0026lt;- newSQLTask(ctx, resCh, func(ctx context.Context, db *sqlx.DB) sqlTaskResult { res, err := db.ExecContext(ctx, query, args...) return sqlTaskResult{sqlRes: res, err: err} }, ) res := \u0026lt;-resCh return res.sqlRes, res.err } Security and Encryption # We need access to the plain text secret key to compute the SigV4 authorization header:\ndateKey := hmacSHA256([]byte(\u0026#34;AWS4\u0026#34;+cred.secretKey), []byte(date)) As you can see, this will be hashed with the date. So, while the secret key doesn\u0026rsquo;t need to be transmitted in plain text, we\u0026rsquo;ll need to have access to the plain text version both on the client and server sides.\nAs such, in order to secure secret keys, we use symmetrical encryption based on 256-bit AES-GCM. In turn, for this to be possible, we first generate a master key that we can use for encryption. This is created on the first startup for LabStore, and stored under the backend.storage.keys_dir, as defined in the config.\nWhen migrating LabStore, make sure you backup the keys directory, and that your configs are pointing to the correct directory, otherwise a new master key will be produced, and subsequent secret keys encrypted with it. We currently provide no way to detect that a new master key, different from a previously used one, came into use, but this is possible to check with AES-GCM, and we might add it in the future for integrity insurance.\nWe generate a master key through the iam.ensureMasterKey method by calling the following functions:\n// Produces a key with the given byte-length func GenerateKey(length int) ([]byte, error) { key := make([]byte, length) if _, err := rand.Read(key); err != nil { return nil, err } return key, nil } // Produces a AES-256 master key (32 bytes) func GenerateMasterKey() ([]byte, error) { return GenerateKey(32) } Each secret key is then encrypted using this master key, as well as a nonce, which a unique salt-like value that is produced for each encryption operation. This is prepended to the ciphertext for the secret key and stored as a byte blob in the database.\nPlease notice that the IAM server is currently unprotected, so, if you deploy LabStore as is, please make sure that you put it behind an authenticated reverse proxy, protected by TLS and HTTP Auth. In the future, we\u0026rsquo;ll protect the IAM service with the administrator credentials.\nBusiness Logic and HTTP Handlers # Our business logic is mainly concerned with interacting with the database, and caching users, groups, or policies into our internal corresponding data types. HTTP handlers will then implement calls to this business logic, but they\u0026rsquo;ll build the response separately, based on their own custom data types, defined according to the AWS IAM API.\nWe also define our own internal error types, as well as IAM errors, since our internal errors provide additional details that are not communicated through the IAM API‚Äîonly IAM errors are XML encodable. This lets us log a more detailed error message, even when the returned IAM error is a general ServiceFailure. While we are now aware of the error handling workflow that should be implemented, this still needs a lot of work to reach the state that we want (e.g., we need to track the request ID when logging internal errors).\nMost business logic is essentially CRUD, handled similarly across the IAM service. We opted to keep the business logic and the HTTP handler in the same file, as this matches our development workflow. We still keep them logically separate though, as we might need to implement similar requests via the CLI, or switch to a different HTTP handling approach in the future (less likely).\nCreatePolicy Example # The business logic is quite extensive, so we\u0026rsquo;ll not cover it all here, but we\u0026rsquo;ll provide a concrete example below, based on CreatePolicy, starting from the Store data type, at the core of the IAM service.\nData Types # --- title: Class Diagram for the CreatePolicy IAM Action --- classDiagram class Store { +CachedUsers : map[string]*CachedUser, +CachedGroups : map[string]*CachedGroup, +CachedPolicies : map[string]*CachedPolicy, +TTL : time.Duration -readDB : *sqlx.DB -writeCh : chan\u003c- sqlTask +CreatePolicy(ctx context.Context, name string, doc *PolicyDocument) :\u0026nbsp;(*Policy, error) +CreatePolicyHandler(w http.ResponseWriter, r *http.Request) } class CachedPolicy { +Policy : *Policy +LoadedAt : time.Time +NeverExpire : bool } class Policy { +PolicyID : string \\`\"db:\"policy_id\"\\` +Name : string \\`db:\"name\"\\` +Arn : string \\`db:\"arn\"\\` +AttachmentCount : int +CreatedAt : time.Time \\`db:\"created_at\"\\` +UpdatedAt : time.Time \\`db:\"updated_at\"\\` +Document : *PolicyDocument \\`db:\"document\"\\` } class PolicyDocument { +Version : string +Statement : []Statement } class Statement { +Effect : Effect +Action : Actions +Resource: Resources } Store --\u003e CachedPolicy CachedPolicy --\u003e Policy Policy --\u003e PolicyDocument PolicyDocument \"1\" --\u003e \"*\" Statement For the HTTP handler, we also define the following response types, inside the iam package, under policies.go, policies_create.go and types.go.\nclassDiagram class CreatePolicyResponse { +XMLName : xml.Name \\`xml:\"https\\://iam.amazonaws.com/doc/2010-05-08/ CreatePolicyResponse\"\\` +CreatePolicyResult : *CreatePolicyResult +ResponseMetadata : *ResponseMetadata } class CreatePolicyResult { +Policy : *PolicyResult } class PolicyResult { +XMLName : xml.Name \\`xml:\"Policy\"\\` +PolicyName : string +DefaultVersionId : string +PolicyId : string +Path : string +Arn : string +AttachmentCount : int +CreateDate : time.Time +UpdateDate : time.Time } class ResponseMetadata { +RequestId : string } CreatePolicyResponse --\u003e CreatePolicyResult CreatePolicyResponse --\u003e ResponseMetadata CreatePolicyResult --\u003e PolicyResult Internal and IAM Errors # The HTTP handler also relies on the following errors, defined inside the errs package, under iam.go:\nfunc IAMServiceFailure() *IAMError { return \u0026amp;IAMError{ Type: IAMReceiverType, Code: \u0026#34;ServiceFailure\u0026#34;, Message: \u0026#34;The request processing has failed because of an internal error.\u0026#34;, StatusCode: http.StatusInternalServerError, } } func IAMEntityAlreadyExists(entityName string) *IAMError { return \u0026amp;IAMError{ Type: IAMReceiverType, Code: \u0026#34;EntityAlreadyExists\u0026#34;, Message: fmt.Sprintf(\u0026#34;The entity %s already exists.\u0026#34;, entityName), StatusCode: http.StatusConflict, } } And we use a non-compliant error to handle missing query parameters (another one to improve in the future, under error handling):\nfunc HTTPMissingQueryParam(param string) error { return fmt.Errorf(\u0026#34;missing query parameter: %s\u0026#34;, param) } JSON Marshaling # Both Action and Resource can be a single string or an array of strings, so they have their own types, Actions and Resources, so that we can handle this:\ntype Action string const ( S3ListAllMyBuckets Action = \u0026#34;s3:ListAllMyBuckets\u0026#34; S3CreateBucket Action = \u0026#34;s3:CreateBucket\u0026#34; S3DeleteBucket Action = \u0026#34;s3:DeleteBucket\u0026#34; S3ListBucket Action = \u0026#34;s3:ListBucket\u0026#34; S3PutObject Action = \u0026#34;s3:PutObject\u0026#34; S3GetObject Action = \u0026#34;s3:GetObject\u0026#34; S3DeleteObject Action = \u0026#34;s3:DeleteObject\u0026#34; ) type Actions []Action func (a *Actions) UnmarshalJSON(data []byte) error { var single Action if err := json.Unmarshal(data, \u0026amp;single); err == nil { *a = []Action{single} return nil } var multi []Action if err := json.Unmarshal(data, \u0026amp;multi); err != nil { return err } *a = multi return nil } Also notice that we use enum-like constants to store each supported action within our policies. These cover all of our existing S3 requests.\nIn order to properly store JSON in SQLite, we also needed to implement the following methods for PolicyDocument:\nfunc (pd *PolicyDocument) Value() (driver.Value, error) { return json.Marshal(pd) } func (pd *PolicyDocument) Scan(src any) error { if src == nil { *pd = PolicyDocument{} return nil } switch s := src.(type) { case []byte: return json.Unmarshal(s, pd) case string: return json.Unmarshal([]byte(s), pd) default: return fmt.Errorf(\u0026#34;unsupported type: %T\u0026#34;, src) } } Notice that s := src.(type) is only valid inside a switch statement, where the case statements will match the type, and the s variable will contain the value cast to that type.\nIAM Middleware # IAM policy checking is then done by the IAM middleware, establishing the correct permissions for each individual S3 request.\nFor example, for GetObject, we set the permissions, under router/s3.go as follows:\nrouter.Handle( \u0026#34;GET /{bucket}/{key...}\u0026#34;, middleware.WithIAM( iam.S3GetObject, http.HandlerFunc(object.GetObjectHandler), ), ) This is then handled by the IAM middleware under middleware/iam.go, which checks if the corresponding access key has the required permissions:\nfunc WithIAM(action iam.Action, next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { slog.Debug(\u0026#34;with iam\u0026#34;, \u0026#34;action\u0026#34;, action) if action == \u0026#34;\u0026#34; { return } bucket := r.PathValue(\u0026#34;bucket\u0026#34;) key := r.PathValue(\u0026#34;key\u0026#34;) accessKey := GetRequestAccessKey(r) if !iam.CheckPolicy(accessKey, bucket, key, iam.Action(action)) { errs.Handle(w, errs.S3AccessDenied()) return } next.ServeHTTP(w, r) }) } The iam.CheckPolicy function, defined in iam/iam.go, will match the resource given by bucket and key, as well as the requested action, with the policy stored in the iam.Store. If the access key has the required permissions, it will be allowed to continue.\nManual Testing # For testing, we wanted to implement unit tests based on net/http/httptest for the REST API, as well as unit tests for the business logic Store methods. However, since we\u0026rsquo;re working under a time constraint, to bring you this content at a decent pace, we scheduled this for v0.2.0.\nDuring development, however, we relied heavily on scripted manual testing, directly based on just commands and httpie POST requests. Recently, we tried the official aws client, and it initially failed to work with the IAM endpoint, because we had used query parameters instead of form parameters. This has been fixed, and is already available on this week\u0026rsquo;s release branch.\nIn order to list all IAM testing commands, run:\njust backend And look at the test-iam group:\n[test-iam] test-iam-add-user-to-group test-iam-attach-group-policy test-iam-attach-user-policy test-iam-create-access-key test-iam-create-group test-iam-create-policy test-iam-create-user test-iam-delete-access-key test-iam-delete-group test-iam-delete-policy test-iam-delete-user test-iam-detach-group-policy test-iam-detach-user-policy test-iam-get-group test-iam-get-policy test-iam-get-user test-iam-group-policy test-iam-list-access-keys test-iam-list-attached-group-policies test-iam-list-attached-user-policies test-iam-remove-user-from-group test-iam-user-policy If you\u0026rsquo;d like to test the API, make sure you have just and httpie installed, and try out this command for example:\njust backend::test-iam-group-policy This will run several other commands, so it represents a moderately extensive testing of the IAM service:\n[group(\u0026#34;test-iam\u0026#34;)] test-iam-group-policy: test-iam-create-user \\ test-iam-create-access-key test-iam-create-group \\ test-iam-add-user-to-group test-iam-create-policy \\ test-iam-attach-group-policy We rely the bucket, user, group, and policy preset as justfile variables‚Äîtest-bucket, test_user, test_group, and test-policy. The policy document we set for test-policy is the one displayed in the Policy Document section above.\nGo Tip of the Day # At some point, we were trying to understand why the go-sqlite3 dependency was being listed under go.sum‚Äîthis is for the C-based SQLite driver that we opted not to use, so it shouldn\u0026rsquo;t be there. If you hit a similar problem, here\u0026rsquo;s my workflow.\nFirst, I checked whether go-sqlite3 was used in any of my source files using ripgrep:\nrg go-sqlite3 This listed the match for go.sum as follows:\ngo.sum 40:github.com/mattn/go-sqlite3 v1.14.22 h1:2gZY6PC6kBnID23Tichd1K+Z0oS6nE/XwU+Vz/5o4kU= 41:github.com/mattn/go-sqlite3 v1.14.22/go.mod h1:Uh1q+B4BYcTPb+yiD3kU8Ct7aC0hY9fxUwlHK0RXw+Y= We then tried to understand if it was also listed in the dependency graph:\ngo mod graph | grep go-sqlite3 And this output a single match for sqlx:\ngithub.com/jmoiron/sqlx@v1.4.0 github.com/mattn/go-sqlite3@v1.14.22 But the way we finally understood where this came from was through the following command:\ngo mod why -m github.com/mattn/go-sqlite3 This outputs exactly the dependencies that lead to go-sqlite3:\n## github.com/mattn/go-sqlite3 github.com/IllumiKnowLabs/labstore/backend/internal/iam github.com/jmoiron/sqlx github.com/jmoiron/sqlx.test github.com/mattn/go-sqlite3 This shows us exactly why go-sqlite3 is included as a dependency: it is needed for testing under sqlx.test, which makes sense, and is unrelated to our code.\n","date":"23 December 2025","externalUrl":null,"permalink":"/posts/labstore-part-4-iam-service/","section":"","summary":"Summary # Learn how to build an IAM service on top of SQLite in Go, from config handling to proper secret encryption.","title":"LabStore - Part 4 - Building an Object Store in Go: IAM - Identity and Access Management","type":"posts"},{"content":"","date":"9 December 2025","externalUrl":null,"permalink":"/tags/benchmark/","section":"Tags","summary":"","title":"Benchmark","type":"tags"},{"content":" Summary # Learn how to deploy multiple S3-compatible object stores, including LabStore, MinIO, Garage, SeaweedFS, and RustFS, using Docker Compose. Once the stack is running, you\u0026rsquo;ll learn how to benchmark throughput using warp by MinIO. You\u0026rsquo;ll also learn how to setup Go profiling, which we use in LabStore to help us identify bottlenecks and drive performance optimization. Everything is orchestrated with Just commands, split into two modules, infra for provisioning the Docker stack, and benchmark for running tests and analyzing throughput.\nFollow this series with IllumiKnow Labs, and let\u0026rsquo;s see where this journey takes us. Hopefully you\u0026rsquo;ll learn a lot along the way!\n\u003e Open Source S3 Object Stores # Deployment with Docker Compose # We selected the following S3-compatible object stores to benchmark alongside LabStore, just because these are the most discussed among the community:\nMinIO Garage SeaweedFS RustFS We also considered Ceph with RADOS Gateway, but deployment was too costly in terms of time/effort, and it only really shines in a distributed scenario, which is not critical for us given the current stage of development for LabStore‚Äîwe\u0026rsquo;re still at the MVP stage (or even pre-MVP). We also considered including Vitastor and Zenko CloudServer, but these aren\u0026rsquo;t as prevalent in online discussions, and we didn\u0026rsquo;t set to do an exhaustive analysis anyway.\nBelow, we cover the setup basics for each store, as a Docker Compose service, as well as a few setup details.\nLabStore # Dockerfile # For LabStore, we only require the backend to be running for our benchmark. We have a Dockerfile.backend under infra/ that is split into two stages, one for building the Go project as a single binary, and another one to deploy the LabStore server.\nFor building the labstore-server binary, we use the Alpine-based golang image for the platform where we\u0026rsquo;re building the Go project, given by BUILDPLATFORM. We take two arguments, TARGETOS and TARGETACH with the target operating system and architecture to build the binary for. We copy go.mod and go.sum to the working directory and download all dependencies with go mod download. Finally, we copy the code and compile the binary for the target platform.\n# ================= # Go builder # ================= FROM --platform=$BUILDPLATFORM \\ golang:1.25-alpine3.22 AS builder ARG TARGETOS ARG TARGETARCH WORKDIR /labstore/backend COPY backend/go.mod backend/go.sum ./ RUN go mod download COPY backend . RUN GOOS=$TARGETOS GOARCH=$TARGETARCH go build \\ -o bin/labstore-server ./cmd/labstore-server On the same Dockerfile.backend, we begin a second stage using Alpine, where we copy the binary from the previous builder stage. We create the appropriate labstore user and group, create a /data directory with the required ownership, that we exposed as a volume, switch to user labstore and set labstore-server as the entry point, making this usable in Docker Compose with any arguments that we\u0026rsquo;d like to use.\n# ================= # LabStore backend # ================= FROM alpine:3.22 COPY --from=builder \\ /labstore/backend/bin/labstore-server \\ /usr/local/bin/labstore-server RUN addgroup -S labstore \\ \u0026amp;\u0026amp; adduser -S labstore -G labstore RUN mkdir /data RUN chown -R labstore:labstore /data VOLUME /data USER labstore ENTRYPOINT [\u0026#34;labstore-server\u0026#34;] Service # Then, on our Docker Compose project, defined under infra/compose.yml, we build the infra/Dockerfile.backend image from the root of the repo, loading the environment variables from the labstore.yml config file via just and yq.\nbenchmark-labstore: profiles: - benchmark build: context: .. dockerfile: infra/Dockerfile.backend ports: - ${BENCHMARK_LABSTORE_PORT:-7789}:6789 networks: - benchmark volumes: - benchmark-labstore:/data environment: - LABSTORE_SERVER_ADMIN_ACCESS_KEY=${BENCHMARK_STORE_ACCESS_KEY} - LABSTORE_SERVER_ADMIN_SECRET_KEY=${BENCHMARK_STORE_SECRET_KEY} command: serve --host 0.0.0.0 --port 6789 --storage-path /data restart: unless-stopped We use the following environment variables:\nBENCHMARK_LABSTORE_PORT, which defaults to 6789, but can be set to overwrite this. BENCHMARK_STORE_ACCESS_KEY and BENCHMARK_STORE_SECRET_KEY to set the user and password for admin. Note that LABSTORE_SERVER_STORAGE_PATH, despite being available in our config, is ignored for the Docker service, defaulting to /data, as set by --storage-path, and being exposed as a mountable volume instead. It\u0026rsquo;s the same with LABSTORE_SERVER_HOST which is set to 0.0.0.0 so that Docker can expose the service, and LABSTORE_SERVER_PORT, which is set to a hardcoded port, since we\u0026rsquo;re exposing it anyway.\nMinIO # As there are no security dependencies for running a benchmark, we rely on the latest Docker image release for MinIO that was provided as a build by MinIO: RELEASE.2025-09-07T16-13-09Z. We use the default port 9000 to expose the S3-compatible API, and exposed the UI port as well, but this is not required for the benchmark.\nbenchmark-minio: profiles: - benchmark - testing image: minio/minio:RELEASE.2025-09-07T16-13-09Z ports: - ${BENCHMARK_MINIO_PORT:-9000}:9000 - 9001:9001 networks: - benchmark - testing volumes: - benchmark-minio:/data environment: - MINIO_ROOT_USER=${BENCHMARK_STORE_ACCESS_KEY} - MINIO_ROOT_PASSWORD=${BENCHMARK_STORE_SECRET_KEY} command: server /data --console-address \u0026#34;:9001\u0026#34; healthcheck: test: [ \u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:9000/minio/health/live\u0026#34; ] interval: 10s retries: 5 restart: unless-stopped We use the following environment variables:\nBENCHMARK_MINIO_PORT to optionally set a different port for MinIO. BENCHMARK_STORE_ACCESS_KEY and BENCHMARK_STORE_SECRET_KEY to set the user and password for admin. Garage # Configuration # In order to make it easier to deploy on a remote Docker context, we also built a custom image to bundle the preconfigured garage.toml. The alternative would have been to manually copy over this file into the Docker host machine, so that we could mount it using Docker Compose. However, since our Docker VMs are to remain accessible only via the Docker API, this wasn\u0026rsquo;t compatible with our home lab workflow, and so that option was discarded.\nOur garage.toml looks like this:\nmetadata_dir = \u0026#34;/var/lib/garage/meta\u0026#34; data_dir = \u0026#34;/var/lib/garage/data\u0026#34; db_engine = \u0026#34;sqlite\u0026#34; metadata_auto_snapshot_interval = \u0026#34;6h\u0026#34; replication_factor = 1 compression_level = 2 rpc_bind_addr = \u0026#34;[::]:3901\u0026#34; rpc_public_addr = \u0026#34;localhost:3901\u0026#34; rpc_secret = \u0026#34;6864f83ced8f82b688eabfd49ce67687892776908e3f8d4714...\u0026#34; [s3_api] s3_region = \u0026#34;eu-west-1\u0026#34; api_bind_addr = \u0026#34;[::]:3900\u0026#34; root_domain = \u0026#34;.s3.domain.com\u0026#34; [s3_web] bind_addr = \u0026#34;[::]:3902\u0026#34; root_domain = \u0026#34;.web.domain.com\u0026#34; index = \u0026#34;index.html\u0026#34; [admin] api_bind_addr = \u0026#34;[::]:3903\u0026#34; admin_token = \u0026#34;0p0Au4l6WWRCV9EUNd+cdEu1Sy+16ez2ZGrtWa0EZnQ=\u0026#34; metrics_token = \u0026#34;MNguY7hRyQ2sMdCSrfm8KCSKEKtLx+IjMwFsRvRh5gE=\u0026#34; Remember to keep the rpc_secret, as well as the admin_token and metrics_token safe for production environments. For our use case this is not relevant, as this deployment is purely for benchmarking.\nWe build a custom Docker image with this configuration, as follows:\nFROM dxflrs/garage:v2.1.0 COPY infra/garage.toml /etc/garage.toml Compose Service # benchmark-garage: profiles: - benchmark build: context: .. dockerfile: infra/Dockerfile.garage networks: - benchmark volumes: - benchmark-garage:/var/lib/garage ports: - ${BENCHMARK_GARAGE_PORT:-3900}:3900 - 3901:3901 - 3902:3903 - 3903:3903 restart: unless-stopped We use the following environment variables:\nBENCHMARK_GARAGE_PORT to optionally set a different port for the Garage S3 API. Notice we don\u0026rsquo;t use BENCHMARK_STORE_ACCESS_KEY and BENCHMARK_STORE_SECRET_KEY here, as Garage doesn\u0026rsquo;t support specifying credentials for an admin user.\nInitialization # Like we said, Garage was the only S3-compatible object store, out of those that we tested, that did not provide an easy setup process for testing. As such, we were required to implement a just command named garage-init to take care of:\nLayout setup (single node with enough space). Access key creation (credentials for benchmarking). Benchmark bucket creation (warp-benchmark-bucket). Read/write permission granting. Creating an access key on Garage doesn\u0026rsquo;t let you specify the key ID or secret key, since this must be generated by Garage directly. Since we rely on shared credentials for the whole benchmark process (BENCHMARK_STORE_ACCESS_KEY and BENCHMARK_STORE_SECRET_KEY), we had to set up a separate benchmarking procedure for Garage.\nGarage should definitely make it easier to setup their object store for testing, providing environment variables to configure an initial admin user, like the other object stores do, and granting it permissions to create buckets, as well as to read and write to all buckets by default.\nYou can find the working implementation for just benchmark garage-init over here, but here\u0026rsquo;s the basic workflow we\u0026rsquo;re trying to reproduce (assume garage is in the PATH and this is the first run):\nnode_id=$(garage status 2\u0026gt;/dev/null \\ | tail -n1 | awk \u0026#39;{print $1}\u0026#39;) garage layout assign $node_id \\ --capacity 5G -z \u0026#34;eu-west-1\u0026#34; garage layout apply --version 1 create_key_output=$(garage key create admin 2\u0026gt;/dev/null) store_access_key=$(printf \u0026#34;%s\\n\u0026#34; \u0026#34;$create_key_output\u0026#34; \\ | grep \u0026#34;Key ID:\u0026#34; | awk \u0026#39;{print $3}\u0026#39;) store_secret_key=$(printf \u0026#34;%s\\n\u0026#34; \u0026#34;$create_key_output\u0026#34; \\ | grep \u0026#34;Secret key:\u0026#34; | awk \u0026#39;{print $3}\u0026#39;) garage bucket create \u0026#34;warp-benchmark-bucket\u0026#34; garage bucket allow --read --write --owner \\ \u0026#34;warp-benchmark-bucket\u0026#34; --key \u0026#34;$store_access_key\u0026#34; Once the initialization is done, we use a separate just command named garage that is similar to the remaining benchmarking commands, but uses the created credentials (store_access_key and store_secret_key) instead of the shared credentials set globally for the whole benchmark process. We\u0026rsquo;ll describe the benchmarking approach in the next major section.\nSeadweedFS # Unlike Garage, SeaweedFS offers a single convenience server command that takes care of starting the master, volume, filer and s3 gateway servers, the latter only when -s3 is specified. At first, we were setting up separate compose services for each of these components, which is also an option for a proper production setup, but we ended up following the easier route, once we found the server command.\nThis is how we setup the compose service, which will expose the S3 API on port 8333 (we ignore the remaining ports during benchmarking):\nbenchmark-seaweedfs: profiles: - benchmark image: chrislusf/seaweedfs:4.00 networks: - benchmark volumes: - benchmark-seaweedfs:/data ports: - 9333:9333 - 8080:8080 - 8888:8888 - ${BENCHMARK_SEAWEEDFS_PORT:-8333}:8333 environment: - AWS_ACCESS_KEY_ID=${BENCHMARK_STORE_ACCESS_KEY} - AWS_SECRET_ACCESS_KEY=${BENCHMARK_STORE_SECRET_KEY} command: server -dir=/data -s3 -s3.port=8333 restart: unless-stopped We use the following environment variables:\nBENCHMARK_SEAWEEDFS_PORT to optionally set a different port for the SeaweedFS S3 API. BENCHMARK_STORE_ACCESS_KEY and BENCHMARK_STORE_SECRET_KEY to set the user and password for admin. RustFS # For RustFS we hit a snag, where we had to switch the CPU type to host on the Proxmox Docker VM, otherwise it would complain about not finding SSE4.1 and PCLMULQDQ. The CPU type we were using before was a preset called x86-64-v2-AES that did list SSE4.1, but not PCLMULQDQ. The Docker VM had to be rebooted to apply the changes and enable the new CPU type, but we also had to destroy and recreate the RustFS service for it to stop complaining about the missing CPU features.\nWhile using the appropriate CPU flags for optimization is ideal‚Äîand, in all fairness, these have been around for over 15 years now‚ÄîRustFS might benefit from proper runtime checks, as the Docker image shouldn\u0026rsquo;t fail to run as long as the architecture matches.\nEither way, once this was fixed, we deployed RustFS as follows, exposing port 9000 as 10000 to avoid colliding with MinIO (we ignored the UI port 10001 during benchmarking):\nbenchmark-rustfs: profiles: - benchmark image: rustfs/rustfs:1.0.0-alpha.69 ports: - ${BENCHMARK_RUSTFS_PORT:-10000}:9000 - 10001:9001 networks: - benchmark volumes: - benchmark-rustfs:/data environment: - RUSTFS_ACCESS_KEY=${BENCHMARK_STORE_ACCESS_KEY} - RUSTFS_SECRET_KEY=${BENCHMARK_STORE_SECRET_KEY} - RUSTFS_CONSOLE_ENABLE=true restart: unless-stopped We use the following environment variables:\nBENCHMARK_RUSTFS_PORT to optionally set a different port for the RustFS. BENCHMARK_STORE_ACCESS_KEY and BENCHMARK_STORE_SECRET_KEY to set the user and password for admin. Benchmarking with Warp # Benchmarking was implemented using warp, by MinIO, and results analyzed using DuckDB, with everything orchestrated using just. We created a top-level project under benchmark/ inside our LabStore monorepo (see the release/backend/v0.1.0 branch, if unreleased). All infrastructure was deployed on a remote Docker host running inside a Proxmox VM with 2 vCPUs, 10 GiB of RAM, and 1 Gbps Ethernet.\nJust Commands # In order to run the benchmark process, we first need to ensure we have deployed the required infrastructure using Docker, by running just infra benchmark-up from the root of the repo. This will provision the Docker Compose services described in the previous sections.\nThen, from the root of the repo, running just benchmark all will trigger the benchmark process. This will measure the available bandwidth using iperf3, via the bandwidth command, and then run the benchmark for LabStore, MinIO, Garage, SeaweedFS, and RustFS, in this order, via the stores command. Both bandwidth and stores will produce several JSON output files that we then process using DuckDB via the analysis command. The output will be the ranking tables for DELETE, GET, PUT, and STAT requests, as we run warp in mixed mode, followed by a termgraph plot, both based on the median objects per second, processed for each request.\nResults # Here are the results, depicting the median for processed objects per second for each operation.\nAs we can see, LabStore is ranked first for all four operations, but please read on for a more critical insight. Below, we provide a more detailed view, including median bandwidth in MiB/s and percentages of bandwidth used. Please note that an operation is randomly picked during benchmarking, with the following assigned probabilities:\nDELETE: 10% GET: 45% PUT: 15% STAT: 30% As such, there might not be enough requests of a given type to exhaust the available bandwidth. Nevertheless, this is helpful for understanding efficiency and ranking the object stores. Regardless, in the tables below, we use Median Objects per Second for ranking, as bandwidth is not always available.\nDELETE # Rank Store Median Objects per Second Median MebiBytes per Second Bandwidth Usage Percentage 1 LabStore 227.00 0.00 0.00% 2 RustFS 191.56 0.00 0.00% 3 SeaweedFS 171.97 0.00 0.00% 4 MinIO 149.1 0.00 0.00% 5 Garage 76.03 0.00 0.00% GET # Rank Store Median Objects per Second Median MebiBytes per Second Bandwidth Usage Percentage 1 LabStore 1028.76 100.46 89.52% 2 RustFS 851.63 83.17 74.11% 3 SeaweedFS 763.01 74.51 66.40% 4 MinIO 665.45 64.99 57.91% 5 Garage 347.66 33.95 30.25% PUT # Rank Store Median Objects per Second Median MebiBytes per Second Bandwidth Usage Percentage 1 LabStore 341.37 33.34 29.68% 2 RustFS 285.28 27.86 24.80% 3 SeaweedFS 256.79 25.08 22.32% 4 MinIO 221.36 21.62 19.24% 5 Garage 116.3 11.36 10.11% STAT # Rank Store Median Objects per Second Median MebiBytes per Second Bandwidth Usage Percentage 1 LabStore 685.12 0.00 0.00% 2 RustFS 571.67 0.00 0.00% 3 SeaweedFS 515.39 0.00 0.00% 4 MinIO 441.92 0.00 0.00% 5 Garage 231.75 0.00 0.00% Final Results # As we can see, the rankings for all operations are in agreement, positioning the object stores as follows, in terms of single node/drive performance, under Docker:\nLabStore RustFS SeaweedFS MinIO Garage LabStore\u0026rsquo;s Performance # Wait, did LabStore, an object store written in Go, just beat RustFS?! Well\u0026hellip; Let\u0026rsquo;s be fair. A few notes on this to follow.\nFirst of all, with LabStore, we\u0026rsquo;re still early days. This means that, feature-wise, we aren\u0026rsquo;t nearly as complete as the competition, which might explain why we blasted through the rankings to first place. Keep in mind that most of the other object stores in the benchmark implement a lot more features than we do, which will surely add some overhead. For example, handling permissions and policies will require some sort of database to be queried, as well as proper security, so maybe next week RustFS will have us beat, when we add a basic IAM layer.\nRegardless, this is a pretty sweet start to the project, and it shows that Go can compete with Rust, definitely beat it in development flow‚Äîdebugging, compilation time, etc. At this stage, I\u0026rsquo;m looking back less and less, having decided to pick Go as the language for this project.\nProfiling in Go # It\u0026rsquo;s fairly easy to setup profiling in Go, using pprof, as this is provided as part of the Go tooling, via go tool pprof.\nThe required steps were the following:\nAdd profiling support to LabStore, enabling it through a CLI flag (we use --pprof). Launch labstore-server with profiling enabled, and run go tool pprof for our module, to capture data over 60 seconds, and expose the results via a web ui on port 8081. Simultaneously trigger the benchmark, right after pprof is launched, to simulate service load. Once the benchmark runs, we are able to explore the corresponding profiling results, to help us identify the time spent on each part of our code. A profiler is a tool as fundamental as a debugger, when performance matters (e.g., web services, databases, search engines, etc.).\nBelow we detail each of the previous three steps, showing the output from pprof, after running warp in mixed mode for benchmarking.\nIn-Code Profiler Support # The profiler is started using a goroutine that is a part of our code. The following snippet illustrates the main code that enables profiling in Go using pprof:\nimport ( _ \u0026#34;net/http/pprof\u0026#34; ) func (profiler *Profiler) Start() { slog.Info(\u0026#34;starting pprof profiler\u0026#34;, \u0026#34;port\u0026#34;, profiler.Port) go func() { addr := fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, profiler.Host, profiler.Port) fmt.Printf(\u0026#34;üïµÔ∏è‚Äç‚ôÇÔ∏è Profiler listening on http://%s\\n\u0026#34;, addr) log.Fatal(http.ListenAndServe(addr, nil)) }() } You can find the full code to instance and start a profiler in LabStore here.\nThat code is run from the root command in labstore-server, whenever the --pprof flag is set, based on the following block of code:\nrun_pprof := helper.Must(cmd.Flags().GetBool(\u0026#34;pprof\u0026#34;)) if run_pprof { pprof_host := helper.Must(cmd.Flags().GetString(\u0026#34;pprof-host\u0026#34;)) pprof_port := helper.Must(cmd.Flags().GetInt(\u0026#34;pprof-port\u0026#34;)) pprof := profiler.NewProfiler( profiler.WithHost(pprof_host), profiler.WithPort(pprof_port), ) pprof.Start() } You can find the full code for the labstore-server root command here.\nRunning App with Profiling # In order to run the profiler, we need to launch the following two commands.\nFirst, we run labstore-server serve, passing it the --pprof flag so that the goroutine opens a secondary profiling service, exposed on port 6060 by default, while LabStore will run on port 6789 by default as usual:\nbin/labstore-server --pprof serve Then, we run go tool pprof, pointing it to the module that we want to focus on (our backend), setting the web ui port to 8081‚Äîthis will open automatically once data capturing ends‚Äîand pointing it to the pprof service that we implemented on LabStore as described above‚Äîthe query parameter seconds will determine how long the profiler will capture data before opening the web ui to display the results:\ngo tool pprof \\ -focus=github.com/IllumiKnowLabs/labstore/backend \\ -http=:8081 \\ http://localhost:6060/debug/pprof/profile?seconds=60 In LabStore, we conveniently implement this process as a single Makefile target:\nprofile: backend npx concurrently \\ -n backend,pprof \\ -c blue,red \\ \u0026#34;$(BACKEND_CMD) --pprof serve\u0026#34; \\ \u0026#34;go tool pprof \\ -focus=github.com/IllumiKnowLabs/labstore/backend \\ -http=:8081 \\ http://localhost:6060/debug/pprof/profile?seconds=60\u0026#34; This relies on the concurrently CLI tool (via npm) to run LabStore\u0026rsquo;s backend with profiling enabled, in parallel with Go\u0026rsquo;s built-in pprof tool to capture and inspect performance data.\nSo we can simply run the profiler for 60 seconds with:\nmake profile Triggering the Benchmark # Once the profiler is running, we have 60 seconds to run whatever operation we want to analyze. A good default is to just trigger our benchmark, with:\njust benchmark store labstore You should see the access log for LabStore rapidly scrolling with DELETE, GET, PUT, and HEAD (STAT) requests, from within the terminal where you ran make profile:\nInspecting Results # Once the 60 seconds are up, the web ui for pprof should open automatically in your browser, on the specified port 8081, displaying the graph view by default:\nThis will display a node per function in your app, with arrows representing function calls and the total time spent in the subroutine. Each node is annotated with the package and function name, alongside the flat time spent directly inside that function (excluding subroutines), as well as the cumulative time spent in the function (including subroutines). For example, in our case, we see that we spend a lot of time in LoggingMiddleware and AuthMiddleware, and that the PutObjectHandler is slightly less efficient than the GetObjectHandler.\nHowever, a better way to understand how time is spent per function is perhaps the flame graph view, which we can access from the VIEW dropdown menu:\nThis view will clarify why the middleware was using so much time, as it becomes clearer that, despite LoggingMiddleware making a few calls to slog.Info, and AuthMiddleware spending some time running auth.VerifiySigV4, most of the time spent in both middleware routines is used in request handlers, PutObjectHandler, GetObjectHandler, DeleteObjectHandler, and HeadObjectHandler, in this order. However, it is also clear that a major slice of time is spent authenticating with SigV4, as well as in logging‚Äîin total, both take more time than either DELETE or HEAD requests together.\nAnd we can keep drilling down into each function by clicking the corresponding node. For example, we might look at GetObjectHandler in more detail:\nOr try to understand why auth.VerifySigV4 is using so much time to run:\nOr do the same for slog.Info:\nAnd we can click the root node at the top, to go back to the original view.\nBy looking at the three drilldown views above, it becomes clear that:\nGetObjectHandler is spending most of its time with I/O operations, which likely represent a hardware bottleneck rather than a design limitation, so there\u0026rsquo;s little we can do here, other than increasing our bandwidth or disk I/O speed, which are not the code-centric solutions, so there\u0026rsquo;s little to improve here, performance wise. auth.VerifySigV4, on the other hand, is spending way too much time on security.TruncParamHeader, which is a utility function to mask secrets in HTTP headers, currently used just for logging at a DEBUG level. As such, this can definitely be improved by lazily calling this function only when the logger is required to run! üí° slog.Info is spending most of its time with the log handler, specifically writing to the standard output. We might tackle this through async logging, or by using a different handler that is more efficient out-of-the-box. Also, we\u0026rsquo;ll need to test how much overhead the tint handler is introducing, and maybe just say goodbye to coloring, or at least make it optional. Whatever the approach is to fixing performance, at least now we know our bottlenecks, and we\u0026rsquo;re not driving blindly anymore! Remember that the simple act of running a profiler might help you identify several easy-to-fix bottlenecks that you can mitigate with little to no cost in terms development time, so always, ALWAYS have a profiler ready to go.\n","date":"9 December 2025","externalUrl":null,"permalink":"/posts/labstore-part-3-benchmarking/","section":"","summary":"Summary # Learn how to deploy multiple S3-compatible object stores, including LabStore, MinIO, Garage, SeaweedFS, and RustFS, using Docker Compose.","title":"LabStore - Part 3 - Building an Object Store in Go: Benchmarking and Profiling","type":"posts"},{"content":"","date":"9 December 2025","externalUrl":null,"permalink":"/tags/profiling/","section":"Tags","summary":"","title":"Profiling","type":"tags"},{"content":" Summary # Let\u0026rsquo;s learn all about the inner workings of AWS Signature Version 4 (SigV4), used in Amazon Simple Storage Service (AWS S3), or any S3-compatible object store, to authenticate requests. We\u0026rsquo;ll learn all about signature verification for standard single chunk requests, as well as for streaming multiple chunk requests.\nFollow this series with IllumiKnow Labs, and let\u0026rsquo;s see where this journey takes us. Hopefully you\u0026rsquo;ll learn a lot along the way!\n\u003e Implementing SigV4 # We\u0026rsquo;re going to discuss how to implement AWS Signature Version 4, used in Amazon Simple Storage Service (AWS S3), or any S3-compatible object store, to authenticate requests. This takes a secret key‚Äîeither associated with an access key or based on a user password‚Äîalong with request-specific elements, using them to compute a chain of \u0026ldquo;signed hashes\u0026rdquo; that will produce the final signature. This process is called HMAC (Hash-based Message Authentication Code) and, for SigV4, we use SHA256 to compute the hashes. The final signature should match the one computed and sent by the client in the Authorization header.\nEach request to an S3-compatible object store is signed using SigV4 and, for PUT /{bucket}/{key...} requests, where uploads are streamed in multiple chunks, each chunk is also signed in a particular way‚Äîit\u0026rsquo;s almost like a blockchain, where a chunk is signed based on the previous chunk. It first computes the seed signature using the standard SigV4 algorithm, signing the first chunk with that seed as the previous signature. The process is then repeated for the following chunks, always using the previous chunk\u0026rsquo;s signature as a part of the hash. Streaming always ends with a signed empty chunk (i.e., zero-size).\nBelow we introduce the specification, linking to the official documentation‚Äîwhich is all that you really need to implement SigV4‚Äîand we\u0026rsquo;ll discuss our Go-based implementation, as a part of LabStore.\nThe Spec # First of all, remember to trust the spec! See the docs for the standard single chunk SigV4 algorithm here and for the streaming multiple chunks SigV4 extension here.\nWhile debugging, you\u0026rsquo;ll need to check most of your code manually, looking at values used to compute the hash, either via logging or breakpoints, because the output is either a match or not. There\u0026rsquo;s a high dependency on code reading here and, as far as I know, this is the only way to debug this‚Äîif you have any additional tips, please do share on Discord.\nMake sure you have canonicalized everything correctly, that there are no missing or additional spaces or new lines that shouldn\u0026rsquo;t be there‚Äîtrue story, took me a while to find and fix an extra \\n‚Äîor that the credentials you\u0026rsquo;re using are correct and were properly set or loaded during server initialization. I have tested this using mc, rclone and s5cmd, and they all respect the spec as expected, so, if you find an issue, it\u0026rsquo;s most likely on your side.\nSingle Chunk # Canonical Request # A canonical request will look like this:\n\u0026lt;METHOD\u0026gt;\\n \u0026lt;PATH\u0026gt;\\n \u0026lt;QUERY_STRING\u0026gt;\\n \u0026lt;HEADERS\u0026gt;\\n \u0026lt;SIGNED_HEADERS\u0026gt;\\n \u0026lt;BODY_SHA256\u0026gt; METHOD (HTTP Verb) ‚Äì HTTP method (GET, PUT, POST, etc.) PATH (Canonical URI) ‚Äì URL encoded path (i.e. split by / and URL encoded) QUERY_STRING (Canonical Query String) ‚Äì Single line with query parameters, with URL encoded keys and values, following the same order as the original request HEADERS (Canonical Headers) ‚Äì lowercase header name and trimmed value, separated by :, no spaces, and ending on a \\n. SIGNED_HEADERS (Signed Headers) ‚Äì list of header names used in signing, separated by ; BODY_HASH (Hashed Payload) ‚Äì SHA256 hex string for the body content‚Äînotice that there is no \\n here‚Äîor UNSIGNED-PAYLOAD or STREAMING-AWS4-HMAC-SHA256-PAYLOAD For example:\nPUT /bucket/my/path/filename%282%29.txt key1=value1\u0026amp;key2=value2 x-amz-content-sha256:STREAMING-AWS4-HMAC-SHA256-PAYLOAD x-amz-date:20251119T103929Z x-amz-decoded-content-length:10485760 host;x-amz-content-sha256;x-amz-date;x-amz-decoded-content-length e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 StringToSign # We then build a string to sign, containing a few request headers and the canonical request:\nAWS4-HMAC-SHA256\\n \u0026lt;ISO8601_TIMESTAMP\u0026gt;\\n \u0026lt;SCOPE\u0026gt;\\n \u0026lt;CANONICAL_REQUEST_SHA256\u0026gt; \u0026quot;AWS4-HMAC-SHA256\u0026quot; ‚Äì used to specify the HMAC hashing algorithm (SHA256) ISO8601_TIMESTAMP ‚Äì timestamp from the original request‚Äîhere, you might validate the timestamp to make sure that it matches up to the day, hour or minute, but we don\u0026rsquo;t (we simply reuse the one from the request) SCOPE ‚Äì date, region and service identifier string in the format \u0026lt;YYMMDD\u0026gt;/\u0026lt;REGION\u0026gt;/\u0026lt;SERVICE\u0026gt;/aws4_request CANONICAL_REQUEST_SHA256 ‚Äì SHA256 hex string for the canonical request string‚Äînotice that there is no \\n here For example:\nAWS4-HMAC-SHA256 20251119T103928Z 20251118/eu-west-1/s3/aws4_request 7c7b924298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 Signature # The final signature is the calculated using HMAC-SHA256 based on:\nSecret key Scope date (YYMMDD) Scope region (e.g., eu-west-1) Scope service (usually s3) Scope request type (aws4_request) String to sign (STRING_TO_SIGN) Directly from the docs, this is how the chain of hashes is computed using HMAC-SHA256(key, value):\nDateKey = HMAC-SHA256(\u0026#34;AWS4\u0026lt;SECRET_KEY\u0026gt;\u0026#34;, \u0026#34;\u0026lt;SCOPE_DATE\u0026gt;\u0026#34;) DateRegionKey = HMAC-SHA256(DateKey, \u0026#34;\u0026lt;SCOPE_REGION\u0026gt;\u0026#34;) DateRegionServiceKey = HMAC-SHA256(DateRegionKey, \u0026#34;\u0026lt;SCOPE_SERVICE\u0026gt;\u0026#34;) SigningKey = HMAC-SHA256(DateRegionServiceKey, \u0026#34;aws4_request\u0026#34;) Signature = HMAC-SHA256(SigningKey, \u0026#34;\u0026lt;STRING_TO_SIGN\u0026gt;\u0026#34;) Notice that our SigningKey derives from the secret key and scope. We use it to sign the previously computed STRING_TO_SIGN, as described on the previous section.\nAnd that\u0026rsquo;s it. Most requests only need to implement this signature method. Once you\u0026rsquo;re done recomputing the signature, you simply compare it with the one provided by the client and let the request through if both the received and recomputed signatures match.\nMultiple Chunks # When we stream objects in multiple chunks, namely during upload requests like PUT /{bucket}/{key...}, we sign the request using the standard single chunk SigV4 algorithm we described in the previous section, but then we need to sign each individual request, chaining the signatures over sequential chunks.\nMost of the code used to implement the single chunk version can be reused here, with the exception of the string to sign, which is specific to chunks. A streaming request will be identified by a special payload hash STREAMING-AWS4-HMAC-SHA256-PAYLOAD (also set as the value for header x-amz-content-sha256).\nWe begin by computing the standard SigV4 signature as usual, and when STREAMING-AWS4-HMAC-SHA256-PAYLOAD is encountered, the body must be processed differently (i.e., signing or validating the signature for each chunk, depending on whether we\u0026rsquo;re doing this client-side or server-side, respectively). We implement this server-side, in Go, as a custom body reader.\nEach chunk has a header with the hex value of bytes to read, along with a signature. Here\u0026rsquo;s an example from the docs for a chunk of size 0x10000 bytes (65536 bytes):\n10000;chunk-signature=ad80c730a21e5b8d04586a2213dd63b9a0e99e0e2307b0ade35a65485a288648 \u0026lt;65536-bytes\u0026gt; StringToSign # Each chunk\u0026rsquo;s string to sign will be built as:\nAWS4-HMAC-SHA256-PAYLOAD\\n \u0026lt;ISO8601_TIMESTAMP\u0026gt;\\n \u0026lt;SCOPE\u0026gt;\\n \u0026lt;PREVIOUS_SIGNATURE\u0026gt;\\n \u0026lt;EMPTY_STRING_SHA256\u0026gt;\\n \u0026lt;CHUNK_DATA_SHA256\u0026gt; \u0026quot;AWS4-HMAC-SHA256-PAYLOAD\u0026quot; ‚Äì used to specify the HMAC hashing algorithm (SHA256) for a chunk (notice the PAYLOAD suffix) ISO8601_TIMESTAMP ‚Äì timestamp from the original request‚Äîhere, you might validate the timestamp to make sure that it matches up to the day, hour or minute, but we don\u0026rsquo;t (we simply reuse the one from the request) SCOPE ‚Äì date, region and service identifier string in the format \u0026lt;YYMMDD\u0026gt;/\u0026lt;REGION\u0026gt;/\u0026lt;SERVICE\u0026gt;/aws4_request PREVIOUS_SIGNATURE ‚Äì signature for the previous chunk, or the seed signature from the standard SigV4 signature (first one computed, like for any other request) EMPTY_STRING_SHA256 ‚Äì SHA256 for an empty string (don\u0026rsquo;t ask me why) CHUNK_DATA_SHA256 ‚Äì SHA256 for the chunk data‚Äînotice that there is no \\n here For example:\nFor example:\nAWS4-HMAC-SHA256-PAYLOAD 20251119T103928Z 20251118/eu-west-1/s3/aws4_request 3f256085d50823c4971f2aaa1be1816e9fa5c948352bbbbc81762dd0f84cc237 e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 6480a50b3148fb7b68bf1c3bfdf616a507cf44cf536cdda6e47f0b4c5cb22876 Client-side, don\u0026rsquo;t forget to add a zero-size chunk to mark the end of the stream. Likewise, server-side, we must look for a chunk header with size zero, so that we know the stream is over.\nApart from parsing the chunks as individual units and having to build a custom string to sign, we can still use the same computation approach as described in the Signature section of the standard single chunk approach.\nClass Diagrams # Having described the inner-workings of SigV4, we now detail our particular implementation using on Go and its net/http standard library.\nWe first provide class diagrams that cover the most relevant package-level functions for auth, as well as the types, receiver methods, and functions created to implement SigV4 for single chunk and multiple chunk scenarios. We close with a sequence diagram illustrating the whole procedure.\nPackage-Level Functions # --- title: AWS Signature Version 4 (SigV4) ‚Äì Package-Level Functions (auth/*.go) --- classDiagram class auth { +VerifySigV4(r *http.Request) :\u0026nbsp;(*sigV4Result, error) +NewSigV4ChunkedReader(r *http.Request, res *sigV4Result) *sigV4ChunkedReader -newSigV4Request(r *http.Request) :\u0026nbsp;(*sigV4Request, error) -newSigV4Authorization(authorization string) :\u0026nbsp;(*sigV4Authorization, error) -newSigV4Credential(credential string) :\u0026nbsp;(*sigV4Credential, error) -computeSignature(cred *SigV4Credential, stringToSign string) :\u0026nbsp;(string, error) } We implement signature verification as a net/http middleware (middleware.AuthMiddleware). Each request to the server will go through this middleware, which calls VerifySigV4‚Äîthis is the standard SigV4 algorithm.\nUpon entry on VerifySigV4, the http.Request will be parsed using newSigV4Request‚Äîthis will call newSigV4Authorization, which in turn will call newSigV4Credential. Other functions will also be called internally to build the canonicalURI, the canonicalQueryString, and the canonicalHeaders. Other fields will be loaded from HTTP headers‚Äîthe timestamp from X-Amz-Date, and the payloadHash from X-Amz-Content-SHA256.\nThis will produce a sigV4Request instance, from which we will run the verification process.\nCheck out auth/auth.go for more details.\nSingle Chunk Signature Types # --- title: AWS Signature Version 4 (SigV4) ‚Äì Single Chunk (auth/signature.go) --- classDiagram class sigV4Request { -method : string -canonicalURI : string -canonicalQueryString : string -canonicalHeaders : map[string]string -authorization : *sigV4Authorization -timestamp : string -payloadHash : string -validatePayloadHash(r *http.Request) error -validateSignature() :\u0026nbsp;(*SigV4Result, error) -buildCanonicalRequest() string -buildStringToSign() string } class sigV4Authorization { -credential : *sigV4Credential -signedHeaders : []string -signature : string } class sigV4Credential { +AccessKey : string -secretKey : string -scope : string } class sigV4Result { +Credential : *sigV4Credential +Signature : string +Timestamp : string +IsStreaming : bool } sigV4Request --\u003e sigV4Authorization sigV4Authorization --\u003e sigV4Credential sigV4Result --\u003e sigV4Credential Here you can see the types produced by the constructors in the previous section. The only one missing is sigV4Result, which is produced by sigV4Request.validateSignature().\nBefore validating the signature, we also validate the payload hash using sigV4Request.validatePayloadHash(), which works as an early fail for invalid payload hashes. However, this is not a part of SigV4, and it will likely be removed in the future to improve performance‚Äîthis is not required, since overall validation already includes the payload hash anyway.\nThe remaining methods on sigV4Request are called from validateSignature, buildStringToSign directly, and buildCanonicalRequest from buildStringToSign.\nThe parsed sigV4Credential instance is also returned on the sigV4Result, as it is used to set the active access key on the request context. The sigV4Result instance is also used to check for an upcoming stream with multiple chunks and passed to the sigV4ChunkedReader, which is described below.\nCheck out auth/signature.go for more details.\nMultiple Chunks Signature Types # --- title: AWS Signature Version 4 (SigV4) ‚Äì Multiple Chunks (auth/streaming.go) --- classDiagram class sigV4ChunkedReader { -body : io.ReadCloser -prevSig : string -credential : *SigV4Credential -timestamp : string -reader: *bufio.Reader -header: *SigV4ChunkHeader -data: []byte +Read(buf []byte) :\u0026nbsp;(int, error) +Close() error -readChunkHeader() error -readChunkData() error -readTrailingCRLF() error -verifyChunkSigV4() error -buildChunkStringToSign() string } class sigV4ChunkHeader { -size : int -signature : string } note for sigV4Credential \"From auth/signature.go ‚Äì returned after calculating the seed signature.\" class sigV4Credential { +AccessKey : string -secretKey : string -scope : string } sigV4ChunkedReader --\u003e sigV4ChunkHeader sigV4ChunkedReader --\u003e sigV4Credential Whenever the standard SigV4 algorithm returns a sigV4Result instance with IsStreaming set to true, we replace the body of the r *http.Request‚Äîr.Body‚Äîwith a sigV4ChunkedReader, which implements the io.ReadCloser interface, just like r.Body. This implements a custom Read() method that iterates over individual chunks, verifying their signature, and returning the pure bytes for the object being streamed (usually uploaded).\nThis means that Read() will readChunkHeader(), which contains the hexadecimal size of the chunk data, as well as the signature to validate. Then we readChunkData() and readTrailingCRLF(), calling verifyChunkSigV4() to validate the signature. The previous signature (prevSig) will be set for the next chunk‚Äîfor the first chunk, this is set to the standard SigV4 signature, also called seed signature. Once the chunk header returns a size zero, we know the stream has ended and we terminate the the reading process by returning the number of bytes read and a nil error.\nNotice that the Read() method also needs to handle partial reads, for when the buf []byte parameter is smaller than the chunk body.\nCheck out auth/streaming.go for more details.\nSequence Diagram # The sequence diagram below serves to illustrate the process we described in the previous sections. We refrain from including too much detail, and focus on the top-level logic. Inspect the source code linked across the previous sections for further detail.\nThis should, however, provide enough detail to make it clear how the SigV4 end-to-end verification process works. The example below is for a PUT request, this way covering both the standard algorithm for single chunks, and the streaming algorithm for multiple chunks.\n--- title: AWS Signature Version 4 (SigV4) ‚Äì PutObject --- sequenceDiagram autonumber actor Client Client-\u003e\u003erouter/router.go: PUT /{bucket}/{keys...} router/router.go-\u003e\u003emiddleware/auth.go: r *http.Request critical Standard SigV4 middleware/auth.go-\u003e\u003eauth/signature.go: auth.VerifySigV4(r) auth/signature.go-\u003e\u003eauth/signature.go: req ‚Üê newSigV4Request(r) auth/signature.go-\u003e\u003eauth/signature.go: req.validatePayloadHash() auth/signature.go-\u003e\u003eauth/signature.go: res ‚Üê req.validateSignature() auth/signature.go-\u003e\u003emiddleware/auth.go: res *sigV4Result option res.IsStreaming middleware/auth.go-\u003e\u003eauth/streaming.go: auth.NewSigV4ChunkedReader(r, res) auth/streaming.go-\u003e\u003emiddleware/auth.go: r.Body ‚Üê *auth.sigV4ChunkReader option No errors middleware/auth.go-\u003e\u003emiddleware/auth.go: Add access key to request context end middleware/auth.go-\u003e\u003erouter/router.go: Next handler router/router.go-\u003e\u003eClient: Response We begin from the client (CLI, web, etc.). The request is handled by our http.ServeMux router and sent to our middleware.AuthMiddleware. This calls the auth package, which handles verification, either returning an error along the way or calling the next handler across the middleware chain, until the client receives a response‚Äîthis response can obviously be the result of an http.Error call.\nAnd that\u0026rsquo;s how you implement SigV4 in Go! :-)\n","date":"25 November 2025","externalUrl":null,"permalink":"/posts/labstore-part-2-sigv4-auth/","section":"","summary":"Summary # Let\u0026rsquo;s learn all about the inner workings of AWS Signature Version 4 (SigV4), used in Amazon Simple Storage Service (AWS S3), or any S3-compatible object store, to authenticate requests.","title":"LabStore - Part 2 - Building an Object Store in Go: Authenticating with SigV4","type":"posts"},{"content":"","date":"25 November 2025","externalUrl":null,"permalink":"/tags/sigv4/","section":"Tags","summary":"","title":"Sigv4","type":"tags"},{"content":" Summary # Let\u0026rsquo;s learn all about the inner workings of S3 object stores, along with enough Go to start building your own solution.\nAs MinIO distances itself from Open Source Software, losing karma with the community, there\u0026rsquo;s no better time to discover more on how to build our very own S3-compatible object store! So let\u0026rsquo;s delve into the inner workings of the AWS S3 Object Store API, learning the basics of Go, and introducing LabStore, a new project by IllumiKnow Labs, where, together with the community, we\u0026rsquo;ll build yet another OSS solution for object storage, this time ensuring that it remains open and free‚Äîno features removed!\nEmbark in this series with IllumiKnow Labs, and let\u0026rsquo;s see where this journey takes us. Hopefully you\u0026rsquo;ll learn a lot along the way!\n\u003e MinIO - The End of a Chapter # MinIO has grown to be loved by the community as the de facto open source solution for on-premise S3 object storage. Unfortunately, more recently they decided to follow a different direction, pushing all their OSS efforts into the background, which has resulted in widely spread negative feedback within the community.\nFirst, they removed critical features from their UI, which led users like me to keep running older Docker image versions, and others, like the user on this Reddit post, to create an alternative UI that brings back removed features. Unfortunately, the most likely outcome here is that those features will also be removed from the community edition backend in the near future, so this might not be a viable solution for the open source or self-hosting communities.\nSome users also decided to create a fork. This is what\u0026rsquo;s great about open source‚Äîit will remain open as long as someone maintains it. Unfortunately, there are also a few challenges here, regarding trust and fragmentation. First of all, it\u0026rsquo;s likely that multiple forks will appear. Not counting the ones done for archival, personal use, or testing, some of them will also be quickly abandoned. Until we know that there are serious devs behind a fork, and that users are adopting that fork, it will be hard to make a decision. The likely scenario here is that users will simply adopt another alternative for S3 object storage, completely dropping any MinIO-related distribution.\nBut it gets worse\u0026hellip; Recently, MinIO decided that their community edition (CE) would be a source only distribution, scrapping their prebuilt binaries along with their Docker image builds. While it looks like they are keeping older images, they will not be building any new binaries or images for MinIO CE.\nRegarding Docker images, users have been discussing the issue here and here, highlighting that, while the community can build their own binaries or images, these might easily be compromised, which is a major breach in trust.\nBut it gets even worse\u0026hellip; On top of moving to a source only distribution, they are also ceasing further development apart from bug fixes or security patches‚Äîthe community has made notice that MinIO is no longer in active development.\nIt looks like they are turning their focus away from their original product, and into their newer MinIO AIStor solution, developed on a separate codebase, and offering only a commercial license. I wonder if this strategy will work for them, after losing so much karma with the community. Users are calling it a bait and switch, while progressively looking for alternatives.\nOf course it will be hard to get an alternative for the new features that AIStor will bring, as this competes directly with Amazon S3 Tables, offering AI/ML lifecycle data management features, along with features to simplify the management of open table formats, like Iceberg, Hudi, or Delta Lake, that support data lakehouses‚Äîno DuckLake though üòü.\nIn all honesty, direction wise, this is all great! It makes sense. I just don\u0026rsquo;t understand the decision to essentially kill the open source solution. It could easily have been planned to fit their recent goals, and, with little effort, avoid compromising the positive standing they cultivated with the community.\nRegardless, if you\u0026rsquo;re using MinIO, this is the time to make a decision. Either switch to AWS S3 directly, opt to pay for a commercial license, or switch to another alternative. And there are a few, like Garage, SeaweedFS, RustFS, or even Ceph with the RADOS gateway. In a future video, we\u0026rsquo;ll test and benchmark these alternatives.\nImplementing an S3 Object Store # How hard can it be, right? Is it complicated? Not really. Can it get complex? Definitely. SigV4 is extensive and, since it\u0026rsquo;s all hashes, the slightest mistake will produce the wrong signature‚Äîand this is hard to debug, as it\u0026rsquo;s mostly manually checking the code. On top of that, the API is also deceptively simple, with each request making extensive use of HTTP headers, each requiring individual attention.\nBeyond the AWS S3 API, we also need to take a look at IAM, establishing an approach to manage our users, groups, policies, etc. This is not a part of S3, but a requirement for it to work properly. It\u0026rsquo;s easy to implement a root user and work from there, but it\u0026rsquo;s harder if you want proper identity and access management (IAM). We don\u0026rsquo;t have a full plan for this yet, but we\u0026rsquo;ll likely reproduce AWS IAM\u0026rsquo;s requests as well.\nSo, what\u0026rsquo;s the fastest path to an MVP? We need to categorize requests, prioritize them, and, for each one, decide on the minimum headers and response elements to make it work. At this stage, we won\u0026rsquo;t be fully compliant with the API, but, to be clear, no one is. Compliance is fractional‚Äîeach open source object store will provide a table with the list of supported requests, while completeness details per request won\u0026rsquo;t be provided.\nLet\u0026rsquo;s then plan, based on the following resources:\nAWS Signature Version 4 https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-header-based-auth.html https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-streaming.html AWS Simple Storage Service (S3) - API Reference https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations_Amazon_Simple_Storage_Service.html AWS Identity and Access Management - API Reference https://docs.aws.amazon.com/IAM/latest/APIReference/API_Operations.html üü• P0 ‚Äì Critical # These are the absolute minimal actions required for an object store to do its job. Each request should take an Authorization header with the corresponding SigV4 hash. Here, if it\u0026rsquo;s just a toy project, you can simply ignore that header and accept all requests regardless, but, if you\u0026rsquo;re serious about it, like we are, then implementing SigV4 should be top priority as well.\nSo also consider SigV4 to be üü• P0 ‚Äì Critical.\nDon\u0026rsquo;t worry about the details on SigV4 now, as I\u0026rsquo;ll write a blog post dedicated to this next week, alongside the customary YouTube video.\nSo, top priority should be for bucket and object CRUD, as we can see below. For each S3 Action, we show the method, path, and a basic description below. Each action links to the official documentation as well.\nBuckets # S3 Action Method Path Description ListBuckets GET / List all buckets CreateBucket PUT /{bucket} Create bucket DeleteBucket DELETE /{bucket} Delete bucket ListObjects GET /{bucket} List objects in bucket ListObjectsV2 GET /{bucket}?list-type=2 List objects in bucket (V2) HeadBucket HEAD /{bucket} Check bucket existence Objects # S3 Action Method Path Description PutObject PUT /{bucket}/{key} Upload an object GetObject GET /{bucket}/{key} Download an object HeadObject HEAD /{bucket}/{key} Get metadata DeleteObject DELETE /{bucket}/{key} Delete an object CopyObject PUT /{bucket}/{key}?x-amz-copy-source=... Copy object üüß P1 ‚Äì High # Once we get our object store doing basic authenticated bucket and object CRUD, we\u0026rsquo;ll then focus on implementing multipart uploads, so we can upload large objects using multiple parallel requests, and then implementing IAM, covering bucket ACL and policy configuration, as well as the actual API to manage users, groups, policies, etc.\nIn the following sections, we cover the requests that we found to be the most relevant for each item.\nObjects: Multipart Uploads # S3 Action Method Path Description ListMultipartUploads GET /{bucket}?uploads List ongoing multipart uploads CreateMultipartUpload POST /{bucket}/{key}?uploads Initiate upload UploadPart PUT /{bucket}/{key}?partNumber={n}\u0026amp;uploadId={id} Upload part ListParts GET /{bucket}?uploadId={id} List parts CompleteMultipartUpload POST /{bucket}/{key}?uploadId={id} Complete upload AbortMultipartUpload DELETE /{bucket}/{key}?uploadId={id} Abort upload Buckets: Configuration # S3 Action Method Path Description GetBucketAcl / PutBucketAcl GET/PUT /{bucket}?acl User/group permissions GetBucketPolicy / PutBucketPolicy / DeleteBucketPolicy GET/PUT/DELETE /{bucket}?policy IAM-style JSON policy GetBucketPolicyStatus GET /{bucket}?policyStatus Check if the bucket is public IAM: Identity and Access Management # IAM Action Method Path Description CreateUser POST /?Action=CreateUser Create user (no password / can\u0026rsquo;t login) CreateLoginProfile POST /?Action=CreateLoginProfile Set password for user (can login) CreateAccessKey POST /?Action=CreateAccessKey Create user access key (API only auth / can\u0026rsquo;t use for login) ListUsers POST /?Action=ListUsers List users for a path prefix GetUser POST /?Action=GetUser Get user by name DeleteUser POST /?Action=DeleteUser Delete user by username CreateGroup POST /?Action=CreateGroup Create group AddUserToGroup POST /?Action=AddUserToGroup Add user to group using names ListGroups POST /?Action=ListGroups List groups for a path prefix ListGroupsForUser POST /?Action=ListGroupsForUser List groups for a given user DeleteGroup POST /?Action=DeleteGroup Delete group by name üü® P2 and üü© P3 # Lower priority requests will include actions covering:\nBucket versioning, encryption, object locking, and CORS. Bucket configurations covering lifecycle, replication, logging, notification, metrics, etc. Other object-related actions, like copying parts from existing objects during multipart uploads, tagging, setting retention policies, etc. Other IAM-related actions, like attaching global policies to users or groups, etc. Backend Programming Language # Long story short, the backend will be written in Go. As this is my first time coding in Go, I\u0026rsquo;ll document a lot of what I learned over the past two weeks, and share it with you. First, I\u0026rsquo;ll go through the decision making process that led me to pick Go over Rust‚Äîand no, there is no clear \u0026ldquo;winner\u0026rdquo;, just a best option for my use case. Then, I\u0026rsquo;ll cover several basic language features that are fundamental knowledge if you\u0026rsquo;re starting to code in Go right now. Hopefully, this works a good primer to get you started in Go as well.\nWhy Go? # When selecting the language to code the backend, it was between Go and Rust. To be honest, I looked at Go\u0026rsquo;s syntax and it felt downright weird.\nif err := f(); err != nil { // ... } var map[string]int m var y int x := 10 y = 20 Two statements in the if condition? Why are map types declared this way? Are there two assignment operators? What?!\nRust felt complex, but not weird, at least to me. And, on the other hand, we all know how efficient tools coded in Rust can be‚Äîuv, just, ripgrep, bat, fd, exa, and the list goes one. So, at first, I was leaning towards Rust.\nWhy did I end up picking Go then? And do I regret it? No regrets here! Let me tell you why.\nFirst, about the syntax, the weirdness goes away quite fast. Go is consistent. It\u0026rsquo;s also minimalistic, and it flows quite well as you type.\nThe two statements in the if condition are just like the ones in a for loop: initialization and condition. Neat, right?\nRegarding map declaration, because Go didn\u0026rsquo;t implement generics originally, this is how to \u0026ldquo;return type\u0026rdquo; for maps is declared, and this is consistently followed for functions as well: func GetMapItem(...) int. Again, consistency!\nWhy the two assignment operators? Notice that y was declared, but x wasn\u0026rsquo;t. When we use :=, we\u0026rsquo;re declaring on assignment, and the compiler will determine the correct variable type at that stage. Conversely, you use = when you\u0026rsquo;re assigning an existing variable. This keeps the code readable and minimalistic.\nAlso, while I won\u0026rsquo;t cover these below, goroutines and channels are a native feature of the language, approaching concurrency in a beautiful way. Watch Concurrency is not Parallelism by Rob Pike for a few cool examples!\nThe video above was recommended by the community on a Reddit post somewhere. I also had separate discussions on other topics, like net/http versus other web frameworks, so thanks for that! As a first impression, the community seems nice and helpful‚Äîthere are a few tech savvy and professional members around as well. This is fundamental for the success of a language‚ÄîPython is a good example of this.\nFinally, let\u0026rsquo;s talk tooling. Compilation time is the fastest I\u0026rsquo;ve ever seen. I can compile and run my whole backend faster than I can print the help message for datalab\u0026rsquo;s dlctl command, which is written in Python! WAT! There is go fmt to format my code. I can install system commands directly using go install, add project dependencies using go get, or cleanup unused dependencies by using go mod tidy. These dependencies are all listed under pkg.go.dev‚Äîat least I haven\u0026rsquo;t found any that aren\u0026rsquo;t yet‚Äîwith documentation available straight in Go\u0026rsquo;s main domain.\nWhat would have I lost by going the Rust route? More time learning the language, debugging the code, or interpreting compiler errors, more time waiting for the compiler to run, more time installing dependencies and reading their docs, etc. In essence, I would have traded development time for performance. There is a reason the compiler is so strict in Rust. It\u0026rsquo;s doing so much upfront! And there is not garbage collector in Rust, but we have one in Go. However, development is not all about how efficient your code is, but also about how easily you can find coders for your language, how fast you can iterate over your code, and how fast you can deliver a working product.\nThe beauty of it all is that you can later refactor the slowest parts of your Go code to run in Rust, if you really can\u0026rsquo;t squeeze any more performance out of Go. I\u0026rsquo;d say that most projects can\u0026rsquo;t go wrong with Go. On the other hand, I\u0026rsquo;d consider carefully when to use Rust.\nUnderstanding the Basics # There are so many lessons or tutorials teaching Go, so I won\u0026rsquo;t do that. Instead I\u0026rsquo;ll cover a few of the basic language features by giving my impression on them. I want to capture more of how they feel, rather than delve deep into details. So I\u0026rsquo;ll compare a bit with other languages I\u0026rsquo;ve used in the past, or cover details that usually don\u0026rsquo;t come up that much, like function suffixes or other conventions.\nComposable Types and Interfaces # Let\u0026rsquo;s begin with the basics: composition!\nYou can define composed types as follows:\ntype Response struct { statusCode int Message string } type ErrorResponse struct { Response ErrorCode int } type UserSession struct { UserID string Token string } type LoginSuccessResponse struct { Response Data UserSession } And it\u0026rsquo;s similar for interfaces:\ntype Responder interface { Body() string Log() } type DataResponder interface { Responder Data() any } If you\u0026rsquo;re coming from Scala, this is a bit like traits, but without the need for extends... with..., which provides a weaker consistency syntax. If you\u0026rsquo;re coming from Python, this is like multiple-inheritance‚Äîusing mixins‚Äîbut without initialization dilemmas:\nDo I use super().__init__(**kwargs)? Or just break composability and call MixinA.__init__(arg1, kwarg1=...)? If you\u0026rsquo;re coming from TypeScript, this is like the \u0026amp; operator for type composition, although for multiple-inheritance of methods it can get harder in TypeScript, as there\u0026rsquo;s no native support there.\n\u0026ldquo;Private\u0026rdquo; and \u0026ldquo;Public\u0026rdquo; Identifiers # In Go, identifier case matters. We use camelCase identifiers for private/unexported variables‚Äîaccessible from inside the package‚Äîand PascalCase for public/exported variables‚Äîaccessible from anywhere. Notice that unexported/exported is the preferred naming. There are really no private or public identifiers in Go.\nIn the example from the previous section, we kept statusCode as unexported, since it will be used to set the HTTP status code, but it won\u0026rsquo;t be included in the response body, which brings us to he next section\u0026hellip;\nSerialization / Marshalling / Encoding # In Go, standard encoders, like encoding/json, encoding/xml, or encoding/gob, don\u0026rsquo;t export camelCase variables‚Äîand, by the way, gob is to Go what pickle is to Python.\nAnother useful feature that Go provides is annotations besides struct fields. These are called tags. They are often used by encoding libraries to set the field\u0026rsquo;s output name, default value, or exclusion rules.\nHere\u0026rsquo;s an example, where UserID will be encoded as id in the corresponding JSON object, and Token, despite being an exported field, will be excluded from encoding:\ntype UserSession struct { UserID string `json:\u0026#34;id\u0026#34;` Token string `json:\u0026#34;-\u0026#34;` } The encoding process will look something like this:\nsession := UserSession{ UserID: \u0026#34;034597ff-f083-41b0-aa6b-2cd1fea83a5d\u0026#34;, Token: \u0026#34;5521f71d5ee3150fbba9ecbbfee7517feadffd65784f8f257a92bca6d56bf41b\u0026#34; } data, _ := json.Marshal(session) fmt.Println(string(data)) // {\u0026#34;id\u0026#34;: \u0026#34;034597ff-f083-41b0-aa6b-2cd1fea83a5d\u0026#34;} Notice that, when calling json.Marshal, we discarded the second element of the return value, but we shouldn\u0026rsquo;t have‚Äîthis returns an error, which we should handle explicitly, but more on that below.\nEnum-Like Constants # Another interesting feature in Go is iota, which we can use to produce enum-like constants with sequential integer values. It is recommended that these constants are typed explicitly.\nHere\u0026rsquo;s an example for an enum starting from 1:\ntype Status int const ( _ = iota Starting Status Running Failed Success ) Notice we assign iota to _, so we discard the initial value, which is zero. We could have used arithmetic as well:\nconst Starting Status = iota + 1 If a function then takes an argument with type Status, we know it will refer to one of these constants:\nfunc StatusDescription(Status s) string { switch s { case Starting: return \u0026#34;Process is starting...\u0026#34; case Running: return \u0026#34;Process is running...\u0026#34; case Failed: return \u0026#34;Error: process has failed!\u0026#34; case Success: return \u0026#34;Success: process completed without errors.\u0026#34; default return \u0026#34;[INVALID STATUS]\u0026#34; } } But, unlike enums in other languages, it can still take regular integers as arguments:\nStatusDescription(Starting) // Process is starting... StatusDescription(1000) // [INVALID STATUS] Defining Receiver Methods for Types # Using the previous example, we could have defined the description function as follows:\nfunc (s Status) Description() string { switch s { case Starting: return \u0026#34;Process is starting...\u0026#34; case Running: return \u0026#34;Process is running...\u0026#34; case Failed: return \u0026#34;Error: process has failed!\u0026#34; case Success: return \u0026#34;Success: process completed without errors.\u0026#34; default: return \u0026#34;[INVALID STATUS]\u0026#34; } } Adding (s Status) before the function name turns it into a receiver method on Status‚Äîit\u0026rsquo;s kinda like self on Python, but you can give it a custom name, and it works with pointers as well.\nThis way, we would call it as:\nStarting.Description() // Process is starting... Status(1000).Description() // [INVALID STATUS] Error Handling # No exceptions, only errors\u0026hellip; Or panic! Ideally, you handle your errors, but sometimes you might want to print a message and exit immediately. You can do that by with:\npanic(\u0026#34;Oh no!\u0026#34;) log.Fatal(\u0026#34;Oh no! But logged.\u0026#34;) Of course handling the error is better‚Äîand don\u0026rsquo;t just ignore it either. Here are a few insights into errors in Go:\nMessages should be lower case, for composability (i.e., combining error messages). errors.New produces a basic error from a message You can use your own custom error types, as long as you implement the error interface, which is native to Go. errors.Is checks for a custom error type. errors.As casts and error into a custom error type. There\u0026rsquo;s no need for errors.Is when using errors.As, since it returns false when it doesn\u0026rsquo;t match. Combining error messages is called wrapping and can be done with fmt.Errorf and %w. Even if there is panic, it can still be intercepted with recover when needed. For example, we currently define the following custom error type for LabStore:\ntype S3Error struct { XMLName xml.Name `xml:\u0026#34;Error\u0026#34;` Code string Message string RequestId string HostId string StatusCode int `xml:\u0026#34;-\u0026#34;` } func (e *S3Error) Error() string { return fmt.Sprintf(\u0026#34;%s: %s\u0026#34;, e.Code, e.Message) } err := \u0026amp;S3Error{ Code: \u0026#34;NotImplemented\u0026#34;, Message: \u0026#34;Operation not implemented\u0026#34;, StatusCode: http.StatusNotImplemented, } We can wrap it as follows:\nfmt.Println(fmt.Errorf(\u0026#34;server error: %w\u0026#34;, err)) And handle it among other regular or custom errors as follows:\nfunc HandleError(w http.ResponseWriter, err error) { logrus.Errorf(\u0026#34;Server error: %s\u0026#34;, err) var s3Error *S3Error if errors.As(err, \u0026amp;s3Error) { WriteXML(w, s3Error.StatusCode, s3Error) } else { http.Error(w, err.Error(), http.StatusInternalServerError) } } If one of the libraries you use calls panic, but you want to handle it rather than exiting:\nfunc PanicRequest() { panic(\u0026#34;oh no!\u0026#34;) } func HandlePanic() { if err := recover(); err != nil { log.Printf(\u0026#34;Panic recovered: %s\u0026#34;, err) } } func PanicDemo() { defer HandlePanic() PanicRequest() fmt.Println(\u0026#34;NOT PRINTED\u0026#34;) } func main() { PanicDemo() fmt.Println(\u0026#34;PRINTED\u0026#34;) } When panic occurs inside PanicDemo(), the function will still return immediately, triggering HandlePanic(), which will recover from panic, log the error, and resume the program cleanly. This means that, any lines after PanicDemo() on main() will still run. This is useful for instance when you\u0026rsquo;re building your own HTTP service and need to make sure a request that produces panic will never shutdown the whole server.\nPrinting and Logging Conventions # When using libraries that print messages, be it fmt or logrus, there are a few common suffixes (or lack thereof) that are used.\nHere are a few examples based on fmt and logrus:\nNo suffix ‚Äî fmt.Print, fmt.Sprint, logrus.Info, logrus.Debug Concatenates all arguments, without spaces, using %v to produce the string representation. ln suffix ‚Äî fmt.Println, fmt.Sprintln, logrus.Infoln, logrus.Debugln Concatenates all arguments, separated by spaces, using %v to produce the string representation. f suffix ‚Äî fmt.Printf, fmt.Sprintf, logrus.Infof, logrus.Debugf Takes a format string (e.g., \u0026quot;Error: %v\u0026quot;) and as many additional arguments as format verbs (one for %v, in this example). Fn suffix ‚Äî logrus.InfoFn, logrus.DebugFn Not a Go convent. This one is mostly specific to logrus, so that a helper function can be used to log more complex workflows by returning an array of string. HTTP Requests with net/http # This one was hard on me. I started building LabStore with net/http, but, as I was feeling that routing was a bit too painful, I decided to search for a better alternative. And what better way to do so than to look for a good benchmark? The old scientific method‚Äîexperimentation!\nSo I found this 2022 benchmark, where Fiber was the clear all-around winner. It was able to handle 28% more requests per second than net/http, while allocating 87.5% less memory, for 5000 concurrent requests. It really stood ou on that benchmark! Since it was based on a different library called fasthttp, and not net/http, like most of the other frameworks, it was settled, I was going to migrate to fiber.\nBefore taking a few days break, did the migration, and it was working fine. Then, as I was resting, I was browsing the r/golang subreddit and found a post of someone trying to find a popular web framework to use. The most upvoted comment suggested using net/http, so I decided to add up to the discussion and talk about fiber, as I had just successfully done the migration a couple of days back. This led to a long discussion, culminating in an upset stomach, because I felt I had made a mistake by jumping the gun on migrating out of net/http.\nEven though fiber had better numbers on benchmarks, my main reason to switch was because I needed a router, which I thought needed to be a third-party library, like httprouter, but I quickly realized that net/http has its own router, called http.ServeMux. Now, I avoid premature optimizations, after many lessons learned in the past. More often than not, the best optimizations are done by refactoring your own code for performance. Of course the libraries you use and the language your software is written with matter, but not as much as your implementation.\nThat was enough to convince me I had wasted my time migrating, but not enough to convince me I was not better of with fiber‚Äîafter all, it was more efficient. Still, annoyed at my mistake, I decided to look for more up-to-date benchmarks. After all, 3 years had passed and a lot could have changed. So I found a 2024 benchmark where they compared Fiber with Gin and Echo, both of which only slightly less efficiently than net/http, as they add features on top of it. While Fiber was still the winner in this benchmark, this time the numbers showed a smaller improvement of 5-6% in requests per second and of 14-24% less allocated memory.\nSo, being Fiber still the winner, albeit by a lower margin on the 2024 benchmark, why did I go back to net/http? It all came down to RFC compliance. Since we\u0026rsquo;re building an object store, which relies heavily on correctness to compute its authorization header based on SigV4, we couldn\u0026rsquo;t risk that a non-compliant fasthttp implementation broke our backend. Furthermore, by staying with net/http we are able to tap into the ecosystem for other interoperable libraries, if we so require. And any optimizations to net/http in future versions of Go will also reflect in improved performance for our code. On Reddit, someone claimed to handle 100k requests per second with net/http, so I see no reason to be worried about my choice.\nI ended up migrating back to net/http, using http.ServeMux as my router, and it worked fine. Bucket and object routes, however, required URL normalization by trimming the last slash. For example, to ensure both of these routes work:\nmux.Handle(\u0026#34;GET /{bucket}\u0026#34;, middleware.WithIAM(iam.ListBucket, http.HandlerFunc(bucket.ListObjectsHandler))) mux.Handle(\u0026#34;GET /{bucket}/{key...}\u0026#34;, middleware.WithIAM(iam.GetObject, http.HandlerFunc(object.GetObjectHandler))) Without the following middleware, only the second route would be matched, as {key...} can match an empty path:\nfunc NormalizeMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { if r.URL.Path != \u0026#34;/\u0026#34; { r.URL.Path = strings.TrimRight(r.URL.Path, \u0026#34;/\u0026#34;) } next.ServeHTTP(w, r) }) } We\u0026rsquo;ll likely need to update this with a more elaborate logic in the future, as query parameters come into play, but for now it works. And, of course, we can always replace http.ServeMux with a better multiplexer, and we have a wide list to pick from, since we are now working within the ecosystem.\nIf you\u0026rsquo;re interested in learning more about net/http, I suggest you take a look at the following video by Dreams of Code on YouTube: The standard library now has all you need for advanced routing in Go.. This helped me get started quite easily.\nThe LabStore Project # Introducing LabStore, the fully open source, S3-compatible object store, built for engineers who value freedom and flexibility. Our goal is to give back to the community the freedom that others took away‚Äîmultiple users, groups, and access keys, as well as the ability to set policies, manage versioning, object locking, etc.\nLong term, we also aim to provide a service that runs on lower-end hardware, so that self-hosters can enjoy running LabStore on their limited memory NAS or Raspberry Pi, or so that devs don\u0026rsquo;t waste local resources while building on top of an object store.\nLabStore is for hobbyists and for prototyping, and it\u0026rsquo;s also a learning experiment for us, its developers, but there\u0026rsquo;s room to adapt and evolve, as the community sees fit, and opportunity arises. If it makes sense do do more, we\u0026rsquo;ll do more.\nIt\u0026rsquo;s still too early to tell what LabStore will become, as it\u0026rsquo;s not even cooking in the oven yet‚Äîthe dough is still being prepared. Once it\u0026rsquo;s fully baked, we will see! üòé\nFor now, let\u0026rsquo;s take a look at the first steps we\u0026rsquo;ve taken to make this work.\nProject Management on GitHub # Since it\u0026rsquo;s not just me working on this project, I\u0026rsquo;ve setup a GitHub Org as neutral territory, where I can collab with a friend who will be working on a web frontend for LabStore. The org is called IllumiKnow Labs, where you can find the repo for LabStore. It\u0026rsquo;s perhaps too soon to open issues, but feel free to do so‚Äîjust keep in mind it won\u0026rsquo;t be a priority at this time.\nIf you\u0026rsquo;re curious, we\u0026rsquo;re using GitHub Projects and a Kanban board to manage our issues and workflow. We keep separate release branches for backend and web frontend, with PRs to main requiring peer approval.\nMonorepo Structure # We do all work in a monorepo that the whole project shares. We provide a Makefile for basic building tasks, and a justfile is being prepared to help with other tasks as well.\nHere\u0026rsquo;s an overview of the target project structure:\nlabstore/ ‚îú‚îÄ‚îÄ backend/ ‚îú‚îÄ‚îÄ web/ ‚îú‚îÄ‚îÄ cli/ ‚îú‚îÄ‚îÄ shared/ ‚îú‚îÄ‚îÄ infra/ ‚îú‚îÄ‚îÄ docs/ ‚îú‚îÄ‚îÄ bin/ ‚îú‚îÄ‚îÄ Makefile ‚îú‚îÄ‚îÄ justfile ‚îú‚îÄ‚îÄ LICENSE ‚îú‚îÄ‚îÄ CONTRIBUTING.md ‚îî‚îÄ‚îÄ README.md backend/ ‚Äì Go project with the backend web services web/ ‚Äì Svelte web frontend that will eventually integrate with the backend cli/ ‚Äì entry-point command line tool to manage the whole application shared/ ‚Äì common resources, like assets, specs, etc. (no packages here) infra/ ‚Äì CI/CD or infrastructure configurations (we\u0026rsquo;re currently using it to run an external S3 object store for web frontend testing) docs/ ‚Äì markdown documentation (mostly used to support development) bin/ ‚Äì user-facing binaries or scripts (currently just holds the backend binary, labstore-server) Makefile ‚Äì build and run backend and frontend justfile ‚Äì entry point to other project-specific justfile (e.g., just backend or just infra) LICENSE ‚Äì Apache License 2.0 (for community freedom and developer protection) CONTRIBUTING.md ‚Äì overall project contributing rules (working document) README.md ‚Äì empty so far (will contain instructions on how to deploy, run and use LabStore) Manual Testing and Benchmarking # So far, we haven\u0026rsquo;t implemented any tests or benchmarks in Go, although I\u0026rsquo;d love to do that, and it will be a requirement for opening the project to external contributions as well.\nWe\u0026rsquo;re relying on manual testing so far‚Äîit is what it is, time and resources are limited. We\u0026rsquo;re using warp, by MinIO, to test performance, which also helped us debug SigV4, and we\u0026rsquo;re using mc, rclone, and s5cmd to implement basic CLI testing commands using just (so far, we only test file copying).\nFinal Remarks # And this is it, the beginning of IllumiKnow Labs and LabStore. We hope this will be ground-zero for many learning projects and collaborations, and for some cool tools in the spirit of open source! Drop us a line on our discussion forum for LabStore, if you have something to share, or use the regular DataLabTechTV social channels, and I\u0026rsquo;ll make sure to convey any message to the team.\n","date":"11 November 2025","externalUrl":null,"permalink":"/posts/labstore-part-1-how-hard-can-it-be/","section":"","summary":"Summary # Let\u0026rsquo;s learn all about the inner workings of S3 object stores, along with enough Go to start building your own solution.","title":"LabStore - Part 1 - Building an Object Store in Go: How Hard Can It Be?","type":"posts"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/categories/data-engineering/","section":"Categories","summary":"","title":"Data Engineering","type":"categories"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/tags/data-catalog/","section":"Tags","summary":"","title":"Data-Catalog","type":"tags"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/tags/data-migration/","section":"Tags","summary":"","title":"Data-Migration","type":"tags"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/tags/data-stack/","section":"Tags","summary":"","title":"Data-Stack","type":"tags"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/categories/devops/","section":"Categories","summary":"","title":"DevOps","type":"categories"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/tags/duckdb/","section":"Tags","summary":"","title":"Duckdb","type":"tags"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/tags/ducklake/","section":"Tags","summary":"","title":"Ducklake","type":"tags"},{"content":" Summary # Learn how to migrate your existing DuckLake catalog from SQLite to PostgreSQL, without losing any data, and see it in action in a production environment.\n\u003e Migrating with pgloader # Since pgloader is able to read from SQLite, we used this as our migration tool. The whole process was also implemented as a just command called migrate-lakehouse-catalog-all. It consists of loading the required environment variables from the .env, file creating the required PostgreSQL schemas, running the pgloader migration, and fixing incorrect data types.\nBasic Migration Script # The following code fragment, extracted from the just command, shows the migration process for a single catalog (stage, graphs, etc.).\necho \u0026#34;Migrating {{catalog}} catalog from SQLite to PostgreSQL...\u0026#34; psql_conn_str=\u0026#34;postgresql://$PGUSER:$PGPASSWORD@$PGHOST:$PGPORT/$PGDATABASE\u0026#34; psql -c \u0026#34;CREATE SCHEMA IF NOT EXISTS $psql_schema\u0026#34; pgloader --set search_path=\u0026#34;\u0026#39;$psql_schema\u0026#39;\u0026#34; $sqlite_db_path $psql_conn_str PGOPTIONS=\u0026#34;--search-path=$psql_schema\u0026#34; psql -f \u0026#34;{{migrate_lakehouse_fix_script}}\u0026#34; We could have used a more elabore *.load command file for pgloader, but since there weren\u0026rsquo;t too many settings, we just used the CLI args with the --set option to configure the Postgres search_path to point to our schema, which we created using psql, and we ran a SQL script to assign the correct data types to columns that match the expected data type for a DuckLake Postgres catalog.\nData Type Fixing # The following table illustrates how the target data types for the catalog tables in DuckLake get implemented in practice using SQLite and PostgreSQL natively, and how pgloader converts the types from SQLite to PostgreSQL during migration. In fact for SQLite, we\u0026rsquo;re looking at type names that map to type affinities and then to a storage class‚ÄîBIGINT is stored as INTEGER, while VARCHAR is stored as TEXT. Let\u0026rsquo;s focus on the two rightmost columns, which should exactly match each other after migration, but don\u0026rsquo;t.\nData Type SQLite PostgreSQL SQLite ‚ñ∂ PostgreSQL BIGINT BIGINT BIGINT BIGINT BOOLEAN BIGINT BOOLEAN BIGINT TIMESTAMPTZ VARCHAR TIMESTAMP WITH TIMEZONE TEXT UUID VARCHAR UUID TEXT VARCHAR VARCHAR CHARACTER VARYING TEXT Perhaps if DuckLake took advantage of type names that matched the desired affinity or class, then pgloader could would have done a better job at migration. For example, using BOOLEAN in SQLite would have mapped to the NUMERIC affinity, which in turn would end up being stored as an INTEGER. For datetime types, unfortunately, there is no equivalent solution, as these can be stored in TEXT, REAL, or INTEGER classes, so only by looking at the value could we determine the type. Also, there is no UUID type in SQLite, which similarly means that the type could only be determined by looking at values.\nSince we cannot define a direct cast using a *.load command file, because there is no direct mapping, the best way to assign the correct typing to the migrated tables is to run a custom SQL script that alters the column types for columns that should have been BOOLEAN, TIMESTAMPTZ or UUID. We\u0026rsquo;ll leave VARCHAR untouched, as it is essentially the same as TEXT, and BIGINT was already correctly migrated.\nAffected Columns # After going through the Full Schema Creation Script for DuckLake, we compiled a summary of the columns that need altering, covering 10 out of 22 catalog tables.\nTable Column Type ducklake_snapshot snapshot_time TIMESTAMPTZ ducklake_schema schema_uuid UUID path_is_relative BOOLEAN ducklake_table table_uuid UUID path_is_relative BOOLEAN ducklake_view view_uuid UUID ducklake_data_file path_is_relative BOOLEAN ducklake_file_column_stats path_is_relative BOOLEAN ducklake_column nulls_allowed BOOLEAN ducklake_table_column_stats contains_null BOOLEAN contains_nan BOOLEAN ducklake_files_scheduled_for_deletion path_is_relative BOOLEAN schedule_start BOOLEAN ducklake_name_mapping is_partition BOOLEAN Altering Column Types # The SQL script we used to assign the correct data types was the following:\nALTER TABLE ducklake_snapshot ALTER COLUMN snapshot_time TYPE TIMESTAMPTZ USING snapshot_time::TIMESTAMPTZ; ALTER TABLE ducklake_schema ALTER COLUMN schema_uuid TYPE UUID USING schema_uuid::UUID, ALTER COLUMN path_is_relative TYPE BOOLEAN USING path_is_relative::INTEGER = 1; ALTER TABLE ducklake_table ALTER COLUMN table_uuid TYPE UUID USING table_uuid::UUID, ALTER COLUMN path_is_relative TYPE BOOLEAN USING path_is_relative::INTEGER = 1; ALTER TABLE ducklake_view ALTER COLUMN view_uuid TYPE UUID USING view_uuid::UUID; ALTER TABLE ducklake_data_file ALTER COLUMN path_is_relative TYPE BOOLEAN USING path_is_relative::INTEGER = 1; ALTER TABLE ducklake_file_column_stats ALTER COLUMN contains_nan TYPE BOOLEAN USING contains_nan::INTEGER = 1; ALTER TABLE ducklake_delete_file ALTER COLUMN path_is_relative TYPE BOOLEAN USING path_is_relative::INTEGER = 1; ALTER TABLE ducklake_column ALTER COLUMN nulls_allowed TYPE BOOLEAN USING nulls_allowed::INTEGER = 1; ALTER TABLE ducklake_table_column_stats ALTER COLUMN contains_null TYPE BOOLEAN USING contains_null::INTEGER = 1, ALTER COLUMN contains_nan TYPE BOOLEAN USING contains_nan::INTEGER = 1; ALTER TABLE ducklake_files_scheduled_for_deletion ALTER COLUMN path_is_relative TYPE BOOLEAN USING path_is_relative::INTEGER = 1, ALTER COLUMN schedule_start TYPE TIMESTAMPTZ USING schedule_start::TIMESTAMPTZ; ALTER TABLE ducklake_name_mapping ALTER COLUMN is_partition TYPE BOOLEAN USING is_partition::INTEGER = 1; In essence, we\u0026rsquo;re running the following conversions:\nTEXT to TIMESTAMPTZ ‚Äì a simple cast was enough BIGINT to BOOLEAN ‚Äì we cast to INTEGER (optional) and compare to 1 TEXT to UUID ‚Äì a simple cast was also enough Changes to DataLab Code # Environment Variables # Our configuration changed from a local SQLite file per catalog to a schema in a remote PostgreSQL database per catalog. As such, we had to revise our configurable environment variables.\nWe added the new catalog configuration variables:\nPSQL_CATALOG_HOST=docker-shared PSQL_CATALOG_PORT=5432 PSQL_CATALOG_DB=lakehouse PSQL_CATALOG_USER=lakehouse PSQL_CATALOG_PASSWORD=lakehouse PSQL_CATALOG_STAGE_SCHEMA=stage PSQL_CATALOG_SECURE_STAGE_SCHEMA=secure_stage PSQL_CATALOG_GRAPH_MART_SCHEMA=graphs_mart PSQL_CATALOG_ANALYTICS_MART_SCHEMA=analytics_mart We kept the ENGINE_DB, since a local DuckDB will still exist for compute:\nENGINE_DB=engine.duckdb But we deprecated the SQLite catalog variables:\nSTAGE_DB=stage.sqlite SECURE_STAGE_DB=secure_stage.sqlite GRAPHS_MART_DB=marts/graphs.sqlite ANALYTICS_MART_DB=marts/analytics.sqlite Just Command for Migration # The deprecated variables above are still required during the migration process, but you can delete them after running:\njust migrate-lakehouse-catalog-all The previous command will call just migrate-lakehouse-catalog \u0026lt;catalog\u0026gt; for each catalog. The output per catalog will look something like this:\njust migrate-lakehouse-catalog stage just check pgloader Checking pgloader... ok just check psql Checking psql... ok Testing lakehouse catalog connection... ok Migrating stage catalog from SQLite to PostgreSQL... CREATE SCHEMA 2025-10-24T09:19:58.011998Z LOG pgloader version \u0026#34;3.6.10~devel\u0026#34; 2025-10-24T09:19:58.099988Z LOG Migrating from #\u0026lt;SQLITE-CONNECTION sqlite:///datalab/local/stage.sqlite {100528A183}\u0026gt; 2025-10-24T09:19:58.099988Z LOG Migrating into #\u0026lt;PGSQL-CONNECTION pgsql://lakehouse@docker-shared:5432/lakehouse {10053A6573}\u0026gt; 2025-10-24T09:19:58.715916Z LOG report summary reset table name errors rows bytes total time ------------------------------------- --------- --------- --------- -------------- fetch 0 0 0.000s fetch meta data 0 27 0.048s Create Schemas 0 0 0.000s Create SQL Types 0 0 0.008s Create tables 0 44 0.132s Set Table OIDs 0 22 0.008s ------------------------------------- --------- --------- --------- -------------- ducklake_metadata 0 4 0.1 kB 0.044s ducklake_snapshot 0 18 0.7 kB 0.044s ducklake_snapshot_changes 0 18 1.0 kB 0.044s ducklake_schema 0 5 0.3 kB 0.044s ducklake_table 0 13 1.0 kB 0.072s ducklake_view 0 0 0.068s ducklake_tag 0 0 0.076s ducklake_column_tag 0 0 0.076s ducklake_file_column_stats 0 71 3.4 kB 0.112s ducklake_data_file 0 13 1.4 kB 0.112s ducklake_delete_file 0 0 0.112s ducklake_column 0 75 3.0 kB 0.116s ducklake_table_stats 0 13 0.3 kB 0.140s ducklake_partition_info 0 0 0.140s ducklake_table_column_stats 0 71 2.4 kB 0.140s ducklake_file_partition_value 0 0 0.188s ducklake_inlined_data_tables 0 0 0.192s ducklake_partition_column 0 0 0.144s ducklake_name_mapping 0 0 0.224s ducklake_files_scheduled_for_deletion 0 0 0.192s ducklake_column_mapping 0 0 0.192s ducklake_schema_versions 0 18 0.1 kB 0.224s ------------------------------------- --------- --------- --------- -------------- COPY Threads Completion 0 4 0.232s Create Indexes 0 5 0.016s Index Build Completion 0 5 0.012s Reset Sequences 0 0 0.048s Primary Keys 0 5 0.012s Create Foreign Keys 0 0 0.000s Create Triggers 0 0 0.000s Install Comments 0 0 0.000s ------------------------------------- --------- --------- --------- -------------- Total import time ‚úì 319 13.7 kB 0.320s ALTER TABLE ALTER TABLE ALTER TABLE ALTER TABLE ALTER TABLE ALTER TABLE ALTER TABLE ALTER TABLE ALTER TABLE ALTER TABLE ALTER TABLE Refactoring for a PSQL Catalog # The files affected by the migration were the following:\n.env.example | 12 ++++++++++++ dlctl/cli.py | 4 ++-- dlctl/dbt_handler.py | 8 -------- justfile | 5 +---- shared/settings.py | 12 ++++-------- shared/templates.py | 33 ++++++++++++++++++++++++++++----- shared/tools.py | 25 ++++++++++++++++++------- transform/profiles.yml | 30 +++++++++++++++++++++++------- The majority of the changes were related to using the new environment variables, but the most relevant ones were regarding the DuckLake configuration, affecting both shared/templates.py and transform/profiles.yml.\nTemplate for init.sql # For shared/templates.py, we replaced the SQLite extension install with:\nINSTALL postgres; We added a PostgreSQL secret:\nCREATE OR REPLACE SECRET postgres ( TYPE postgres, HOST \u0026#39;$psql_host\u0026#39;, PORT $psql_port, DATABASE $psql_db, USER \u0026#39;$psql_user\u0026#39;, PASSWORD \u0026#39;$psql_password\u0026#39; ); And a DuckLake secret, to simplify attachment:\nCREATE OR REPLACE SECRET ( TYPE ducklake, METADATA_PATH \u0026#39;\u0026#39;, METADATA_PARAMETERS MAP { \u0026#39;TYPE\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;SECRET\u0026#39;: \u0026#39;postgres\u0026#39; } ); We replaced the attachment statements with a PostgreSQL catalog config based on the previous default secret (unnamed), using METADATA_SCHEMA to point to each separate catalog schema, within the lakehouse database:\nATTACH \u0026#39;ducklake:\u0026#39; AS $psql_schema ( METADATA_SCHEMA \u0026#39;$psql_schema\u0026#39;, DATA_PATH \u0026#39;s3://$s3_bucket/$s3_prefix\u0026#39; ); Updating dbt Profiles # We then reproduced the same configuration as previously in transform/profiles.yml:\ntransform: outputs: lakehouse: type: duckdb path: \u0026#34;{{ env_var(\u0026#39;LOCAL_DIR\u0026#39;) }}/{{ env_var(\u0026#39;ENGINE_DB\u0026#39;) }}\u0026#34; extensions: - httpfs - parquet - ducklake - postgres secrets: - type: s3 name: minio region: \u0026#34;{{ env_var(\u0026#39;S3_REGION\u0026#39;) }}\u0026#34; key_id: \u0026#34;{{ env_var(\u0026#39;S3_ACCESS_KEY_ID\u0026#39;) }}\u0026#34; secret: \u0026#34;{{ env_var(\u0026#39;S3_SECRET_ACCESS_KEY\u0026#39;) }}\u0026#34; endpoint: \u0026#34;{{ env_var(\u0026#39;S3_ENDPOINT\u0026#39;) }}\u0026#34; use_ssl: \u0026#34;{{ env_var(\u0026#39;S3_USE_SSL\u0026#39;) }}\u0026#34; url_style: \u0026#34;{{ env_var(\u0026#39;S3_URL_STYLE\u0026#39;) }}\u0026#34; - type: postgres name: postgres host: \u0026#34;{{ env_var(\u0026#39;PSQL_CATALOG_HOST\u0026#39;) }}\u0026#34; port: \u0026#34;{{ env_var(\u0026#39;PSQL_CATALOG_PORT\u0026#39;) }}\u0026#34; database: \u0026#34;{{ env_var(\u0026#39;PSQL_CATALOG_DB\u0026#39;) }}\u0026#34; user: \u0026#34;{{ env_var(\u0026#39;PSQL_CATALOG_USER\u0026#39;) }}\u0026#34; password: \u0026#34;{{ env_var(\u0026#39;PSQL_CATALOG_PASSWORD\u0026#39;) }}\u0026#34; - type: ducklake name: \u0026#34;\u0026#34; metadata_path: \u0026#34;\u0026#34; metadata_parameters: type: postgres secret: postgres attach: - path: \u0026#34;ducklake:\u0026#34; alias: stage options: metadata_schema: stage data_path: s3://{{ env_var(\u0026#39;S3_BUCKET\u0026#39;) }}/{{ env_var(\u0026#39;S3_STAGE_PREFIX\u0026#39;) }} - path: \u0026#34;ducklake:\u0026#34; alias: secure_stage options: metadata_schema: secure_stage data_path: s3://{{ env_var(\u0026#39;S3_BUCKET\u0026#39;) }}/{{ env_var(\u0026#39;S3_SECURE_STAGE_PREFIX\u0026#39;) }} encrypted: 1 - path: \u0026#34;ducklake:\u0026#34; alias: graphs options: metadata_schema: graphs data_path: \u0026gt; s3://{{ env_var(\u0026#39;S3_BUCKET\u0026#39;) }}/{{ env_var(\u0026#39;S3_GRAPHS_MART_PREFIX\u0026#39;) }} - path: \u0026#34;ducklake:\u0026#34; alias: analytics options: metadata_schema: analytics data_path: \u0026gt; s3://{{ env_var(\u0026#39;S3_BUCKET\u0026#39;) }}/{{ env_var(\u0026#39;S3_ANALYTICS_MART_PREFIX\u0026#39;) }} target: lakehouse Notice that we added the name attribute to each secrets entry, which we didn\u0026rsquo;t use previously. This is mostly due to metadata_parameters pointing to a named postgres secret. In reality, only the Postgres secret required the name attribute, but we added this to all secrets for consistency.\nUpdating Backup Tool # Finally, we also updated our backup tool to run pg_dump instead of simply copying local SQLite files to a backup bucket in our object store.\nOur file structure changed from:\ns3://lakehouse/ ‚îî‚îÄ‚îÄ backups/ ‚îî‚îÄ‚îÄ catalog/ ‚îú‚îÄ‚îÄ YYYY_MM_DD/ ‚îÇ ‚îî‚îÄ‚îÄ HH_mm_SS_sss/ ‚îÇ ‚îú‚îÄ‚îÄ engine.duckdb ‚îÇ ‚îú‚îÄ‚îÄ stage.sqlite ‚îÇ ‚îî‚îÄ‚îÄ marts/*.sqlite ‚îî‚îÄ‚îÄ manifest.json To:\ns3://lakehouse/ ‚îî‚îÄ‚îÄ backups/ ‚îî‚îÄ‚îÄ catalog/ ‚îú‚îÄ‚îÄ YYYY_MM_DD/ ‚îÇ ‚îî‚îÄ‚îÄ HH_mm_SS_sss/ ‚îÇ ‚îî‚îÄ‚îÄ lakehouse.dump ‚îî‚îÄ‚îÄ manifest.json So now, running the following commands will backup and restore lakehouse.dump files:\ndlctl backup create dlctl backup restore In a production scenario, we might also want to consider a backup and/or migration workflow for our lakehouse bucket, as the catalog is pointless without the DuckLake parquet files. For now, however, the data lab infra we\u0026rsquo;re running only provides a single object store endpoint, and our buckets all live in the same instance, which makes it pointless to backup on top of this.\nFor now, if you\u0026rsquo;re migrating to new infrastructure, simply download your lakehouse bucket to an intermediate location and restore it back to the new bucket. You\u0026rsquo;ll then be able to restore the PostgreSQL catalog backup from the new bucket.\nUpdating ML Server Deployment # Now that our data stack is completely running on the [[Architecture Design|data lab infra]], we can update the apps docker compose project for our mlserver service, so that it connects to the PostgreSQL catalog and the MinIO storage.\nNext you can see the environment variables that we added to infra/apps/docker/compose.yml, besides the already existing MLFLOW_TRACKING_URI and KAFKA_BROKER_ENDPOINT:\nservices: mlserver: build: context: ../../../ dockerfile: infra/apps/docker/mlserver/Dockerfile ports: - \u0026#34;8000:8000\u0026#34; environment: MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI} KAFKA_BROKER_ENDPOINT: ${KAFKA_BROKER_ENDPOINT} S3_ENDPOINT: ${S3_ENDPOINT} S3_USE_SSL: ${S3_USE_SSL} S3_URL_STYLE: ${S3_URL_STYLE} S3_ACCESS_KEY_ID: ${S3_ACCESS_KEY_ID} S3_SECRET_ACCESS_KEY: ${S3_SECRET_ACCESS_KEY} S3_REGION: ${S3_REGION} S3_BUCKET: ${S3_BUCKET} S3_STAGE_PREFIX: ${S3_STAGE_PREFIX} S3_SECURE_STAGE_PREFIX: ${S3_SECURE_STAGE_PREFIX} S3_GRAPHS_MART_PREFIX: ${S3_GRAPHS_MART_PREFIX} S3_ANALYTICS_MART_PREFIX: ${S3_ANALYTICS_MART_PREFIX} S3_EXPORTS_PREFIX: ${S3_EXPORTS_PREFIX} S3_BACKUPS_PREFIX: ${S3_BACKUPS_PREFIX} PSQL_CATALOG_HOST: ${PSQL_CATALOG_HOST} PSQL_CATALOG_PORT: ${PSQL_CATALOG_PORT} PSQL_CATALOG_DB: ${PSQL_CATALOG_DB} PSQL_CATALOG_USER: ${PSQL_CATALOG_USER} PSQL_CATALOG_PASSWORD: ${PSQL_CATALOG_PASSWORD} PSQL_CATALOG_STAGE_SCHEMA: ${PSQL_CATALOG_STAGE_SCHEMA} PSQL_CATALOG_SECURE_STAGE_SCHEMA: ${PSQL_CATALOG_SECURE_STAGE_SCHEMA} PSQL_CATALOG_GRAPHS_MART_SCHEMA: ${PSQL_CATALOG_GRAPHS_MART_SCHEMA} PSQL_CATALOG_ANALYTICS_MART_SCHEMA: ${PSQL_CATALOG_ANALYTICS_MART_SCHEMA} healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8000/health\u0026#34;] interval: 10s retries: 5 restart: unless-stopped We also had to re-run terraform apply for infra/services/gitlab, so that the CI/CD variables were updated with the new .env configs.\nOnce this was done, we simply committed and pushed to GitLab with git push infra, as described in [[Layer 3 - Services]], which redeployed the docker compose project.\n","date":"28 October 2025","externalUrl":null,"permalink":"/posts/migrating-ducklake-catalog/","section":"","summary":"Summary # Learn how to migrate your existing DuckLake catalog from SQLite to PostgreSQL, without losing any data, and see it in action in a production environment.","title":"Migrating DuckLake Catalog From SQLite to PostgreSQL","type":"posts"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/tags/postgres/","section":"Tags","summary":"","title":"Postgres","type":"tags"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/tags/sqlite/","section":"Tags","summary":"","title":"Sqlite","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/applications/","section":"Tags","summary":"","title":"Applications","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/architecture/","section":"Tags","summary":"","title":"Architecture","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/ci-cd/","section":"Tags","summary":"","title":"Ci-Cd","type":"tags"},{"content":" Summary # On part 5 of this series, you\u0026rsquo;ll learn how to use multi-project CI/CD with the GitLab Free tier, in order to provision resources like PostgreSQL databases and credentials. You\u0026rsquo;ll also learn how easy it is to deploy your ML model with Docker, once you have the correct infrastructure setup. And, finally, we\u0026rsquo;ll look back at the pros and cons of the implemented architecture, doing a full retrospective, and proposing a redesigned architecture to fix the pitfalls of our current design.\n\u003e Changes to CI/CD # Attempted Refactoring # At first, we attempted to refactor the CI/CD pipeline as follows:\n.ci/ ‚îú‚îÄ‚îÄ scripts/ ‚îÇ¬†‚îú‚îÄ‚îÄ kafka/ ‚îÇ¬†‚îú‚îÄ‚îÄ ollama/ ‚îÇ¬†‚îî‚îÄ‚îÄ postgres/ ‚îî‚îÄ‚îÄ templates/ ‚îú‚îÄ‚îÄ deploy.yml ‚îú‚îÄ‚îÄ kafka.yml ‚îú‚îÄ‚îÄ ollama.yml ‚îî‚îÄ‚îÄ postgres.yml However, if we run a script instead of defining it inline, we won\u0026rsquo;t be able to run these jobs from another repository, which is a concern for our current workflow, as we want to be able to provision databases and credentials, or other resources, by calling jobs within the datalab repository as effortlessly as possible. We ended up going back to the original approach, with templates on the root of .ci/.\nWe also fixed an error in the changes globbing, where infra/services/docker/** should have been infra/services/docker/**/* so that files were matched rather than directories.\nCustom Ubuntu Image # Since we were continuously in need of a few common command line tools, like curl or jq, we decided to build a custom Ubuntu image that we pushed to the container registry for the datalab project. This let us reuse this image directly from our container registry, without the need to add a before_script block to setup the Ubuntu instance each time a new runner was launched for jobs requiring additional commands beyond the base image.\nBuilding and pushing for this image was done through the Terraform project under infra/services/gitlab, so the workflow remains unchanged, assuming you setup your CI/CD variables using your .env via Terraform.\nThe first attempt to push a large image into our container registry failed. This ended up being fixed by adding checksum_disabled to /etc/gitlab/gitlab.rb as follows:\nregistry[\u0026#39;storage\u0026#39;] = { \u0026#39;s3_v2\u0026#39; =\u0026gt; { ... \u0026#39;checksum_disabled\u0026#39; =\u0026gt; true, } } And then SSHing into the GitLab VM and running:\nsudo gitlab-ctl reconfigure This configuration was also added to the L2 (Platform) Terraform project, to the cloud-config for GitLab, so if you\u0026rsquo;re deploying this now, you don\u0026rsquo;t have to worry about this.\nWe also had to change the project visibility to \u0026ldquo;Public\u0026rdquo; for datalab, otherwise CI/CD job were unable to pull from the container registry. Of course our GitLab instance is not exposed to the outside, otherwise we would need a different strategy to handle access.\nImproving Postgres Workflow # We also improved the postgres template in order to store credentials as JSON, using a CI/CD variable, so that we could return them, if run multiple times, as opposed to just producing a new password and not providing access to the original credentials. This is a requirement that we missed during the initial design of this workflow, as we\u0026rsquo;ll need these credentials on our external application projects during deployment. This also makes the workflow idempotent.\nHere\u0026rsquo;s a summary of changes:\nCredentials no longer printed in logs. Credentials stored as JSON, using a CI/CD variable. External projects will always need to call this workflow to load credentials into the env during application container deployment. Database and credentials will be created as required, if they don\u0026rsquo;t exist. Notice that any project can access any database and credentials, as this wasn\u0026rsquo;t a concern here, but the JSON format and workflow can be expanded to handle this if required. A better way, as we\u0026rsquo;ll see next, is to just stop resisting using an extra service to handle secrets.\nMulti-Project CI/CD on GitLab Free Tier # Templates for Provisioning # We provide, on the original datalab project, CI/CD templates that can be included as required, on external projects, as well as internally, to provision resources, be it within PostgreSQL, Kafka, or Ollama.\nSince we are using the GitLab Free tier, we lack the syntactic sugar to trigger jobs on an external project, particularly when we need artifacts from that project. So, the best way to handle this is via the REST API.\nWe provide the following CI/CD templates:\n.ci/provision/ ‚îú‚îÄ‚îÄ kafka.yml ‚îú‚îÄ‚îÄ ollama.yml ‚îî‚îÄ‚îÄ postgres.yml These are set to call the CI/CD pipeline with the appropriate input values to provision resources. The strategy is similar for all three templates. Here\u0026rsquo;s how we do it for .ci/provision/ollama.yml:\nspec: inputs: pull: description: \u0026#34;Pull Ollama model\u0026#34; type: string --- stages: - provision variables: PROVISIONER_ID: datalabtechtv%2Fdatalab provision_ollama_model: stage: provision image: gitlab:5050/datalabtechtv/datalab/ubuntu:custom variables: PULL: $[[ inputs.pull ]] script: - echo \u0026#34;Triggering downstream pipeline\u0026#34; - | PIPELINE_ID=$(curl -s -X POST \\ --form token=$GITLAB_TRIGGER_TOKEN \\ --form ref=infra \\ --form inputs[ollama_pull]=$PULL \\ $CI_API_V4_URL/projects/$PROVISIONER_ID/trigger/pipeline | jq -r \u0026#39;.id\u0026#39;) - echo \u0026#34;Triggered pipeline $PIPELINE_ID\u0026#34; - | while true; do STATUS=$(curl -s -H \u0026#34;PRIVATE-TOKEN: $GITLAB_TOKEN\u0026#34; \\ $CI_API_V4_URL/projects/$PROVISIONER_ID/pipelines/$PIPELINE_ID \\ | jq -r \u0026#39;.status\u0026#39;) echo \u0026#34;Pipeline status: $STATUS\u0026#34; [[ \u0026#34;$STATUS\u0026#34; == \u0026#34;success\u0026#34; || \u0026#34;$STATUS\u0026#34; == \u0026#34;failed\u0026#34; ]] \u0026amp;\u0026amp; break sleep 10 done The spec.inputs will be set to whatever information you need from an external project to provision the required resource:\nspec: inputs: pull: description: \u0026#34;Pull Ollama model\u0026#34; type: string --- Then, we set stages to provision, which is the only one required for these template:\nstages: - provision External projects that include this template must make sure that this stage is defined for them as well.\nWe set a global variable, PROVISIONER_ID, with the project ID for datalab in its string format:\nvariables: PROVISIONER_ID: datalabtechtv%2Fdatalab And, finally, we define the provisioning job, which always triggers the datalab pipeline regardless of where it\u0026rsquo;s called from. Let\u0026rsquo;s take a look at the script for the provision_ollama_model job.\nFirst, we trigger the pipeline, obtaining it\u0026rsquo;s run ID:\necho \u0026#34;Triggering downstream pipeline\u0026#34; PIPELINE_ID=$(curl -s -X POST \\ --form token=$GITLAB_TRIGGER_TOKEN \\ --form ref=infra \\ --form inputs[ollama_pull]=$PULL \\ $CI_API_V4_URL/projects/$PROVISIONER_ID/trigger/pipeline | jq -r \u0026#39;.id\u0026#39;) echo \u0026#34;Triggered pipeline $PIPELINE_ID\u0026#34; Notice that we need the GITLAB_TRIGGER_TOKEN we previously set. As you can see, we send the user configs via inputs[...] form data fields.\nWe then poll the API every 10 seconds to check if the pipeline has finished:\nwhile true; do STATUS=$(curl -s -H \u0026#34;PRIVATE-TOKEN: $GITLAB_TOKEN\u0026#34; \\ $CI_API_V4_URL/projects/$PROVISIONER_ID/pipelines/$PIPELINE_ID \\ | jq -r \u0026#39;.status\u0026#39;) echo \u0026#34;Pipeline status: $STATUS\u0026#34; [[ \u0026#34;$STATUS\u0026#34; == \u0026#34;success\u0026#34; || \u0026#34;$STATUS\u0026#34; == \u0026#34;failed\u0026#34; ]] \u0026amp;\u0026amp; break sleep 10 done Once this is run, control will be ceded to the external project CI/CD pipeline.\nExternal Test Project # We created an external test project called datalab-infra-test to demo how this works.\nFirst, we needed a trigger token from the datalab project, which was created by navigating to Settings ‚Üí CI/CD ‚Üí Pipeline trigger tokens ‚Üí Add new token, under datalab. We then stored the token in the GITLAB_TRIGGER_TOKEN CI/CD variable under datalab-infra-test.\nAdditionally, we had to reconfigure /etc/gitlab-runner/config.toml to increase job concurrency, otherwise we wouldn\u0026rsquo;t be able to trigger a job, and wait for it from another job‚Äîwith the default concurrency of 1, the pipeline would just freeze completely.\nWe SSHed into the GitLab VM, set concurrency = 4, and restarted the runner with:\nsudo sed -i \u0026#39;s/^concurrent = 1/concurrent = 4/\u0026#39; /etc/gitlab-runner/config.toml sudo gitlab-runner restart This configuration was also added to the L2 (Platform) Terraform project, within the cloud-config for GitLab, so if you\u0026rsquo;re deploying this now you won\u0026rsquo;t have to worry about it.\nFor example, if you need a Postgres database and credentials, you can configure your CI/CD jobs as follows:\nstages: - provision - test variables: POSTGRES_DB_USER: ci_cd_user POSTGRES_DB_NAME: ci_cd_db include: - project: datalabtechtv/datalab ref: infra file: \u0026#39;.ci/provision/postgres.yml\u0026#39; inputs: db_user: $POSTGRES_DB_USER db_name: $POSTGRES_DB_NAME test_db_connection: stage: test image: postgres:18.0-alpine needs: - fetch_db_credentials script: - \u0026#39;echo Connecting to database: $DB_NAME\u0026#39; - \u0026#39;echo Connecting with user: $DB_USER\u0026#39; - PGPASSWORD=$DB_PASS psql -h docker-shared -U $DB_USER -d $DB_NAME -c \u0026#39;\\q\u0026#39; Here, test_db_connection would usually be replaced by something like a docker compose up for your own application. The point here is that this workflow will ensure that the database you need is created, and it will handle the secrets for you, making them available as env vars.\nFor Kafka and Ollama, we only run a provisioning job, since we don\u0026rsquo;t need any credentials back from the job, but for Postgres, the pipeline will also fetch the job ID for psql_create_db, which contains the credentials.env artifact (this expires after 15m), loading those credentials as environment variables. The pipeline for the test project looks like this:\nAnd now you know of a CI/CD strategy, running on top of the GitLab Free tier, for provisioning resources in your data lab infrastructure! It might not be the best, but it works. Of course, we\u0026rsquo;ll keep improving on it, and we\u0026rsquo;ll share everything with you, as we go!\nModel Deployment # Starting Point # Last time, on our ML End-to-End Workflow we had produced a REST endpoint, using FastAPI, that provided a way to run inference over one or multiple models (A/B/n testing) that had been previously logged to MLflow. Optionally, we could log the inference to DuckLake, which was running on top of a local SQLite catalog and a remote MinIO storage. Logged inferences were streamed to a Kafka topic, and then consumed and buffered, up to a point when they were inserted into the appropriate DuckLake table.\nWhat we want to do now is prepare this REST API to be deployed on the Docker instance running on the docker-apps VM, while using available services running on docker-shared. This includes MLflow and Kafka, but also PostgreSQL and MinIO (L1) for DuckLake. Today, we\u0026rsquo;ll only be concerned with ensuring MLflow and Kafka are integrated, as we\u0026rsquo;ll have a blog post (and video) focusing on migrating your catalog from SQLite to PostgreSQL, at which time we\u0026rsquo;ll configure DuckLake adequately to run on top of docker-shared services.\nAsking CI/CD for Kafka Topics # Since our goal is essentially to expose ml.server, which is a part of the datalab project, we\u0026rsquo;ll setup the CI/CD within this project. This time, we use two trigger jobs, for each of our Kafka topics, one for the logging the inference results (provision_mlserver_results_topic), and the other one to handle inference feedback sent by our users (provision_mlserver_feedback_topic).\nBoth jobs will be similar, so let\u0026rsquo;s take a look at provision_mlserver_results_topic:\nprovision_mlserver_results_topic: stage: deploy trigger: include: - local: .ci/provision/kafka.yml inputs: topic: ml_inference_results group: lakehouse-inference-result-consumer strategy: depend rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;push\u0026#34; changes: - .ci/deploy.yml - infra/apps/docker/**/* - if: \u0026#39;\u0026#34;$[[ inputs.force_apps_deploy ]]\u0026#34; == \u0026#34;true\u0026#34;\u0026#39; Similarly to what we did for the datalab-infra-test project, we include the provision template, but this time it\u0026rsquo;s a local include. We ask that it creates topic ml_inference_results, initializing a consumer for it with group lakehouse-inference-result-consumer.\nThe job triggering rules match the ones that we use for our apps_deploy job.\nDeploying Applications # The apps_deploy job is defined under .ci/deploy.yml, for the datalab project, as follows:\napps_deploy: stage: deploy image: docker:28.4.0-cli needs: - provision_mlserver_results_topic - provision_mlserver_feedback_topic variables: DOCKER_HOST: tcp://docker-apps:2375 DOCKER_BUILDKIT: 1 INFERENCE_RESULTS_TOPIC: ml_inference_results INFERENCE_FEEDBACK_TOPIC: lakehouse-inference-result-consumer INFERENCE_RESULTS_GROUP: ml_inference_feedback INFERENCE_FEEDBACK_GROUP: lakehouse-inference-feedback-consumer script: - docker compose -p datalab -f infra/apps/docker/compose.yml up -d --build - docker ps rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;push\u0026#34; changes: - .ci/deploy.yml - infra/apps/docker/**/* - if: \u0026#39;\u0026#34;$[[ inputs.force_apps_deploy ]]\u0026#34; == \u0026#34;true\u0026#34;\u0026#39; Regarding the rules, notice that we provide a new boolean input that we can set during pipeline triggering to force redeploy the docker compose for our applications. This is useful when we just want to update the env vars for it.\nAs you can see, we set the two topic provisioning jobs as a dependency, and we then configure the environment variables required by our ml.server REST API. Also notice that we set DOCKER_BUILDKIT, which will reduce the overhead of redeploying with the --build flag, as image layers will be cached between deployments.\nDocker Compose # Let\u0026rsquo;s take a look at infra/apps/docker/compose.yml:\nservices: mlserver: build: context: ../../../ dockerfile: infra/apps/docker/mlserver/Dockerfile ports: - \u0026#34;8000:8000\u0026#34; environment: MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI} KAFKA_BROKER_ENDPOINT: ${KAFKA_BROKER_ENDPOINT} healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8000/health\u0026#34;] interval: 10s retries: 5 restart: unless-stopped Notice that we set our build context to the root of the datalab project, so that it will pickup our .env when building locally. Otherwise, environment variables will either be set via the apps_deploy CI/CD job, or as a CI/CD variable, as is the case for MLFLOW_TRACKING_URI and KAFKA_BROKER_ENDPOINT, which are set to:\nMLFLOW_TRACKING_URI=\u0026#34;http://docker-shared:5000\u0026#34; KAFKA_BROKER_ENDPOINT=\u0026#34;docker-shared:9092\u0026#34; Dockerfile # The Dockerfile for ml.server is quite minimal, and based on the official uv image bundled with Python 3.13 and running on Debian Trixie:\nFROM astral/uv:python3.13-trixie-slim RUN apt update \u0026amp;\u0026amp; apt install -y git curl WORKDIR /datalab COPY pyproject.toml pyproject.toml COPY uv.lock uv.lock RUN uv sync --frozen COPY . . RUN uv sync --frozen ENTRYPOINT [\u0026#34;./infra/apps/docker/mlserver/docker-entrypoint.sh\u0026#34;] Order matters, if you want to optimize caching.\nFirst we install system dependencies‚Äîour pyproject.toml is using a dependency straight from a git repo, so we\u0026rsquo;ll need the git command:\nRUN apt update \u0026amp;\u0026amp; apt install -y git curl Then, we switch to /datalab and copy only the require files to install uv dependencies:\nWORKDIR /datalab COPY pyproject.toml pyproject.toml COPY uv.lock uv.lock RUN uv sync --frozen Installing dependencies before copying the source code for datalab will ensure that, unless dependencies change, we\u0026rsquo;ll be able to change the code in datalab and redeploy without having to reinstall all dependencies again, which takes quite a while.\nThen, we finally copy our complete datalab repo and install our source code as the last missing dependency:\nCOPY . . RUN uv sync --frozen docker-entrypoint.sh # We set the entry point for our container as a shell script that loads the Python virtual environment and then calls the CLI command to start the REST API server:\n###!/usr/bin/env bash set -e ### shellcheck source=/dev/null . .venv/bin/activate dlctl ml server \u0026#34;$@\u0026#34; Using set -e will ensure that, if any command fails, the script will terminate there.\nRetrospective No. 1 # We\u0026rsquo;ll now do a retrospective on the architecture that we designed for our data lab infrastructure, identifying the good and the bad, and proposing a redesigned architecture that takes all of this into account.\nThese are my fairly unedited notes. For a more digestible version, please what the video on this topic, where I restructure this into smaller topics fitting of a slide deck.\nWhat Went Well # The architecture was deployable, and everything works! Having a custom Ubuntu image for GitLab runners was useful to avoid constant apt update and package installs, which take time to download and install, each time a job with these requirements is run. Container registry was already useful for the Ubuntu custom image. To Improve # Splitting Docker into multiple VMs was a bad move‚Äîa single beefier instance would have been better. It\u0026rsquo;s easier to pool resources, but it also lowers the overhead of communicating among services within the same Docker instance. Using GitLab for secret management along with .env and terraform.tfvars is a pain‚Äîwe might have been better off deploying HashiCorp Vault into Layer 1 and just using that for everything, with a dev version on docker compose for a local deployment. We might use a Vault Agent to load secrets as env vars as well. We have a container registry, but we haven\u0026rsquo;t used it yet‚Äîwe might need a workflow to manage images separately while tracking their versions. It might have been better to go with Gitea, which still has a container registry as well as CI/CD runners, rather than GitLab, given resource constraints. GitLab is also quite bloated for a small home lab, running a few redundant services by default, like its own PostgreSQL instance, which we don\u0026rsquo;t need, Prometheus, which we don\u0026rsquo;t care about, or Praefect for HA, which we don\u0026rsquo;t use. GitLab\u0026rsquo;s CI/CD must be defined in a single pipeline, as there are no separate workflows, like with GitHub Actions, or Gitea CI/CD for that matter. Documentation is hard to browse, mainly due to the project\u0026rsquo;s complexity and dimension. Some components can be quite slow likely due to the interpreted nature of the Ruby language (e.g., starting gitlab-rails console takes nearly 30 secondsü•∂). Monetization is dependent on feature tiers, which makes it harder to get into (e.g., multi-project pipelines that require needs:project only work in Premium or Ultimate tiers). Given the single workflow/pipeline approach of GitLab CI/CD, and assuming we would continue to use GitLab, a cleaner way to activate different workflows would have been to use a boolean input for each workflow to determine whether to activate the corresponding jobs‚Äîthis is cleaner and more general than relying on non-empty values. We used a few init services on our Docker Compose project, but we could have just implemented this via CI/CD and strip it completely from Compose. Maybe we could have produced a single Terraform project for all layers of the infrastructure, although it\u0026rsquo;s unclear whether setting layer dependencies would be needlessly complex to manage. Not using Ansible was a bad move‚Äîcloud-init is great for provisioning standard VMs, but not to handle configs, specially when we might need to change them. Having a proxy cache would be useful to avoid too many requests to package repositories (e.g., apt), specially during the initial setup stage, but also if we\u0026rsquo;re continuously installing packages within runners for a few workflows, it will make sense to avoid constantly connecting to the base servers, both to ease load on the servers and to improve speed locally. MLflow is running on a SQLite backend, but we do have a PostgreSQL instance that we should switch to. Redesigning the Architecture # Here are the overall changes for each layer:\nL1: Foundation Add nginx to serve as a proxy cache for apt, apk, or others. Add HashiCorp Vault, since it integrates with the shell environment, via the vault agent, with Terraform, via its official provider, and with CI/CD, either via the vault agent, or through the Terraform provider, depending on whether we prefer a more or less real-time configuration update. Keep Terraform for deployment, but replace mutable configuration management with Ansible. L2: Platform Combine the three Docker VMs into a single VM. Keep Terraform for deployment, but replace mutable configuration management with Ansible. L3: Services Nothing changes here, except we extracted DuckLake into its own \u0026ldquo;L3: Edge\u0026rdquo; layer, since it doesn\u0026rsquo;t really run on the infrastructure, at least not directly, but on client machines, like a desktop or laptop, connecting to the PostgreSQL and MinIO instances. L4: Applications Added NodeJS as an example, since we might want to deploy our own web apps (e.g., dynamic visualizations for our data science projects). Made it clear that all apps are deployed as containers in this layer. Notice that it might also be the case that Gitea cannot adequately replace GitLab for our needs, and there is nothing free capable of doing it in a satisfactory way. We\u0026rsquo;ll need to test and compare Gitea with GitLab first. We might end up keeping GitLab in the stack‚Äîit\u0026rsquo;s hard to predict at this time.\n","date":"21 October 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-apps/","section":"","summary":"Summary # On part 5 of this series, you\u0026rsquo;ll learn how to use multi-project CI/CD with the GitLab Free tier, in order to provision resources like PostgreSQL databases and credentials.","title":"Data Lab Infra - Part 5: Retrospective \u0026 MLOps - Model Deployment","type":"posts"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/gitlab/","section":"Tags","summary":"","title":"Gitlab","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/homelab/","section":"Tags","summary":"","title":"Homelab","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/kafka/","section":"Tags","summary":"","title":"Kafka","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/mlflow/","section":"Tags","summary":"","title":"Mlflow","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/categories/mlops/","section":"Categories","summary":"","title":"MLOps","type":"categories"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/ollama/","section":"Tags","summary":"","title":"Ollama","type":"tags"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/tags/retrospective/","section":"Tags","summary":"","title":"Retrospective","type":"tags"},{"content":"","date":"14 October 2025","externalUrl":null,"permalink":"/tags/core-services/","section":"Tags","summary":"","title":"Core-Services","type":"tags"},{"content":" Summary # On part 4 of this series, you\u0026rsquo;ll learn how to automate data stack deployment using docker compose and GitLab, setting up CI/CD variables with Terraform, and using GPU passthrough to a Docker VM running on Proxmox.\nWe\u0026rsquo;ll deploy Portainer, PostgreSQL, MLflow, Kafka, and Ollama with Open WebUI. We\u0026rsquo;ll also implement a CI/CD pipeline that lets us create a PostgreSQL database and associated credentials, create a Kafka topic and initialize consumers for topics/groups, and pull an Ollama model.\nBefore starting to implement our .gitlab-ci.yml, we noticed that the CPU for the GitLab VM was overloaded, and the GitLab instance was unstable and even froze over long periods of time. We describe how we addressed these issues, having to significantly increase the resource allocation for the GitLab VM.\n\u003e Shared Service Planning # Our core services are focused on three main categories: Data Storage \u0026amp; Management, Data Processing, and ML \u0026amp; AI.\nIn the future, we would like to also consider software in the following categories: Data Visualization \u0026amp; BI, Monitoring \u0026amp; Logging, Search \u0026amp; Indexing, and Backup \u0026amp; Archiving.\nData Storage \u0026amp; Management # Category Service Description Object Storage MinIO S3-compatible object store. Already a part of the [[Layer 1 - Foundation|foundation]] layer. Relational Database PostgreSQL General-purpose relational database to share among all services. Data Lakehouse DuckLake Data Lakehouse solution (runs on top of MinIO and PostgreSQL, without any additional services required). Data Processing # Category Service Description Batch Processing DuckDB DuckLake is accessed via DuckDB, which we also use for batch processing (runs on local machines, not the data lab infra). Stream Processing Kafka Even streaming platform, combined with the faust Python library for stream processing (planned but untested; takes the place of the likes of Flink or Spark Streaming). ETL Pipelines Just Makefile style task runner. Since our pipelines are based on calling dlctl commands, just is a good minimal tool to help organize our pipelines. ML \u0026amp; AI # Category Service Description ML Platforms MLflow Supports the ML lifecycle, from experimentation to deployment. LLMs Ollama + Open WebUI Provides a local chatbot via Open WebUI and it can also be used by applications to produce LLM pipelines (e.g., using LangChain). Future Services # Other services that we might consider in the future include JupyterHub (ML \u0026amp; AI), Prometheus and Grafana (Monitoring \u0026amp; Logging), Elasticsearch (Search \u0026amp; Indexing), and even Superset (Data Visualization \u0026amp; BI). Later, we\u0026rsquo;ll also explore backup solutions, and, if it makes sense, we\u0026rsquo;ll consider switching from just to a more specialized orchestration tool, like Airflow. However, it is more likely that we downsize the data stack rather than expand it, as we are looking for a minimal but robust data stack to run on-prem.\nOur current setup is already feeling overcomplex, but we will improve on this as we go. For example, if we run Ollama on the docker-shared VM, we\u0026rsquo;ll need to enable GPU passthrough for it, but what happens when we want to run project-specific apps that also require the GPU, when those are meant to run on docker-apps? Keep in mind that consumer GPUs do not support virtualization. We might be better off with a single Docker VM for everything. And this would even make it easier to use simpler UIs, like Dockge, as a lightweight alternative to Portainer.\nGPU-Passthrough for QEMU VMs # On Layer 1 - Foundation we had setup GPU support with NVIDIA drivers for the host machine, i.e., Proxmox itself, which only works for LXCs. Since our Docker host is running on a VM, we\u0026rsquo;ll need to rollback and disable any drivers on the host machine, so that we can install them on the VM instead. Let\u0026rsquo;s go through the required steps.\nUninstall NVIDIA Drivers from Host # First, let\u0026rsquo;s uninstall the NVIDIA drivers from the host machine:\nnvidia-uninstall Delete or comment out any NVIDIA related modules from /etc/modules:\n# nvidia # nvidia-modeset # nvidia_uvm And reboot.\nEnable IOMMU and Passthrough # First, we need to edit /etc/default/grub to set:\nGRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;quiet amd_iommu=on iommu=pt\u0026#34; Setting iommu=pt is what enables passthrough (pt). If your CPU is intel, use intel_iommu=on instead. You can use lscpu to check which CPU you have.\nThen, add the following modules to /etc/modules:\nvfio vfio_iommu_type1 vfio_pci vfio_virqfd Now, let\u0026rsquo;s configure VFIO, but first let\u0026rsquo;s get the device IDs for the NVIDIA device:\nlspci -nn | grep NVIDIA In my case, this is the output I get, with four IDs:\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation TU106M [GeForce RTX 2060 Mobile] [10de:1f15] (rev a1) 01:00.1 Audio device [0403]: NVIDIA Corporation TU106 High Definition Audio Controller [10de:10f9] (rev a1) 01:00.2 USB controller [0c03]: NVIDIA Corporation TU106 USB 3.1 Host Controller [10de:1ada] (rev a1) 01:00.3 Serial bus controller [0c80]: NVIDIA Corporation TU106 USB Type-C UCSI Controller [10de:1adb] (rev a1) However, we\u0026rsquo;ll only need the IDs for VGA and Audio, which, in our case, are 10de:1f15 and 10de:10f9, respectively. Now, we can edit /etc/modprobe.d/vfio.conf:\noptions vfio-pci ids=10de:1f15,10de:10f9 disable_vga=1 Make sure to replace the two IDs with the ones for your own system!\nFinally, we should update grub and initramfs, and then reboot:\nupdate-grub update-initramfs -u -k all reboot Check if Passthrough Is Active # You can check that VFIO is the active driver for your GPU, by running the following command with for your PCI device (e.g., 01:00.0):\nlspci -k -nn -s \u0026#34;01:00.0\u0026#34; Among other things, it should print:\nKernel driver in use: vfio-pci If another driver is active, you might have to edit /etc/modprobe.d/blacklist.conf and blacklist all NVIDIA drivers:\nblacklist nouveau blacklist nvidia blacklist nvidiafb blacklist nvidia_drm blacklist nvidia_modeset blacklist nvidia_uvm If you had to update the blacklist, reboot, and re-run the lspci command to check again.\nUpdating VM with GPU Access # Let\u0026rsquo;s go back to our Terraform platform project and add two PCI mappings for the GPU VGA and Audio devices:\nresource \u0026#34;proxmox_virtual_environment_hardware_mapping_pci\u0026#34; \u0026#34;gpu_vga\u0026#34; { name = \u0026#34;gpu_vga\u0026#34; map = [ { node = var.pm_node path = \u0026#34;0000:01:00.0\u0026#34; id = \u0026#34;10de:1f15\u0026#34; subsystem_id = \u0026#34;17aa:3a47\u0026#34; iommu_group = 10 } ] } resource \u0026#34;proxmox_virtual_environment_hardware_mapping_pci\u0026#34; \u0026#34;gpu_audio\u0026#34; { name = \u0026#34;gpu_audio\u0026#34; map = [ { node = var.pm_node, path = \u0026#34;0000:01:00.1\u0026#34;, id = \u0026#34;10de:10f9\u0026#34;, subsystem_id = \u0026#34;10de:10f9\u0026#34; iommu_group = 10 } ] } To find out the IOMMU group and the subsystem ID, run:\nlspci -nnv -s \u0026#34;01:00.0\u0026#34; lspci -nnv -s \u0026#34;01:00.1\u0026#34; You\u0026rsquo;ll see something like:\n01:00.1 Audio device [0403]: NVIDIA Corporation TU106 High Definition Audio Controller [10de:10f9] (rev a1) Subsystem: NVIDIA Corporation TU106 High Definition Audio Controller [10de:10f9] Flags: fast devsel, IRQ 255, IOMMU group 10 Memory at d1000000 (32-bit, non-prefetchable) [disabled] [size=16K] Capabilities: [60] Power Management version 3 Capabilities: [68] MSI: Enable- Count=1/1 Maskable- 64bit+ Capabilities: [78] Express Endpoint, IntMsgNum 0 Capabilities: [100] Advanced Error Reporting Kernel driver in use: vfio-pci Kernel modules: snd_hda_intel Under Subsystem you\u0026rsquo;ll find the ID in brackets, and under Flags you\u0026rsquo;ll find IOMMU group \u0026lt;n\u0026gt;.\nWe then modified our existing proxmox_virtual_environment_vm.docker resource to make sure that any GPU-enabled Docker VM will use q35 for the machine, adding a mapping for the GPU devices (notice that only one VM can access the GPU, so only docker-shared will have GPU support):\nmachine = try(local.docker[count.index].gpu, false) ? \u0026#34;q35\u0026#34; : \u0026#34;pc\u0026#34; dynamic \u0026#34;hostpci\u0026#34; { for_each = try(local.docker[count.index].gpu, false) ? [ proxmox_virtual_environment_hardware_mapping_pci.gpu_vga.name, proxmox_virtual_environment_hardware_mapping_pci.gpu_audio.name ] : [] content { device = \u0026#34;hostpci${hostpci.key}\u0026#34; mapping = hostpci.value pcie = true } } After we terraform apply and once the VM boots, you should login and run:\nlspci | grep NVIDIA If you find two entries, for the VGA and Audio devices, similar to this, then you\u0026rsquo;ve got access to the GPU:\n01:00.0 VGA compatible controller: NVIDIA Corporation TU106M [GeForce RTX 2060 Mobile] (rev a1) 02:00.0 Audio device: NVIDIA Corporation TU106 High Definition Audio Controller (rev a1) Installing NVIDIA Drivers on the VM # We use cloud-init to install the NVIDIA drivers and add GPU support to Docker. At the stage when we install the drivers, it\u0026rsquo;s likely that on old kernel will be running, so we need to take measures to ensure that the driver will be installed for the kernel loaded in the next boot (i.e., the latest installed). After setting the NVIDIA driver version on local.nvidia_driver_version, the installation is done as follows:\n## This retrieves the kernel name (e.g., 6.8.0-85-generic). ls /boot/vmlinuz-* | sort | tail -n1 | cut -d- -f2- \\ \u0026gt; /run/nvidia-kernel-name ## Install dependencies for building the NVIDIA driver. apt update \u0026amp;\u0026amp; apt install -y build-essential \\ dkms linux-headers-$(cat /run/nvidia-kernel-name) ## Download the installation script. wget https://us.download.nvidia.com/XFree86/Linux-x86_64/${local.nvidia_driver_version}/NVIDIA-Linux-x86_64-${local.nvidia_driver_version}.run ## Run the installation script for kernel that will be loaded on the ## next boot, and DKMS will ensure the driver will be rebuilt when a ## new kernel is installed. sh NVIDIA-Linux-x86_64-${local.nvidia_driver_version}.run \\ --silent --dkms --kernel-name=$(cat /run/nvidia-kernel-name) ## Load the nvidia driver kernel modules. cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;/etc/modules-load.d/modules.conf nvidia nvidia-modeset nvidia_uvm EOF ## Update initamfs to install the modules. update-initramfs -u ## List all GPU devices, make GPU devices accessible by all users, and ## load required NVIDIA modules dynamically, when not loaded. cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;/etc/udev/rules.d/70-nvidia.rules KERNEL==\u0026#34;nvidia\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-smi -L \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia*\u0026#39;\u0026#34; KERNEL==\u0026#34;nvidia_modeset\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-modprobe -c0 -m \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia-modeset*\u0026#39;\u0026#34; KERNEL==\u0026#34;nvidia_uvm\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-modprobe -c0 -u \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia-uvm*\u0026#39;\u0026#34; EOF We turned the cloud-init config from Docker VMs into a Terraform template, adding the previous commands as extracmds when required (i.e., gpu = true). Once the Docker VM is provisioned with GPU access, you should be able to run nvidia-smi with any user and get information about your available GPUs and running processes.\nThat\u0026rsquo;s it, we can run Ollama! üéâ\nInstalling Services # Besides Portainer, which we\u0026rsquo;ll use to monitor our Docker hosts and run small deployment tests, these are the services that we actually need to deploy:\nPortainer PostgreSQL MLflow Kafka Ollama Open WebUI With the exception of Portainer and Open WebUI, these services are already configured in datalab within the docker-compose.yml that we provide for running locally. Therefore, we really just need to adapt the existing configuration, install Portainer and Open WebUI, and make sure our VM, and then Docker, both have access to the GPU, which is the bulk of the work here.\nIn our case, we\u0026rsquo;ll get 6 GiB VRAM to load models, which, in practice, is extra memory that we get to use. The NVIDIA driver can then manage shared memory, as an overflow to RAM, that will take from the VM\u0026rsquo;s assigned memory‚ÄîI would disable this completely if I could, but in consumer hardware it doesn\u0026rsquo;t seem possible.\nWe\u0026rsquo;ll also migrate our existing docker-compose.yml into the new infra/services/compose.yml file, keeping MinIO available through a dev profile. You\u0026rsquo;ll have to run:\ndocker compose --profile dev up -d This way, we\u0026rsquo;ll keep things tidy.\nSetting Up CI/CD # Before being able to setup a .gitlab-ci.yml to automate docker compose stack deployment, we hit a snag with GitLab that we had to solve. The following section describes the problem and how we solved it, while the last section covers the actual CI/CD implementations.\nHandling GitLab Instability # While I was reworking the Docker VMs, to configure GPU passthrough and install the NVIDIA drivers, I had to destroy the docker-gitlab VM and even reboot Proxmox to uninstall the NVIDIA drivers from the host, as I\u0026rsquo;m not using the GPU on LXCs. Out of nowhere, my GitLab instance started experiencing an extremely high CPU load.\nI ended up increasing the number of cores from 1 to 2, leaving only 1 core dedicated to the host. As planned, increasing the number of cores didn\u0026rsquo;t require the VM to be destroyed, but it did stop it to make change, and then booted it up again. GitLab once again resumed its operation, hitting 100% of 2 cores for a while. Moments later, it looked like it had become stable, but once I visited my repo I got a UI message saying An error occurred while fetching commit data and it froze, experiencing a high CPU load yet again.\nAt that point, after a few VM reboots to regain control, I noticed that kswapd0 was working intensively and that the VM was listing only 2 GiB RAM internally when running free -h, which was the minimum set for ballooning. While it could theoretically be assigned 6 GiB RAM, and other VMs were using only a small fraction of their RAM, memory didn\u0026rsquo;t seem to expand to 6 GiB, so I ended up increasing the floating setting to 4096, with an extra 2048 that I took from docker-apps. Since there is no swap anywhere, kswapd0 was trying to get memory memory from other VMs with ballooning enabled, and it kept trying, while hogging resources.\nDespite all efforts, the GitLab instance remained unstable, so, as a final attempt, I decided to move into resource overcommitment territory, increasing allocation to 4 cores and 8 GiB of RAM‚Äîwithout ballooning, i.e., the 8 GiB were 100% dedicated to the VM. This finally worked, so I decided to move forward with GitLab, for now at least, despite it feeling quite bloated, which I dislike‚ÄîUNIX philosophy and all.\nBesides the bloat‚Äîwhich might be justified‚ÄîI also started questioning whether I could even run GitLab, given my resource constraints, without compromising my main services. Gitea has been reported to run with as little as 1 core and 512 MiB RAM, and I did run it in the past on a NAS with a 4-core arm64 and 1 GiB RAM, so I know it runs on environments with limited resources, albeit slower. In the future, I might do a trial run for Gitea, and consider migrating if it runs with less resources, while still fitting my needs. Something for another blog post and video though!\nEither way, lesson learned: don\u0026rsquo;t go below 4 cores and 8 GiB of RAM for GitLab!\nUpdated Resource Allocation Table # The updated resource allocation table, with resource overcommitment, is the following:\nID VM/CT Cores Disk Dedicated Mem. Floating Mem. Free on Max. 101 minio 2 200 2048 2048 4096 201 gitlab 4 100 8192 8192 4096 202 docker-gitlab 1 100 6144 2048 0 203 docker-shared 8 200 20480 12288 -4096 204 docker-apps 2 100 10240 4096 -2048 ALL TOTALS 17 700 47104 22528 -14336 We have now oversubscribed 1 core‚Äî17 cores out of 16 available cores are allocated to VMs. This not only means that there is no dedicated core for the host, but also that, in the unlikely scenario that all VMs try to use 100% CPU power, there won\u0026rsquo;t be enough resources available, which will make them compete for CPU time‚Äîthis is tracked in KVM as steal time‚Äîwhich is ok, as long as we don\u0026rsquo;t abuse it too much.\nWe have also overcommitted 14,336 MiB of memory, over the maximum host memory of 32 GiB. Since we no swap on the host, we\u0026rsquo;ll monitor memory usage and OOM kills so that we tune the overall memory allocation based on real-world statistics, or we decide to simply buy more RAM. In practice, we should never have negative values on the \u0026ldquo;Free on Max.\u0026rdquo; column, except for its total‚Äînegative values mean that the VM will never be able to fully allocate its \u0026ldquo;Dedicated Mem.\u0026rdquo;, even when all other VMs are at minimum usage (i.e., only \u0026ldquo;Floating Mem.\u0026rdquo;, the minimum, is allocated).\nAutomating Data Stack with CI/CD # Creating a GitLab Project # First, you need to login to a non-root user on GitLab, and create a new project. I suggest that you call it datalab, but that\u0026rsquo;s optional. Once created, go into Settings ‚Üí Access tokens and add a new terraform token, with Maintainer role, and api permissions. This will be used to initialize all GitLab variables with your local .env configs, so you don\u0026rsquo;t have to manually input the variables twice (i.e., for generic local use and for CI/CD).\nUnder infra/services/gitlab/terraform.tfvars, make sure to set:\ngitlab_token = \u0026#34;glpat-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.XX.XXXXXXXXX\u0026#34; Then, after having configured your .env, run terraform apply under infra/services/gitlab as usual. This will create the corresponding GitLab variables under your project, masking and hiding all variables with a name containing PASSWORD, ACCESS_KEY, or SECRET.\nSetting Up Git for Deployment # After cloning the datalab repo from GitHub, you should setup your GitLab remotes as follows:\ncd datalab/ git remote add infra git@gitlab:datalabtechtv/datalab.git git config remote.infra.push \u0026#34;+HEAD:infra\u0026#34; This will enable push-only to a GitLab project of your choice, always to the infra branch, always force pushing. This way, we can deploy from our main or dev branch, by running:\ngit push infra Organizing CI/CD Pipeline # Once the variables are available to CI/CD, we can create our .gitlab-ci.yml file. Unfortunately, since GitLab doesn\u0026rsquo;t support multiple workflows, all of our workflows will live under the same pipeline (i.e., Docker Compose deployments, PostgreSQL database and credential creation, Kafka topics and groups, Ollama models).\nWe\u0026rsquo;ll define four stages, one per workflow, with task dependencies being set through needs rather than stages:\nstages: - deploy - postgres - kafka - ollama We\u0026rsquo;ll then split the workflows into four includable files under .ci/:\ninclude: - .ci/deploy.yml - local: .ci/postgres.yml inputs: db_user: $[[ inputs.postgres_db_user ]] db_name: $[[ inputs.postgres_db_name ]] - local: .ci/kafka.yml inputs: topic: $[[ inputs.kafka_topic ]] group: $[[ inputs.kafka_group ]] - local: .ci/ollama.yml inputs: pull: $[[ inputs.ollama_pull ]] Each of these files will have their own spec.inputs, as will our main pipeline under .gitlab-ci.yml, as the header:\nspec: inputs: postgres_db_user: description: \u0026#34;PostgreSQL username to create and grant privileges to\u0026#34; type: string default: \u0026#34;\u0026#34; postgres_db_name: description: \u0026#34;PostgreSQL database to create\u0026#34; type: string default: \u0026#34;\u0026#34; kafka_topic: description: \u0026#34;Kafka topic to create\u0026#34; type: string default: \u0026#34;\u0026#34; kafka_group: description: \u0026#34;Kafka group to create\u0026#34; type: string default: \u0026#34;\u0026#34; ollama_pull: description: \u0026#34;Ollama model name to pull\u0026#34; type: string default: \u0026#34;\u0026#34; --- Docker Deployments # Under .ci/deploy.yml, we will trigger the deployment job whenever files under infra/services/docker/ are changed‚Äîthis only activates on push.\nservices_deploy: stage: deploy image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - docker compose -p datalab -f infra/services/docker/compose.yml up -d - docker ps rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;push\u0026#34; changes: - infra/services/docker/** As you can see, we use the docker image instead of the default ubuntu runner, setting DOCKER_HOST to the docker-shared context, directly using its hostname and port. The rules are set so that this job only activate on push and if there are changes to the corresponding files. This assumes that the CI/CD variables are already available within the GitLab project.\nCompose Project Details # Our Docker Compose Project has 7 services and an additional 3 init services, that run only once (we set them to restart: no).\nIn order to ensure datalab remains backward compatible in the sense that we can still run the whole stack on a local Docker instance, we include minio as a service, under an optional dev project. MinIO is not a part of the data stack running on layer 3, since we already run it on layer 1, however we keep this as a convenience for quickly spinning up the datalab stack locally.\nApart from minio, we also provide postgres, with an admin user root, ollama, open-webui, mlflow running on a SQLite backend and a MinIO bucket for artifact storage, kafka as a single node without replication, running one broker and controller, and portainer to help monitor our Docker instances.\nThe provided *-init services produce a container that runs only once for initializations: minio-init creates default buckets, ollama-init pulls default models, and kafka-init creates topics and initializes consumers for topics/groups.\nIn general, each service has its own network and volume, with init services sharing the same network as their corresponding parent services. All init services depend on their parent services being on a healthy status, so that we can run commands on them. For example, for ollama-init we set:\ndepends_on: ollama: condition: service_healthy Implementing health checks on the parent services was mostly constrained by the available tools within the specific base image used for each service (e.g., curl wasn\u0026rsquo;t necessarily available). Here are is a summary of the health check commands that we used:\nService Health Check minio curl -f http://localhost:9000/minio/health/live postgres pg_isready ollama ollama ls mlflow python -c \u0026quot;import urllib.request; urllib.request.urlopen('http://localhost:5000')\u0026quot; kafka /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:29092 --list The configuration for each service is heavily dependent on environment variables defined within a .env file on the root of the project, as we don\u0026rsquo;t deploy any secrets management service, like HashiCorp Vault.\nThe open-webui service also runs on the ollama network, and the ollama service requires GPU support, which was defined as follows:\ndeploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] Finally, portainer porta 9000 was remapped to 9080, so it wouldn\u0026rsquo;t collide with minio. We opted to change the port for Portainer rather than MinIO, since this is a secondary service used only for monitoring, and it doesn\u0026rsquo;t require any configs on datalab.\nPostgreSQL Databases # Under .ci/postgres.yml, we provide a workflow to provision a PostgreSQL database and credentials, which will be used by our applications from Layer 4 (see upcoming blog post and video).\nWe specify a spec.inputs header with db_user and db_name inputs:\nspec: inputs: db_user: description: \u0026#34;PostgreSQL username to create and grant privileges to\u0026#34; type: string db_name: description: \u0026#34;PostgreSQL database to create\u0026#34; type: string --- And two jobs, psql_create_user and psql_create_db:\npsql_create_user: stage: postgres image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 before_script: - apk add --no-cache openssl script: - # OMITTED rules: - if: \u0026#39;\u0026#34;$[[ inputs.db_user ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never psql_create_db: stage: postgres image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - # OMITTED needs: - psql_create_user rules: - if: \u0026#39;\u0026#34;$[[ inputs.db_name ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never Notice that psql_create_db needs psql_create_user, so it will always run after it.\nAlso notice that each job will only activate when the corresponding input is non-empty, which only happens when we manually launch the CI/CD pipeline, filling those values, via Build ‚Üí Pipelines ‚Üí New pipeline, as shown in the following screenshot:\nKafka Topics and Groups # Under .ci/kafka.yml, we provide a workflow to create a Kafka topics and initialize consumers for topics/groups. Similarly, this will be used by our applications from Layer 4.\nWe specify a spec.inputs header with topic and group inputs:\nspec: inputs: topic: description: \u0026#34;Kafka topic to create\u0026#34; type: string group: description: \u0026#34;Kafka group to create\u0026#34; type: string And two jobs, kafka_create_topic and kafka_init_consumer:\nkafka_create_topic: stage: kafka image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - # OMITTED rules: - if: \u0026#39;\u0026#34;$[[ inputs.topic ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never kafka_init_consumer: stage: kafka image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - # OMITTED needs: - kafka_create_topic rules: - if: \u0026#39;\u0026#34;$[[ inputs.topic ]]\u0026#34; != \u0026#34;\u0026#34; \u0026amp;\u0026amp; \u0026#34;$[[ inputs.group ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never Again, notice a dependency of kafka_create_topic by kafka_init_consumer, and notice that these jobs require their corresponding inputs to be non-empty to run (if: ...), otherwise they will never run (when: never).\nOllama Models # Under .ci/ollama.yml, we provide a very basic workflow to pull models. There is no workflow to otherwise manage models.\nHere is the complete content for the Ollama CI workflow:\nspec: inputs: pull: description: \u0026#34;Pull Ollama model\u0026#34; type: string --- ollama_pull: stage: ollama image: docker:28.4.0-cli variables: DOCKER_HOST: tcp://docker-shared:2375 script: - \u0026#39;echo \u0026#34;Installing Ollama model: $[[ inputs.pull ]]\u0026#34;\u0026#39; - docker exec datalab-ollama-1 ollama pull $[[ inputs.pull ]] rules: - if: \u0026#39;\u0026#34;$[[ inputs.pull ]]\u0026#34; != \u0026#34;\u0026#34;\u0026#39; - when: never The logic is similar to the one described for the PostgreSQL and Kafka CI workflows.\nFinal Remarks # There is a lot to criticize based on our design choices, as well as tech stack selection. Next time, we\u0026rsquo;ll show you how to deploy an application using this stack, but also do a retrospective on what could be improved. Overall, the same logic will apply, but there\u0026rsquo;s a few things we would definitely change, mainly regarding the decision to use GitLab versus Gitea. There are also a few added complexities that we would like to simplify, while at the same time revising our decision to purely rely on GitLab for secrets management rather than a more dedicated solution like HashiCorp Vault.\nSubscribe to @DataLabTechTV on YouTube, or keep following this blog so you don\u0026rsquo;t miss it! There\u0026rsquo;s RSS, if you\u0026rsquo;re old-school like me, or social media (Bluesky, Reddit, Discord), if you prefer it that way.\n","date":"14 October 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-services/","section":"","summary":"Summary # On part 4 of this series, you\u0026rsquo;ll learn how to automate data stack deployment using docker compose and GitLab, setting up CI/CD variables with Terraform, and using GPU passthrough to a Docker VM running on Proxmox.","title":"Data Lab Infra - Part 4: Core Services","type":"posts"},{"content":"","date":"14 October 2025","externalUrl":null,"permalink":"/tags/open-webui/","section":"Tags","summary":"","title":"Open-Webui","type":"tags"},{"content":"","date":"14 October 2025","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"","date":"30 September 2025","externalUrl":null,"permalink":"/tags/cloud-init/","section":"Tags","summary":"","title":"Cloud-Init","type":"tags"},{"content":" Summary # On part 3 of this series, you\u0026rsquo;ll learn how to provision Proxmox VMs with Terraform, using QEMU to build on top of the Ubuntu\u0026rsquo;s official cloud image (qcow2).\nWe\u0026rsquo;ll deploy GitLab and Docker using cloud-init, configuring GitLab as a container registry, and deploying a shared Docker runner for CI/CD that executes on a separate VM.\nWe\u0026rsquo;ll also learn how to setup CI/CD variables and secrets, and how to write a simple CI/CD job, using a .gitlab-ci.yaml file, to print information about the runner instance, as well as a couple of variables that we set in via the web UI.\nAgain, keep in mind that these skills are loosely transferable to cloud platforms like AWS, GCP or Azure, with the advantage that it costs zero to setup Proxmox at home, if you\u0026rsquo;ve got some old hardware lying around.\n\u003e Resource Planning # Our Proxmox node has 16 CPU cores, 32 GiB RAM, and a 1 TiB hard drive, that we can allocate to CTs and VMs. For the foundation layer (L1), we allocated 2 CPU cores and 2048 MiB RAM to an LXC, but, for the platform layer (L2), we\u0026rsquo;ll need to plan a bit better, as L2 handles the most load, particularly on the docker-shared VM, as it runs all core services that are accessed by the other data stack components and applications.\nWe also want to keep 2 CPU cores available to the Proxmox hypervisor at all times. Proxmox also requires a minimum of 1 GiB RAM to run, but our node was using 2 GiB RAM, so this is our target free RAM.\nBelow, we plan global memory allocation based on the maximum active memory for a single VM, while assuming other VMs remain at the minimum usage set by floating memory. Such an assumption is based on the fact that this infrastructure will be used by a single person. You should do a similar exercise for your hardware and use case, but the overall logic remains useful.\nNotice that we might still observe a heavy load on gitlab, docker-gitlab, and docker-shared, at the same time‚Äîe.g., gitlab launches a docker-gitlab runner to update the docker compose stack on docker-shared. Regardless, the worst case scenario based on a single VM is still a good average to target.\nBelow, you find the resource allocation table for all the CTs and VMs in our Proxmox node. You\u0026rsquo;ll find the ID and name for the VM or CT, along with the allocated number of CPU cores, disk space (GiB), dedicated memory (MiB), and floating memory (MiB). We also include a \u0026ldquo;Free on Max.\u0026rdquo; column with the free or unallocated memory when the given VM is using all of its dedicated memory. This is calculated by subtracting the total floating memory (22,528 MiB) and the difference between the VM\u0026rsquo;s dedicated and floating memory (e.g., 20,480 MiB and 12,288 MiB for VM 203) from the total available memory (32,768 MiB). As such, for VM 203, the free memory on maximum usage is calculated as $32768 - 22528 - (20480 - 12288) = 2048$.\nID VM/CT Cores Disk Dedicated Mem. Floating Mem. Free on Max. 101 minio 2 200 2048 2048 10240 201 gitlab 1 100 6144 2048 6144 202 docker-gitlab 1 100 6144 2048 6144 203 docker-shared 8 200 20480 12288 2048 204 docker-apps 2 100 12288 4096 2048 ALL TOTALS 14 700 47104 22528 -14336 As we can see, worst case scenario, assuming maximum memory usage only happens for a single VM at a time, we get 2,048 MiB RAM free at all times.\nIf we look at the totals, we find only 14 CPU cores were allocated, leaving 2 cores for Proxmox, as required. We can also see that, at maximum usage for all CTs and VMs, our system would require 14,336 MiB of additional RAM.\nThis is an estimate, but, with real-world usage statistics, we can better tune VM configurations simply by editing our Terraform variables for cpu.cores and memory.*. And, if there aren\u0026rsquo;t enough resources, this is a good reason to go look for a mini PC to expand our home lab! üòâ\nBackup Strategy? # During foundation layer deployment, we had setup a separate volume for /data. In most cloud platforms, we can easily detach a data volume while recreating the root volume, or even create snapshots for it that we can later restore. On the other hand, Proxmox only provides snapshots for the whole VM, which is not practical to manage as a backup solution. As such, we simplified our MinIO LXC to use a single root disk, and we do the same for the VMs in the platform layer.\nBackups will be added later on and they will be application-specific. The overall strategy will be based on mounting an external volume from a NAS under /backups for each VM, and scheduling a backup process based on a systemd timer and service, which will write to that directory.\nFor our current use-case, backups are not particularly concerning, as we can always re-ingest our lakehouse datasets, or recreate our GitLab repos, as these will only be used for CI/CD, which obvious also require that Docker images are re-registered, and variables and secrets are re-added. While this might justify setting up a proper backup strategy, failure wouldn\u0026rsquo;t be catastrophic. This is perfectly fine for a home lab setting, where we\u0026rsquo;re constantly tearing down and re-deploying our infrastructure.\nDocker VMs # We\u0026rsquo;ll provision three Docker VMs using Terraform, the bpg/proxmox provider, and cloud-init:\ndocker-gitlab, where our GitLab Runner will run CI/CD jobs; docker-shared, to deploy the core services for our data stack (PostgreSQL, Apache Kafka, etc.); docker-apps, where project-specific services will live. Cloud-Init # Wait, so did we drop Packer? Yes, we did. We ended up going with a simpler cloud-init config instead, since there really isn\u0026rsquo;t much to deploy on each VM, or too much to gain from building a single image to deploy only three Docker VMs. We might still do Packer in the future, for the educational value, but, for the home lab, we want to keep it as minimalistic as possible.\nHere\u0026rsquo;s the cloud-init config that we used to setup our Docker VMs. This was directly extracted from our Terraform config, which is currently available at the dev branch of the datalab repo, so you\u0026rsquo;ll find ${...} blocks which represent variable or resource output replacements.\n#cloud-config hostname: \u0026#34;${local.docker[count.index].name}\u0026#34; password: \u0026#34;${random_password.docker_vm[count.index].result}\u0026#34; chpasswd: expire: false ssh_pwauth: true apt: sources: docker: source: \u0026#34;deb [arch=amd64 signed-by=/etc/apt/trusted.gpg.d/docker.gpg] https://download.docker.com/linux/ubuntu noble stable\u0026#34; key: | ${indent(8, chomp(data.http.docker_gpg.response_body))} package_update: true package_upgrade: true packages: - qemu-guest-agent - docker-ce write_files: - path: /etc/systemd/system/docker.service.d/override.conf owner: \u0026#39;root:root\u0026#39; permissions: \u0026#39;0600\u0026#39; content: | [Service] ExecStart= ExecStart=/usr/bin/dockerd - path: /etc/docker/daemon.json owner: \u0026#39;root:root\u0026#39; permissions: \u0026#39;0600\u0026#39; content: | { \u0026#34;hosts\u0026#34;: [\u0026#34;fd://\u0026#34;, \u0026#34;tcp://0.0.0.0:2375\u0026#34;], \u0026#34;containerd\u0026#34;: \u0026#34;/run/containerd/containerd.sock\u0026#34; } runcmd: - systemctl enable --now qemu-guest-agent - netplan apply - usermod -aG docker ubuntu - reboot We use the apt block to add the official repo for Docker, so that we can install docker-ce with the latest version of Docker. Then, we override the config for docker.service, which essentially means removing the arguments from dockerd so that /etc/docker/daemon.json gets used instead. We then reproduce the original CLI config and add tcp://0.0.0.0:2375 to the listening hosts for Docker. This will let us access Docker externally, as described in the following section.\nClient Access # On your local machine, create a context for each Docker VM:\ndocker context create docker-gitlab --docker \u0026#34;host=tcp://docker-gitlab:2375\u0026#34; docker context create docker-shared --docker \u0026#34;host=tcp://docker-shared:2375\u0026#34; docker context create docker-apps --docker \u0026#34;host=tcp://docker-apps:2375\u0026#34; You can then switch to any context and run Docker commands as usual:\ndocker context use docker-gitlab docker ps Often, custom shell prompts, like Starship, will display your current Docker context, when it\u0026rsquo;s something other than the default ones. I definitely recommend Starship for this, as it supports any shell, including Bash, Fish, or even PowerShell, and it will display your Docker context by default.\nGitLab VM # We\u0026rsquo;ll provision a GitLab instance, which will power multiple features of our home lab, including the container registry for custom docker images‚Äîbe it our own project-specific services or extended installations for core services‚Äîand also GitOps, using CI/CD to run docker compose deployment jobs, along with variables to store configurations and secrets.\nMinIO Bucket # As a requirement to setup the container registry component, we added the gitlab bucket to our foundation layer MinIO instance, both to the provisioning config and manually via mc (to avoid redeploying):\ncd infra/foundation mc alias set lab http://minio:9000 admin \\ $(terraform output -raw minio_admin_password) mc mb lab/gitlab This is used on the cloud-init config shown in the following section.\nCloud-Init # Here\u0026rsquo;s the cloud-init config that we used to setup our GitLab VM:\n#cloud-config hostname: ${local.gitlab.name} password: \u0026#34;${random_password.gitlab_vm.result}\u0026#34; chpasswd: expire: false ssh_pwauth: true package_update: true package_upgrade: true packages: - qemu-guest-agent - curl write_files: - path: /etc/gitlab/gitlab.rb owner: \u0026#39;root:root\u0026#39; permissions: \u0026#39;0600\u0026#39; content: | external_url \u0026#39;http://${local.gitlab.name}\u0026#39; gitlab_rails[\u0026#39;initial_root_password\u0026#39;] = \u0026#39;${random_password.gitlab_root.result}\u0026#39; gitlab_rails[\u0026#39;registry_enabled\u0026#39;] = true registry_external_url \u0026#39;http://${local.gitlab.name}:5050\u0026#39; registry[\u0026#39;database\u0026#39;] = { \u0026#39;enabled\u0026#39; =\u0026gt; true, } registry[\u0026#39;storage\u0026#39;] = { \u0026#39;s3_v2\u0026#39; =\u0026gt; { \u0026#39;regionendpoint\u0026#39; =\u0026gt; \u0026#39;${var.s3_endpoint}\u0026#39;, \u0026#39;region\u0026#39; =\u0026gt; \u0026#39;${var.s3_region}\u0026#39;, \u0026#39;accesskey\u0026#39; =\u0026gt; \u0026#39;${var.s3_access_key}\u0026#39;, \u0026#39;secretkey\u0026#39; =\u0026gt; \u0026#39;${var.s3_secret_key}\u0026#39;, \u0026#39;pathstyle\u0026#39; =\u0026gt; ${var.s3_path_style}, \u0026#39;bucket\u0026#39; =\u0026gt; \u0026#39;${var.gitlab_s3_registry_bucket}\u0026#39;, } } runcmd: - systemctl enable --now qemu-guest-agent - netplan apply - curl \u0026#34;https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh\u0026#34; | sudo bash - apt install -y gitlab-ce - gitlab-ctl reconfigure - | gitlab-rails console -e production \u0026lt;\u0026lt;EOT s = ApplicationSetting.current s.update!( signup_enabled: false, version_check_enabled: false, usage_ping_enabled: false, usage_ping_generation_enabled: false, whats_new_variant: \u0026#39;disabled\u0026#39; ) EOT - curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash - apt install -y gitlab-runner - | gitlab-runner register --non-interactive \\ --url \u0026#34;http://${local.gitlab.name}/\u0026#34; \\ --registration-token \u0026#34;$(gitlab-rails runner \u0026#39;puts ApplicationSetting.current.runners_registration_token\u0026#39;)\u0026#34; \\ --executor \u0026#34;docker\u0026#34; \\ --docker-image \u0026#34;ubuntu:latest\u0026#34; \\ --description \u0026#34;ubuntu-latest-runner\u0026#34; \\ --tag-list \u0026#34;docker,remote,ubuntu\u0026#34; \\ --run-untagged=\u0026#34;true\u0026#34; \\ --docker-host \u0026#34;tcp://${local.docker[0].name}:2375\u0026#34; - reboot Let\u0026rsquo;s break it down into. We begin by setting a few basic properties for the VM, including its hostname and password (setting to not expire, thus avoiding having to change it on the first login). We also enable password authentication for SSH, so that we can login with the default ubuntu user.\n#cloud-config hostname: ${local.gitlab.name} password: \u0026#34;${random_password.gitlab_vm.result}\u0026#34; chpasswd: expire: false ssh_pwauth: true Then, we update the system packages and install the guest agent to help Proxmox monitor the VM and handle memory ballooning, and we install curl, which will be required during GitLab installation.\npackage_update: true package_upgrade: true packages: - qemu-guest-agent - curl Next, we write the gitlab.rb file, which is used to configure GitLab when running gitlab-ctl reconfigure.\nwrite_files: - path: /etc/gitlab/gitlab.rb owner: \u0026#39;root:root\u0026#39; permissions: \u0026#39;0600\u0026#39; content: | ... You\u0026rsquo;ll find the content value below. We set the external_url to the host where GitLab will run, as well as the initial root account password. We then enable the container registry, setting it up to use the existing database, as well as our existing MinIO instance, with the newly created bucket, to store the image layers that will be pushed to the container. As you can see, most of these settings are directly managed via Terraform variables.\nexternal_url \u0026#39;http://${local.gitlab.name}\u0026#39; gitlab_rails[\u0026#39;initial_root_password\u0026#39;] = \u0026#39;${random_password.gitlab_root.result}\u0026#39; gitlab_rails[\u0026#39;registry_enabled\u0026#39;] = true registry_external_url \u0026#39;http://${local.gitlab.name}:5050\u0026#39; registry[\u0026#39;database\u0026#39;] = { \u0026#39;enabled\u0026#39; =\u0026gt; true, } registry[\u0026#39;storage\u0026#39;] = { \u0026#39;s3_v2\u0026#39; =\u0026gt; { \u0026#39;regionendpoint\u0026#39; =\u0026gt; \u0026#39;${var.s3_endpoint}\u0026#39;, \u0026#39;region\u0026#39; =\u0026gt; \u0026#39;${var.s3_region}\u0026#39;, \u0026#39;accesskey\u0026#39; =\u0026gt; \u0026#39;${var.s3_access_key}\u0026#39;, \u0026#39;secretkey\u0026#39; =\u0026gt; \u0026#39;${var.s3_secret_key}\u0026#39;, \u0026#39;pathstyle\u0026#39; =\u0026gt; ${var.s3_path_style}, \u0026#39;bucket\u0026#39; =\u0026gt; \u0026#39;${var.gitlab_s3_registry_bucket}\u0026#39;, } } Finally, under the runcmd block, which is an array of commands, we\u0026rsquo;ll essentially run the following script, which we reformat and comment:\n# This enables and starts the guest agent. systemctl enable --now qemu-guest-agent # This ensures that the hostname is broadcast to DHCP, otherwise # we\u0026#39;d need to sign in once using the IP address. netplan apply # Here we install GitLab, but this doesn\u0026#39;t start it. curl \u0026#34;https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh\u0026#34; | sudo bash apt install -y gitlab-ce # This will configure and start GitLab using the settings from # /etc/gitlab/gitlab.rb. gitlab-ctl reconfigure gitlab-rails console -e production \u0026lt;\u0026lt;EOT s = ApplicationSetting.current s.update!( signup_enabled: false, version_check_enabled: false, usage_ping_enabled: false, usage_ping_generation_enabled: false, whats_new_variant: \u0026#39;disabled\u0026#39; ) EOT # We also install the GitLab Runner locally. curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash apt install -y gitlab-runner # And we set it up as a shared runner that executes using an Ubuntu # image on a separate Docker VM (docker-gitlab). gitlab-runner register --non-interactive \\ --url \u0026#34;http://${local.gitlab.name}/\u0026#34; \\ --registration-token \u0026#34;$(gitlab-rails runner \u0026#39;puts ApplicationSetting.current.runners_registration_token\u0026#39;)\u0026#34; \\ --executor \u0026#34;docker\u0026#34; \\ --docker-image \u0026#34;ubuntu:latest\u0026#34; \\ --description \u0026#34;ubuntu-latest-runner\u0026#34; \\ --tag-list \u0026#34;docker,remote,ubuntu\u0026#34; \\ --run-untagged=\u0026#34;true\u0026#34; \\ --docker-host \u0026#34;tcp://${local.docker[0].name}:2375\u0026#34; # We reboot, as it\u0026#39;s likely Ubuntu will ask you to anyway, on first # login. reboot Testing # Repository Management # In order to test a basic git repo workflow, we do the following:\nCreate a new user with the root account, e.g., datalabtechtv. Edit the user to be able to set a default password. Sign in to the new user and change your password Create a sandbox repo. Generate a gitlab SSH key on the client machine. Add the SSH key to the new user\u0026rsquo;s settings. Clone the sandbox repo on the client machine. Container Registry # For testing the container registry, on the cloned sandbox repo, we create the following Dockerfile:\nFROM ubuntu:latest CMD [\u0026#34;echo\u0026#34;, \u0026#34;Hello GitLab Container Registry!\u0026#34;] We can optionally commit and push it:\ngit add Dockerfile git config user.name \u0026#34;Data Lab Tech TV\u0026#34; git config user.email \u0026#34;mail@datalabtechtv.com\u0026#34; git commit Dockerfile -m \u0026#34;feat: add a testing Dockerfile\u0026#34; git push Because we\u0026rsquo;re not using SSL for our GitLab instance, we\u0026rsquo;ll need to add the following config to /etc/docker/daemon.json on our client machine (also available via Docker Desktop, under Settings ‚Üí Docker Engine):\n{ \u0026#34;insecure-registries\u0026#34;: [\u0026#34;gitlab:5050\u0026#34;] } We can optionally create a personal access token (PAT) with read_registry and write_registry permissions for our user account, under Settings ‚Üí Access tokens to use as a password, but we can also login using our regular user and password, which is what we do. Either way, the command is the same:\ndocker login gitlab:5050 We can then build, tag, and push an image, making sure our tag includes the user account (e.g., datalabtechtv) and the project/repo name (e.g., sandbox), followed by our image name and version (e.g., test:latest):\ndocker build -t test:latest . docker tag test:latest gitlab:5050/datalabtechtv/sandbox/test:latest docker push gitlab:5050/datalabtechtv/sandbox/test:latest Once this is done, the image should be listed on GitLab\u0026rsquo;s UI under Deploy ‚Üí Container registry for the sandbox project. I recommend that you pin Container registry, as we\u0026rsquo;ll be using it often on our home lab in the future.\nAlso, check the gitlab bucket on MinIO to make sure that the image blobs are being written to the proper storage.\nCI/CD Workflows # Again, on the sandbox repo, create the following .gitlab-ci.yml file:\nrunner_info: script: - lscpu - free -h - df -h / - echo \u0026#34;USER:\u0026#34; $SOME_USER - echo \u0026#34;PASS:\u0026#34; $SOME_PASS Go into GitLab and, under the sandbox project, go to Settings ‚Üí CI/CD ‚Üí Variables ‚Üí CI/CD Variables ‚Üí Add variable, and add the SOME_USER and SOME_PASS variables‚Äîset the first one to \u0026ldquo;Visible\u0026rdquo; and the second one to \u0026ldquo;Masked and hidden\u0026rdquo;.\nThen you can add, commit and push:\ngit add .gitlab-ci.yml git commit .gitlab-ci.yml -m \u0026#34;ci: basic task to test runner\u0026#34; git push The job will trigger immediately, and you\u0026rsquo;ll be able to find its status and output under Build ‚Üí Jobs for the sandbox project. I also recommend that you pin this menu entry. You\u0026rsquo;ll notice that the value of SOME_USER was correctly printed, while the value of SOME_PASS was be printed as [MASKED].\nMoving forward, we\u0026rsquo;ll take advantage of the CI/CD features to setup a GitOps workflow to manage the docker compose stack for our data lab. Stay tuned for part 4 of this series!\nFinal Remarks # Despite a few bumps on the road, and questioning some decisions, we got it working smoothly in the end. Always expect the unexpected, a bit of a bumpy road, when working in operations. And be ready to make changes. Prioritize the robustness of your infrastructure rather than the successful execution of your original plan!\nOn Configuration Management\u0026hellip; # To me, it remains unclear which configuration solution is preferrable, since, to be honest, I dislike them all.\nPrebuilt images with Packer would have been ideal in a real-world scenario, if we wanted to eventually move out of qcow2 images on Proxmox VMs and into AMIs (Amazon Machine Images) on AWS EC2, but frankly this is overkill for a home lab.\nI questioned my decision not to use Ansible, which might still be the best overall solution, but it still felt slightly overkill for this. So, I ended up going with cloud-init, as it\u0026rsquo;s easily available on Proxmox, providing a few of the features that Ansible provides, but having a much simpler syntax. Although, of course, Ansible is more powerful and provides idempotency, while cloud-init is enough for most use cases, but runs only once per deployment.\nOn GitLab\u0026hellip; # After working on GitLab deployment for a while, I questioned the decision to go with GitLab and asked myself if Gitea would have been a better and more lightweight option, with it being written in Go rather than Ruby, like GitLab.\nWhile the apt package for GitLab installs flawlessly, and all the tooling is robust, there\u0026rsquo;s a lot to improve as well. For example, most configs can be done via /etc/gitlab/gitlab.rb, which is not the best file format for configurations. Why not toml or yaml instead? And even disregarding that, a few configurations cannot be done via gitlab.rb, requiring gitlab-rails console instead. Fortunately it can read from the standard input, but it\u0026rsquo;s also extremely slow‚ÄîI even thought there was some error as it was starting, because it took quite a while without producing any message whatsoever.\nRegardless, I cannot complain much. Everything worked pretty much at first try, after following the documentation and asking ChatGPT a few things. Also, once deployed, performance is quite satisfactory, with GitLab being quite smooth to use.\n","date":"30 September 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-platform/","section":"","summary":"Summary # On part 3 of this series, you\u0026rsquo;ll learn how to provision Proxmox VMs with Terraform, using QEMU to build on top of the Ubuntu\u0026rsquo;s official cloud image (qcow2).","title":"Data Lab Infra - Part 3: Platform Setup with Terraform","type":"posts"},{"content":"","date":"30 September 2025","externalUrl":null,"permalink":"/tags/platform/","section":"Tags","summary":"","title":"Platform","type":"tags"},{"content":"","date":"30 September 2025","externalUrl":null,"permalink":"/tags/proxmox/","section":"Tags","summary":"","title":"Proxmox","type":"tags"},{"content":" Summary # On part 2 of this series, you\u0026rsquo;ll learn the basics of Terraform, provisioning a MinIO server as an LXC (Linux Container) running on Proxmox.\nThen, you\u0026rsquo;ll learn how to use this foundation infrastructure, consisting of the MinIO S3-compatible object store, to track the state of a separate Terraform project in a way that will let you update your infrastructure from any location with access to your Proxmox instance.\nThese skills are loosely transferable to cloud platforms like AWS, GCP or Azure, with the advantage that it costs zero to setup Proxmox at home, if you\u0026rsquo;ve got some old hardware lying around.\n\u003e Proxmox # Terraform Access # After installing Proxmox, we\u0026rsquo;ll create a terraform user. This needs to be a PAM user, since Terraform\u0026rsquo;s bpg/proxmox provider will need shell access to be able to run a few actions that are not supported by the API, like downloading Debian or Ubuntu images. We\u0026rsquo;ll disable password login for this user, because we\u0026rsquo;ll exclusively rely on SSH keys for shell access.\nWe create the system user:\nadduser --disabled-password terraform pveum user add terraform@pam And an associated role with the permissions required by the provider:\npveum role add Terraform -privs \"Realm.AllocateUser, VM.PowerMgmt, VM.GuestAgent.Unrestricted, Sys.Console, Sys.Audit, Sys.AccessNetwork, VM.Config.Cloudinit, VM.Replicate, Pool.Allocate, SDN.Audit, Realm.Allocate, SDN.Use, Mapping.Modify, VM.Config.Memory, VM.GuestAgent.FileSystemMgmt, VM.Allocate, SDN.Allocate, VM.Console, VM.Clone, VM.Backup, Datastore.AllocateTemplate, VM.Snapshot, VM.Config.Network, Sys.Incoming, Sys.Modify, VM.Snapshot.Rollback, VM.Config.Disk, Datastore.Allocate, VM.Config.CPU, VM.Config.CDROM, Group.Allocate, Datastore.Audit, VM.Migrate, VM.GuestAgent.FileWrite, Mapping.Use, Datastore.AllocateSpace, Sys.Syslog, VM.Config.Options, Pool.Audit, User.Modify, VM.Config.HWType, VM.Audit, Sys.PowerMgmt, VM.GuestAgent.Audit, Mapping.Audit, VM.GuestAgent.FileRead, Permissions.Modify\" pveum acl modify / -user terraform@pam -role Terraform We\u0026rsquo;ll need an API key as well, which we can create with:\npveum user token add terraform@pam datalabtech --privsep 0 We disable Privilege Separation to make sure that we use the Terraform role we defined previously, but API keys can have their own, more restrict permissions. Make sure to save the token ID and value to use when configuring Terraform.\nNow, on your host machines‚Äîthose that will be able to run Terraform‚Äîcreate an SSH key to be added to the terraform account:\nssh-keygen -t ed25519 -C proxmox -f ~/.ssh/proxmox Currently, ed25519 is the default key type anyway, but making it explicit ensures the command will endure the test of time. We also use -C so that information about our user and host is not leaked in the key. Finally, the naming scheme for the key file is simply the name of the target service‚Äîif we had multiple accounts for a service (e.g., multiple GitHub accounts), then we\u0026rsquo;d use something like github-account1, github-account2, etc., but one thing we should avoid is sharing keys, even between the same user on different machines.\nCopy the public key from the host machine:\ncat ~/.ssh/proxmox.pub | pbcopy And then just drop it on Proxmox under /home/terraform/.ssh/authorized_keys:\nmkdir ~terraform/.ssh vi ~terraform/.ssh/authorized_keys # Paste public key and save From your host machine, you can then test if access was correctly setup (e.g., using proxmox as your hypervisor host):\nssh -i ~/.ssh/proxmox terraform@proxmox GPU Support # It\u0026rsquo;s possible to passthrough your GPU NVIDIA card to VMs or CTs. This is how we setup the hypervisor machine drivers.\nStep 1: Blacklist nouveau # We\u0026rsquo;ll want to use the proprietary drivers, so we need to disable the open source nouveau drivers.\nvi /etc/modprobe.d/blacklist.conf Add the following line:\nblacklist nouveau And then run:\nupdate-initramfs -u reboot Step 2: Install NVIDIA drivers # Under Updates ‚ñ∂ Repositories, make sure that the No-Subscription and Ceph No-Subscription repositories are configured and that the corresponding Enterprise versions are disable.\nThen run:\napt update \u0026amp;\u0026amp; apt upgrade -y apt install pve-headers build-essential -y Download the official NVIDIA drivers for Linux 64-bit, assuming that\u0026rsquo;s your arch. I usually just search for my card and copy the link to the .run script, so I can download directly to Proxmox using wget. For example:\nwget https://us.download.nvidia.com/XFree86/Linux-x86_64/\u0026lt;version\u0026gt;/NVIDIA-Linux-x86_64-580.82.09.run sh NVIDIA-Linux-x86_64-580.82.09.run You can accept 32-bit libraries, if the option is provided (but I didn\u0026rsquo;t). Don\u0026rsquo;t run the nvidia-xconfig utility, as Proxmox is headless, and there is no X11 installation. You can also safely ignore any warnings about inferring or not finding X11 libraries.\nStep 3: Load drivers on boot # In order for drivers to be loaded on boot, you need to edit your modules.conf:\nvi /etc/modules-load.d/modules.conf Add:\nnvidia nvidia-modeset nvidia_uvm And run:\nupdate-initramfs -u Also edit:\nvi /etc/udev/rules.d/70-nvidia.rules And add:\nKERNEL==\u0026#34;nvidia\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-smi -L \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia*\u0026#39;\u0026#34; KERNEL==\u0026#34;nvidia_modeset\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-modprobe -c0 -m \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia-modeset*\u0026#39;\u0026#34; KERNEL==\u0026#34;nvidia_uvm\u0026#34;, RUN+=\u0026#34;/bin/bash -c \u0026#39;/usr/bin/nvidia-modprobe -c0 -u \u0026amp;\u0026amp; /bin/chmod 666 /dev/nvidia-uvm*\u0026#39;\u0026#34; Then you can reboot, and run nvidia-smi to ensure the drivers are operational. That\u0026rsquo;s it. Everything else will be done at the VM or CT level, and we\u0026rsquo;ll discuss it at a later time.\nTerraform # Installation # The best way to install Terraform is using tfswitch. Follow the installation instructions and, once that is done, just run tfswitch and select your version of Terraform.\nIf you have other initialized Terraform projects, running tfswitch from the root directory will automatically install the version of Terraform required for that project. You\u0026rsquo;ll need to re-run this for Terraform projects that require different versions.\nThe version is usually setup under versions.hcl as follows:\nterraform { required_version = \u0026#34;~\u0026gt; 1.13.2\u0026#34; } Accessing Secrets # By default, running terraform output will redact sensitive variables (i.e., secrets), but it\u0026rsquo;s possible access the value with the -raw argument, only for a specific output. We suggest that you never print the secret plainly to the console, but instead pipe it to a copy command, like pbcopy (pasteboard copy) on Mac, or xclip -selection clipboard on Linux (I personally alias this to pbcopy on Linux as a convention).\nterraform -chdir=infra/foundation \\ output -raw minio_admin_password | pbcopy S3 State Storage # Since variables can only be accessed after terraform init, we cannot use regular variables to configure the backend for state storage. Instead, the documentation suggests that we use a state.config file for this.\nSo, we begin with an empty backend config:\nterraform { backend \u0026#34;s3\u0026#34; {} } And we produce a state.config file that looks like this:\nbucket = \u0026#34;terraform\u0026#34; key = \u0026#34;state/platform/terraform.tfstate\u0026#34; endpoints = { s3 = \u0026#34;http://minio:9000\u0026#34; } region = \u0026#34;eu-west-1\u0026#34; access_key = \u0026#34;admin\u0026#34; access_key = \u0026#34;XXXXXXXXXXXXXXXXXXXX\u0026#34; skip_credentials_validation = true skip_metadata_api_check = true skip_requesting_account_id = true use_path_style = true The previous file is already provided to you under state.config.example. You can copy it to state.config and replace the access key:\ncp infra/platform/state.config.example infra/platform/state.config vim infra/platform/state.config # Replace secret_key with your MinIO admin password and save You can then init your stored state Terraform project:\nterraform -chdir=infra/platform init -backend-config=state.config Deployment # After having run terraform init for each project, you can then deploy the infrastructure as shown below. Remember that the platform layer is built on top of the foundation layer, so this it is a requirement that is deployed first.\nterraform -chdir=infra/foundation apply -auto-approve terraform -chdir=infra/platform apply -auto-approve Now, if you clone the datalab repo to a different location and run the init command for the platform project, using the proper state.config, you\u0026rsquo;ll be able to access your latest Terraform state. This will let you work from anywhere, as long as you have access to Proxmox‚ÄîI recommend setting up a VPN to your homelab using WireGuard, if you\u0026rsquo;re outside of your local network.\nJustfile Tasks # For your convenience, we provide several top-level just tasks on datalab that you can use, if you forget the commands:\ninfra-config-check ‚Äì check for terraform and the required configs for all infra projects. infra-init ‚Äì run the proper terraform init commands for each project (must be manually run before infra-deploy). infra-deploy ‚Äì deploy each layer of the architecture in sequence (foundation and platform are the only ones supported at this time). infra-show-credentials ‚Äì print all credentials, for each layer, in plain text. Any task in this video series will begin with the prefix infra-, so you can keep a look for these in the upcoming videos.\n","date":"23 September 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-foundation/","section":"","summary":"Summary # On part 2 of this series, you\u0026rsquo;ll learn the basics of Terraform, provisioning a MinIO server as an LXC (Linux Container) running on Proxmox.","title":"Data Lab Infra - Part 2: Bootstrapping with Terraform","type":"posts"},{"content":"","date":"23 September 2025","externalUrl":null,"permalink":"/tags/foundation/","section":"Tags","summary":"","title":"Foundation","type":"tags"},{"content":"","date":"23 September 2025","externalUrl":null,"permalink":"/tags/minio/","section":"Tags","summary":"","title":"Minio","type":"tags"},{"content":"","date":"23 September 2025","externalUrl":null,"permalink":"/tags/secrets-management/","section":"Tags","summary":"","title":"Secrets-Management","type":"tags"},{"content":" Summary # In this video, we\u0026rsquo;ll learn how to design a modern data stack, built for home labs, freelancing data experts, or general on-premise needs.\n\u003e Architecture Overview # We split our infrastructure 4 layers due to the following reasons:\nLayer 1 provides a foundation based on Proxmox and a single LXC running MinIO (or any other S3 object store). We add nothing more to this layer, because we will rely on S3 to track the Terraform state in layer 2, so that we can update or add to our VMs or CTs from any location with access to Proxmox. Layer 2 provides a platform for all the services that we want to run, provisioning three Docker VMs for different needs, and a VM with GitLab, which we will use mostly for configuration management (variables and secrets), as a Docker registry to track custom images we might need to build, and for CI/CD to automate docker compose stack deployment. Layer 3 is concerned with core or shared services, like PostgreSQL or MLflow, which we deploy to the corresponding Docker instance. Layer 4 is concerned with application services, like REST APIs or documentation, which we deploy to the corresponding Docker instance. Here is a diagram for the described 4-layer architecture, with the core services we aim to deploy:\nEach layer from L1 to L4 will live in the datalab repo under the infra/ directory, each on its own subfolder:\nfoundation/ platform/ services/ applications/ We\u0026rsquo;ll have a mirror of the datalab repo on GitLab, exclusively meant for deploying into production‚Äîand it conveniently works as a backup as well. The deployment workflow is run whenever a change in docker-compose.yml files are detected in the main branch, particularly under the services/ and applications/ directories.\ncd datalab/infra/ git switch -c dev # Do work git add \u0026lt;work\u0026gt; git commit -m \u0026#34;work\u0026#34; git push # Once ready for production... git switch main git merge --no-ff dev git push # And then deploy your infra. git remote add prod git@gitlab.lan:DataLabTechTV/datalab.git git push -u prod main L4, the application layer, can also be implemented under project-specific repos, when unrelated to datalab. If service initializations need to be run for a specific app (e.g., issue credentials, or create a database), then a GitLab job will be exposed on the datalab repo that will be callable from any application repo to provision the required resources without exposing any admin credentials.\nLayer 1: Foundation - Bootstrapping # Proxmox LXC: MinIO ID: 101 Name: minio Function: Terraform state storage General-purpose object store Setup using bpg/proxmox provider, which provides more granular configuration options than the Telmate/proxmox provider. Access can be configured either using the root password (not recommended), an API key (limited to API based requests‚Äîwon\u0026rsquo;t fully cover all available features for the provider), or a private key (preferred option for more control).\nLayer 2: Platform - Core Infrastructure # VM: GitLab ID: 201 Name: gitlab Function: Docker registry ‚Äì custom service images, project-specific microservices, etc. CI/CD ‚Äì production configs as variables/secrets, deploy docker compose stacks VM: Docker ID: 202 Name: docker-gitlab Function: GitLab runners VM: Docker ID: 203 Name: docker-shared Function: core services (e.g., PostgreSQL) VM: Docker (project-specific services) ID: 204 Name: docker-apps Function: project-specific services GitLab will be installed and configured using Packer, producing a QCOW2 image to be deployed to Proxmox, under a QEMU VM. This will include the GitLab runners configuration pointing to docker-gitlab.\nA Portainer instance will run under docker-shared, letting us monitor and manage the Docker instances in the three VMs‚Äîdocker-gitlab, docker-shared, and docker-apps. Deployments will be based on one or multiple docker compose stacks. Docker compose will be triggered via CI/CD on a push to main, using variables and secrets stored in GitLab, for a production deployment. For testing, during development, a .env and local Docker instance can be used instead. This also leaves room, when local resources are scarce, for a setup CI/CD where a push to a dev branch will deploy to a separate staging VM.\nLayer 3: Services - Core/Shared Services # PostgreSQL Description: Shared instance for the whole home lab‚Äîeven beyond the data lab‚Äîalso much easier to backup. DuckLake Description: Data Lakehouse Dependencies: PostgreSQL (catalog) MinIO (storage) Apache Kafka Description: Event log MLflow Description: ML model tracking and registry Ollama Description: LLM server Open WebUI Description: ChatGPT like UI for Ollama Dependencies: Ollama A single PostgreSQL instance will be shared among all services. On the first deployment, a root password will be set and stored on the datalab GitLab repo. Application-level deployments will rely on a GitLab\u0026rsquo;s CI/CD task for initializations (e.g., credentials, a database) that will be triggered from any application repo but always run on the datalab GitLab repo, thus keeping admin credentials isolated.\nLayer 4: Applications - Project-Specific Services # Microservices REST APIs Documentation Microservices usually have their own isolated storage layer, but in a lab setting, where resources are scarce, we won\u0026rsquo;t fully respect this constraint. An example of application-specific containers, already in our present context, would be a classifier REST API endpoint. It is at this layer that we will do such deployments.\nUnderstanding Decisions # DevOps # We considered multiple alternative tools\nTerraform vs OpenTofu\nOpenTofu‚Äîit exists because Terraform\u0026rsquo;s license went from MPL 2.0 to BSL 1.1 on August 2023; the community didn\u0026rsquo;t like this restriction and OpenTofu was born. Terraform‚Äîthe BSL license is essentially designed for non-competing; if this doesn\u0026rsquo;t affect you, there\u0026rsquo;s no reason not to go with Terraform. Docker vs Podman\nPodman‚Äîlooks interesting, but requires you to think about podman machine, or manually setup podman.socket to integrate with other tools, like Portainer. Docker‚Äîno reason not to use Docker, but there are a few to not use Podman, so we went with good old Docker. Secrets Management\n.env‚Äînon-committed, a .env.example is provided instead. direnv‚Äîsimilar to .env, but uses a .envrc that is automatically loaded in the shell when you switch to a path inside the root directory. SOPS‚Äîstarted at Mozilla, it\u0026rsquo;s similar to Ansible Vault, but encrypts only the values in structured formats like JSON or YAML; can be committed to a git repo and even diffed. HashiCorp Vault‚Äîbeautiful web app; well-loved by the community; good integration and support all around; there is even a browser extension for it, called VaultPass; but using it represents using yet another service, and adding to the complexity of our infrastructure. GitLab‚Äînot exactly made for secrets management, but it does provide variables and a way to mask and hide them (secrets); as a bonus it provides CI/CD for convenient deployments; this is the overall best choice, and a general-purpose good thing to have. Docker Registry\nDocker Registry UI‚Äîlightweight UI for the registry. Harbor‚Äîfull fledged artifact registry, with policies and role-based access control; supports multiple registries and replication; overkill for our use-case; perhaps a better option when deploying to Kubernetes. GitLab‚Äîalready using it for secrets management, so it\u0026rsquo;s perfect as a Docker Registry as well; a no-brainer. Configuration Management\nAnsible‚Äîhave used it before and know that works well; its approach can be overly complicated, with simple tasks requiring multiple configuration files. Packer‚Äîprioritizes building pre-configured, ready-to-use images, with the added bonus that these can easily be deployed to a cloud platform with very little effort; great skill to have; simpler solution based on HCL, the same language used in Terraform. Data Stack # Portainer\nTo use or not to use? We wanted a minimal stack, but\u0026hellip; This is useful for quick testing or to easily monitor containers, volumes, and images. PostgreSQL\nGreat as a general-purpose relational database, and not only that. Required by GitLab, although we\u0026rsquo;ll probably keep the instances separate (lives in a higher level layers). Can be used as a DuckLake catalog, or to support most apps. DuckLake\nSimplest available solution. Perfect for any scale. Separates catalog, storage, and compute, and compute will be local for us. Catalog can be setup with PostgreSQL, which we already run. Storage can be setup with an S3 object store, and we\u0026rsquo;ve got MinIO. Compute will be done on local machines, in an edge computing fashion, based on DuckDB. Apache Kafka\nWhy include Kafka? Why not a Redis queue or pub/sub? Or even custom gRPC microservices? Having middleware for inter-process communication is always easier to handle than custom gRPC solutions. But why not Redis or other lightweight solutions like ZeroMQ? Well, mostly for the learning experience, and because Kafka might be the single most useful piece of software you could use in production systems, because it can help your app scale up or down as required, and it will help you stay compliant or debug your system, since it provides replayability. A base deployment just requires ~600 MB of RAM anyway. We also thought about Apache Flink for stream processing from Kafka topics, but there is also Faust, a Python-native solution, which we\u0026rsquo;re likely to use later instead. MLflow\nWhen you need open source self-hostable ML model tracking and registration, it\u0026rsquo;s hard to find a better alternative. Other options include Weights \u0026amp; Biases (not fully open source), Neptune.ai (cloud-based service), or Kubeflow (requires Kubernetes, which is overkill for our single-machine home lab). Ollama + Open WebUI\nUsed for self-hosted LLMs. With Open WebUI, we get our own local ChatGPT Or we can use Ollama for LangChain or other implementations. Requires GPU support, which might be hard to setup, as it needs to be configured in the Proxmox host machine, as well as the VM, and possible Docker as well. From On-Premise to Cloud # Let\u0026rsquo;s establish a parallel between on-premise Proxmox based services and their cloud platform alternatives. This will help you understand how to transfer knowledge from working on your home lab into a cloud platform like AWS, GCP, or Azure.\nL1: Foundation # A cloud platform is essentially our foundation layer. These are a given for AWS, GCP, Azure, or any other cloud platform.\nVMs\nGCP Compute Engine Amazon EC2 (Elastic Compute Cloud) Azure Virtual Machines Object Stores\nGoogle Cloud Storage Amazon S3 (Simple Storage Service) Azure Blob Storage L2: Platform # In a cloud platform, L1 and L2 are essentially at the same level, at least for Docker on L2. For the Git, filling the role of our GitLab instance, it depends‚Äîif we\u0026rsquo;re talking secrets management, then each cloud platform has its own dedicated product for it, but, if we\u0026rsquo;re talking code tracking git workflows, then we might look at Git as a service instead.\nDocker\nAWS Fargate GCP Cloud Run Azure Container Instances Git\nAWS CodeCommit GCP Cloud Source Repositories (sunset) GCP Secure Source Manager Azure Repos L3: Services # There are often multiple options per service, depending on your needs. For example, you might want to be able to administer a database yourself, via the command line (e.g., use psql to login to the Postgres root user), or you might want a single database that scales horizontally transparently. Being familiar with the product catalog and knowing how to distinguish between such services is at the root of cloud engineering. Below you\u0026rsquo;ll find a few examples for our use case.\nPostgreSQL\nAmazon RDS Amazon Aurora GCP Cloud SQL GCP AlloyDB Azure Database for PostgreSQL Cosmos DB (not relational, but built on top of PostgreSQL) DuckLake\nEasily integrates with most of the PostgreSQL alternatives, regarding the catalog. And with each of the cloud object store alternatives, regarding storage. Apache Kafka\nAmazon MSK (Managed Streaming for Apache Kafka) Confluent Cloud on GCP, Azure, or AWS MLflow\nCan be deployed on a VM or as Docker a container, on either of the alternatives listed above. It integrates with most of the PostgreSQL alternatives, regarding the backend store. And with each of the cloud object store alternatives, regarding artifact storage. Ollama\nTitan on Amazon Bedrock Gemini on Google Vertex AI Phi on Azure AI Foundry Azure OpenAI Service L4: Applications # Deploying a layer 4 application will depend on the requirements, but for our use cases‚ÄîREST API, microservices, web apps‚Äîit could easily be done using container services, like AWS Fargate, along with storage services, like AWS S3. There\u0026rsquo;s nothing specifically designed on a cloud platform for this, as the cloud platform is itself designed to support your applications.\nFinal Remarks # What a cloud platform offers in simplicity, it also takes in cost. Storage is cheap, and basic services like VMs are accessible as well, but once you go deeper into specialized services, e.g., for AI or highly scalable or available databases, then the cost skyrockets. At that point, you\u0026rsquo;re most likely better off just hiring someone to handle it using traditional infrastructure. Managing your cloud infrastructure cost might be the difference between staying in business or shutting down. Sometimes considering on-premise or cheaper cloud infrastructure, like basic VMs, even outside the major cloud providers, can be a cheaper and even better solution for your use case.\n","date":"16 September 2025","externalUrl":null,"permalink":"/posts/data-lab-infra-architecture/","section":"","summary":"Summary # In this video, we\u0026rsquo;ll learn how to design a modern data stack, built for home labs, freelancing data experts, or general on-premise needs.","title":"Data Lab Infra - Part 1: Architecture Design","type":"posts"},{"content":"","date":"27 August 2025","externalUrl":null,"permalink":"/tags/ab-testing/","section":"Tags","summary":"","title":"Ab-Testing","type":"tags"},{"content":" Summary # Learn how to implement an end-to-end machine learning workflow, from data ingestion, to A/B testing and monitoring, using MLflow, Kafka and DuckLake. We\u0026rsquo;ll discuss model training and tracking with MLflow, real-world deployment approaches, how to track inference results and feedback in your data lakehouse using Kafka and DuckLake, and how to compute monitoring statistics such as prediction drift, feature drift, or estimated performance, for unlabeled data, as well as taking advantage of user feedback to estimate model error. This should give you the tools you need to build and maintain your own ML lifecycle.\n\u003e Organizational Roles # I\u0026rsquo;ll start with a bit of a comment on organizational roles in a data team, and their responsibilities. This will help clarify a design choice I made in the architecture diagram below.\nWe\u0026rsquo;re considering a team with a Data Engineer, a Data Scientist, and a ML Engineer. As most ops roles, the ML Engineer has responsibilities that overlap with other areas covered by other roles, but these usually have a distinct focus when compared to those other roles.\nFor example, both a Data Scientist and a ML Engineer will work with model training and evaluation in some capacity, but the Data Scientist is usually more interested in picking the right algorithm, hyperparameters and features for training, or the right evaluation metrics for the problem at hand, while the ML Engineer is concerned with making sure that these processes can be tracked in a way that models are properly registered and information is easily available to make a decision on the best model and its staging or production readiness.\nSimilarly, the Data Engineer also overlaps with the ML Engineer, particularly regarding the data quality aspects of the training and test datasets.\nArchitecture # In the diagram below, we highlight the Data Engineering, Data Science, and ML Engineering responsibilities. In blue, you\u0026rsquo;ll be able to track the training flow, in yellow the evaluation flow, and in green the inference flow. The middle layer is an overview on the data schemas for DuckLake tables and Kafka topics. On the bottom, you\u0026rsquo;ll find the service layer, with the REST API, and the Kafka producers and consumers.\nIn our implementation, FastAPI was responsible for initializing the Kafka producers and consumers, but in a real-world scenario this would be done separately, each running on their own container.\nWhy Kafka and not something else, like gRPC or Redis Pub/Sub? Kafka acts as an event log, so it provides topic replayability, which is great for compliance, but also for debugging. It can also front-face your product in a way that more brokers can be deployed in groups, handling partitioned messages to the same topic, which is a great way to scale up and down as required. The trade-off is that a controller and a single broker will require ~600 MiB of RAM to run, which is usually acceptable, but, depending on your priorities, it might be too much for basic RPC or queuing, and this will only provide replayability as an advantage, but not partitioning or the reliability of multiple brokers.\nOrchestration # Previously, we hadn\u0026rsquo;t implemented any approach for orchestration on datalab, but with the new ML package, this gained more relevance, so we opted to use just for handling tasks via the command line. This is yet another great tool built in Rust, that mimics the make command, but it\u0026rsquo;s specifically designed for running tasks, as a opposed to building files. Implementing tasks in a Makefile usually requires that those targets are added as dependencies of the .PHONY target, so that make knows they won\u0026rsquo;t produce a file. This would avoid that a task with a name matching an existing file would be skipped due to the file existing and being up-to-date. Instead, when using a justfile, all targets are essentially .PHONY. In addition, since just is a tool specifically designed to run tasks, it also provides several utilities that simplify your life. For example, preloading your existing .env is as easy as adding:\nset dotenv-load And while make will run using /bin/sh by default, just will use your login shell by default. Additionally, just also supports positional parameters with default values, and it provides a simple way to list all tasks and their parameters:\njust -l A good way to setup your justfile is to use this as your first task:\ndefault: just -l The first task is the one that\u0026rsquo;s run by default when invoking just without any arguments. And yes, just can call just from other tasks. For example, we do this:\ncheck-init-sql: test -r {{init_sql_path}} || just generate-init-sql It also distinguishes internal variables from environment variables:\nengine_db_path := join(local_dir, env_var(\u0026#34;ENGINE_DB\u0026#34;)) check-engine-db: test -r {{engine_db_path}} So, if your orchestration needs don\u0026rsquo;t require something like Apache Airflow or Prefect, and make or shell scripting don\u0026rsquo;t quite do it, you should consider just using just.\nTraining and Tracking # As a way to test our workflow, we decided to train a model to classify user-generated text as depression or not depression. For training and testing, we used the ShreyaR/DepressionDetection dataset from Hugging Face. This was ingested and transformed as usual, using our dbt and DuckLake workflow.\nWe then trained four models, combining two algorithms and two feature sets:\nLogistic Regression + TF-IDF Logistic Regression + Embeddings (all-MiniLM-L6-v2) XGBoost + TF-IDF XGBoost + Embeddings (all-MiniLM-L6-v2) For each model, we tracked the following metadata and artifacts on MLflow:\nMetadata Data source (dataset tags) Algorithm/method used Features used Hyperparameter grid Cross-validation config Evaluation metrics (validation and testing) Input datasets (schema and shape) Artifacts Serialized model with input/output signature Python dependencies We used the following two helper functions, at the beginning and end of our training function, which helped to keep the training code clean.\nAt the beginning of a run (training starting), we point to the correct MLflow server, create the experiment, if it doesn\u0026rsquo;t exist, and start a run with a name based on the algorithm/method and feature set. Then, we use mlflow.set_tags to track metadata, including dataset based metadata, which is not yet visible in the current version of the MLflow UI when associated with a logged dataset directly. We also use mlflow.log_inputs to log the schema and size of the training and test sets. See next:\ndef mlflow_start_run( experiment_name: str, run_name: str, tags: dict[str, Any], datasets: list[Dataset], dataset_tags: dict[str, Any], ): tracking_uri = env.str(\u0026#34;MLFLOW_TRACKING_URI\u0026#34;) mlflow.set_tracking_uri(tracking_uri) log.info(\u0026#34;MLflow tracking URI: {}\u0026#34;, tracking_uri) mlflow.set_experiment(experiment_name) log.info(\u0026#34;MLflow experiment: {}\u0026#34;, experiment_name) mlflow.start_run(run_name=run_name) log.info(\u0026#34;MLflow run: {}\u0026#34;, run_name) log.info(\u0026#34;MLflow: logging tags and input datasets\u0026#34;) mlflow.set_tags(tags | dataset_tags) contexts = [ds.name for ds in datasets] tags_list = [dataset_tags for _ in datasets] mlflow.log_inputs( datasets=datasets, contexts=contexts, tags_list=tags_list ) At the end of a run (training finished), we log the Python requirements, the serialized model, the best parameters from cross-validation (CV), and the metrics from CV and test set evaluation. See next:\ndef mlflow_end_run( model_name: str, model: Pipeline, params: dict[str, Any] | None = None, metrics: dict[str, Any] | None = None, train: pd.DataFrame | None = None, ): signature = None if train is not None: log.info(\u0026#34;MLflow: inferring model signature\u0026#34;) model_output = model.predict(train.input) signature = infer_signature(train.input, model_output) log.info(\u0026#34;Extracting pip requirements from uv\u0026#34;) pyproject = tomllib.load(open(\u0026#34;pyproject.toml\u0026#34;, \u0026#34;rb\u0026#34;)) requirements = ( pyproject .get(\u0026#34;project\u0026#34;, {}) .get(\u0026#34;dependencies\u0026#34;, {}) ) log.info(\u0026#34;MLflow: logging model\u0026#34;) mlflow.sklearn.log_model( sk_model=model, name=model_name, registered_model_name=model_name, signature=signature, pip_requirements=requirements, ) if params is not None: log.info(\u0026#34;MLflow: logging parameters\u0026#34;) mlflow.log_params(params) if metrics is not None: log.info(\u0026#34;MLflow: logging metrics\u0026#34;) mlflow.log_metrics(metrics) mlflow.end_run() MLflow Artifact Storage # MLflow uses a database, such as SQLite or PostgreSQL, as its backend store to log metadata. However, for artifacts, it can either log them locally (mostly just for testing), directly on S3, or by acting as an artifact proxy, transparently pointing to either a local or S3 backed location. Here\u0026rsquo;s an overview on how to setup these two options, when starting MLflow:\nDirect S3 Storage Set --default-artifacts-root to an S3 path (e.g., s3://mlflow/artifacts). Preferred when each user has its own AWS/S3 credentials. Artifact Proxy Set --serve-artifacts. Set --artifacts-destination to a local or S3 path. Artifacts will be stored under the default root of mlflow-artifacts:/. Preferred when users have no direct interaction with AWS/S3, or to simplify model serving (no need to create S3 credentials for inference services). Please see our docker-compose.yml file for details on setting up MLflow as an artifact proxy, with SQLite for the backend store. And also checkout the Common Setups section on the official documentation, where you\u0026rsquo;ll be able to find a diagram of the three most common MLflow deployment options.\nEvaluation and Inferencing # We used a train/test split of 80/20, and then split our training set into 3-folds for validation. Our models were trained and optimized using cross-validation and F1 scoring over a minimal hyperparameter grid. For each model, we provided the accuracy and F1 score for the best fold, as well as for the test set. These were logged into MLflow, as described above.\nOut of the four models, we selected the best model, which was XGBoost + Embeddings, along with the most different model, Logistic Regression + TF-IDF, and served these models through a unique endpoint, randomly selecting one of them per request, for A/B testing.\nEach inference was assigned a UUID and a creation date, and then sent to a Kafka topic. A topic consumer buffered these requests until hitting a large enough batch or a given timeout, at which point they were flushed into a DuckLake catalog with encrypted storage (remember, this is user data). An example of how to call this endpoint is provided in the justfile:\nmlops_test_inference_payload := \u0026#39;\u0026#39;\u0026#39; { \u0026#34;models\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;dd_logreg_tfidf\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;dd_xgboost_embeddings\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; } ], \u0026#34;data\u0026#34;: \u0026#34;...bc i have depression how are you all d\u0026#34;, \u0026#34;log_to_lakehouse\u0026#34;: true } \u0026#39;\u0026#39;\u0026#39; mlops-test-inference: check-curl curl -f -X POST \u0026#34;http://localhost:8000/inference\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{{mlops_test_inference_payload}}\u0026#39; @echo curl -f -X GET \u0026#34;http://localhost:8000/inference/logs/flush\u0026#34; We also provided an API endpoint where users were able to submit a feedback score (1.0 or 0.0 for our depression classifier), based on the inference UUID. Multiple requests with the same inference UUID appended the feedback to the feedback DOUBLE[] column.\nDeployment # Below, we cover two approaches for model deployment:\nUsing MLflow to build a Docker image per model. Building your own custom FastAPI REST endpoint. The MLflow approach provides a more straightforward and standard way to deploy, but each model requires its own separate deployment, and you can\u0026rsquo;t control the output‚Äîfor example, if you want to return the output from predict_proba for a scikit-learn model, instead of using predict, then you\u0026rsquo;d need to code that into your model\u0026rsquo;s prediction function before logging it to MLflow.\nThe custom approach provides more flexibility and potential for optimization‚Äîfor example, we can share resources for multiple models, or build a custom environment with minimal dependencies. Moreover, we can still create our own Docker image for a custom REST endpoint. This is the preferred approach by seasoned ML Engineers, and this is what we do here as well, even more so since our inference results return probabilities and need to be logged to DuckLake via Kafka.\nMLflow Models # While MLflow provides an out-of-the-box approach to deploy logged models, the Docker images it produces tends to be huge by default (~25 GB for the datalab project). You\u0026rsquo;ll need to properly determine the minimum required dependencies when logging your model, if you want to make sure that the resulting image size is optimized. Even then, this deployment approach will produce individual images per model, which is good for isolation, but harder on resource management and cost. Instead, you might want to build your own custom solution for deployment, which we will describe in the following section. Meanwhile, here\u0026rsquo;s an example on how to use the MLflow\u0026rsquo;s deployment workflow.\nIn order to create a Docker image for your model, you can simply run the following command, providing it a model URI and the target image name:\nmlflow models build-docker \\ -m \u0026#34;models:/dd_logreg_tfidf/latest\u0026#34; \\ -n \u0026#34;mlflow_model_dd_logreg_tfidf\u0026#34; You can then run a container based on the produced image, optionally limiting the number of workers (which defaults to all CPU cores) by setting MLFLOW_MODEL_WORKERS:\ndocker run -d \\ -p 5001:8080 \\ -e MLFLOW_MODELS_WORKERS=4 \\ \u0026#34;mlflow_model_dd_logreg_tfidf\u0026#34; In order to classify a batch of examples for a model that takes a DataFrame with a single column input, you can POST JSON to the /invocations path:\ncurl -X POST \u0026#34;http://localhost:5001/invocations\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;dataframe_split\u0026#34;: { \u0026#34;columns\u0026#34;: [\u0026#34;input\u0026#34;], \u0026#34;data\u0026#34;: [ [\u0026#34;...bc i have depression how are you all d\u0026#34;], [\u0026#34;...we were really so lucky...\u0026#34;] ] } }\u0026#39; Custom API # Like we said, in practice, it\u0026rsquo;s more common to deploy models using a custom REST API, usually created with FastAPI, often chosen due to its self-documenting approach via Swagger UI and ReDoc. This also lets you produce your own Docker image, optimizing it as much as you\u0026rsquo;d like, and you can also serve multiple models using the same API, which you cannot do with the MLflow approach.\nImagine a 25 GiB image for each of your classifiers, the time it would take to build and deploy, and the resources it would require! This is why most ML Engineers create their own web services to deploy their models.\nOur API provides the following two main endpoints, which encapsulate the whole workflow logic.\nPOST /inference to INSERT inference results:\n@app.post(\u0026#34;/inference\u0026#34;) async def inference( inference_request: InferenceRequest, request: Request ): try: inference_result = predict(inference_request) except ModelNotFound: return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Model not found\u0026#34;}, status_code=status.HTTP_404_NOT_FOUND, ) if inference_request.log_to_lakehouse: log.info(\u0026#34;Queuing lakehouse insertion for inference result\u0026#34;) await queue_inference_result( request.app.state.inference_result_producer, inference_result, ) return inference_result PATCH /inference to UPDATE an inference result by appending to its feedback:\n@app.patch(\u0026#34;/inference\u0026#34;) async def inference( inference_feedback: InferenceFeedback, request: Request ): log.info(\u0026#34;Queuing lakehouse append for inference feedback\u0026#34;) await queue_inference_feedback( request.app.state.inference_feedback_producer, inference_feedback, ) return Response(status_code=status.HTTP_204_NO_CONTENT) Monitoring Strategy # During monitoring, we compare reference data with the current data. Our reference data is always based on the training set (e.g., prediction labels or features based on the training set). Our current data, on the other hand, is based on a sliding window of 7 days, that we compute per day.\nBelow, we will describe the metrics that we considered, along with our particular implementation, which you can find in the datalab codebase, under ml.monitor.\nPrediction Drift # Prediction drift, or concept drift, is concerned with comparing the reference and current prediction probability distributions.\nWhile there are multiple possible statistical approaches to compare these distributions, we use a Kolmogorov‚ÄìSmirnov test, particularly looking at the D-statistic, which illustrates the largest possible gap between the two distributions. The higher the D-statistic, the higher the prediction drift.\nFeature Drift # Feature drift, or data drift, is usually concerned with comparing the distributions of individual features. Drift scores for individual features can be aggregated into a global feature drift score, but feature drift can also be computed over all the feature set directly. Below, we briefly explain both approaches, per feature or dataset.\nPer Feature # We start by comparing each feature individually for the reference and current datasets. For example, if our features are text, we might compare term distributions or the distributions of TF-IDF over all examples. If we\u0026rsquo;re working with embeddings, it\u0026rsquo;s similar‚Äîassuming rows are examples and columns are features, we simply compare equivalent columns on the reference and current data.\nAgain, there are several statistical approaches that we can use here, but let\u0026rsquo;s assume we\u0026rsquo;re again working with a Kolmogorov‚ÄìSmirnov test, which provides the D-statistic, as well as the p-value.\nOnce we compare all pairs of features, we\u0026rsquo;ll end up with a 1D array with a score per feature. If this score is the D-statistic, it\u0026rsquo;s common for us to take the median, which is less sensitive to outliers, and compute an aggregate feature drift score. Another, less informative, approach is to count the number of p-values \u0026lt; 0.05 and return that fraction, which just tells us how many features drifted significantly, thus imputing drift to the overall data.\nThis is not our implementation here. Instead, we relied on a per dataset feature drift, as described next.\nPer Dataset # For this approach, all features are used simultaneously. A simple way to achieve this is by concatenating reference and current data, assigning each subset its own label:\nReference ‚Üí 0 Current ‚Üí 1 Keep in mind that these are not the labels in your original data, so, for example, our depression training set contained label 0 for not depression and label 1 for depression, but here the whole dataset will have label 0, regardless of whether we\u0026rsquo;re considering a negative or positive example of depression. Now, instead, we\u0026rsquo;re trying to distinguish between examples in reference or current.\nAfter training a simple classifier (e.g., logistic regression) with the described dataset, we evaluate using ROC AUC.\nIf AUC ~ 0.5, the classifier is no better than random guessing, so it cannot distinguish between the datasets, which means there is no feature drift. If AUC \u0026gt; 0.5, the classifier is able to distinguish both datasets, so they are not the same and there is as much drift as the AUC. If AUC \u0026lt; 0.5, the interpretation is similar to that of AUC \u0026gt; 0.5 in regards to drift, but it also tells us that the classifier is predicting in reverse (this should never happen). Estimated Performance # Inspired by NannyML, which was incompatible with our remaining dependencies, we implemented confidence-based performance estimation (CBPE).\nFor the model we want to test (e.g., logistic regression with TF-IDF features), we train an isotonic regression model to predict correctness based on the output probabilities of the model we\u0026rsquo;re testing. The isotonic regressor is trained over a dataset with a single feature in the format:\n$\\text{pred_prob} \\mapsto (\\text{pred_label} = \\text{correct_label})$\nWhere $\\text{pred_prob}$ is the output of predict_proba for the model we\u0026rsquo;re testing, over examples in the original training set, $\\text{pred_label}$ is the corresponding binary output after applying the decision threshold (e.g., $\\text{pred_label} \\leftarrow \\text{pred_prob} \\ge 0.5$), and $\\text{correct_label}$ is the target on the original training set, used to train the model we\u0026rsquo;re testing.\nOnce we have the isotonic regressor (one per model to test), we\u0026rsquo;re be able to predict the inference correctness for new unseen examples, and, from that, we\u0026rsquo;re then able to obtain true and false positives and negatives, and calculate evaluation metrics like accuracy or F1.\nUser Feedback # Finally, based on user feedback, we\u0026rsquo;re also able to a calculate Brier score per inference result, comparing our binary prediction to a probabilistic user feedback score. We can then average the Brier scores to obtain a global error-like metric (notice the similarity with mean-squared error, MSE):\n$\\displaystyle\\text{Avg}(BS) = \\frac{1}{n}\\sum_{i=1}^n\\left(\\text{prediction}_i - \\frac{1}{m}\\sum_{j=1}^m\\text{feedback}_{ij}\\right)$\nWhere:\n$\\text{prediction}_i \\in \\{0, 1\\}$ $\\text{feedback}_{ij} \\in [0, 1]$. Similarly to MSE, a lower average Brier score is better:\n0 is a perfect calibration. 1 is the worst possible outcome. Extending Observability # Other elements that we might track to extend observability would be:\nData Quality Shared responsibility with data engineering. Rule-based validations (e.g., check for all zeros in predictions, or missing values). For example, expressed as an aggregate score of rule compliance. Model Explainability SHAP (SHapley Additive exPlanations) LIME (Local Interpretable Model-agnostic Explanations) Simulating Inference and Feedback # Since we do not have a production system, we simulate inference requests and user feedback, in order to obtain credible data to test our monitoring statistics with. We use a dataset external to our train/test set, also from Hugging Face, to help with this task: joangaes/depression. We call this our monitor set. This follows a similar structure to the train/test set, with a text column and a label column for depression or not depression.\nUsing this data, we simulate inference and feedback assignment in multiple passes (default 3) over the monitor set, where we:\nSample a fraction of the dataset (optional). Run inference over all examples using a given decision threshold (default 0.5). Log results to the data lakehouse with a random date (default range of 4 weeks). Provide feedback for a fraction (default range of $[0.45, 0.55]$) of those results\u0026hellip; \u0026hellip;simulating wrong feedback by using the complement over a fraction of feedback outputs (default range of $[0.10, 0.25]$) of the considered results. Simulated data should contain mostly correct feedback, with a few errors, simulating a real-world scenario. Inferences might have no feedback, or any number of feedback scores up to the number of passes, i.e., 0 to 3 feedback scores.\nA/B Testing Results # Based on the simulated inference results and user feedback, we now look at and interpret the monitoring statistics.\nInferences Over Time # We track the number of inferences ran per day, over the simulated period of one month. This also shows how many times each model was randomly selected per day. Volume is higher for mid-August and although model selection is uniform, the logistic regression model using TF-IDF features seems to have been selected more frequently when looking at daily aggregations.\nPrediction Drift # Also known as concept drift, we measure prediction drift based on the D-statistic from the Kolmogorov‚ÄìSmirnov test. The lower the D-statistic, the less prediction drift there is. As we can see, prediction drift is reaching critical levels for the logistic regression model, while being much lower for the XGBoost model, despite still showing a considerable magnitude. However, the logistic regression is particularly concerning, as the D-statistic is not only high, but also extremely consistent over time, indicating an issue with that model, beyond just optimization.\nFeature Drift # Also known as data drift, feature drift is measured based on ROC AUC. As such, we are looking for values of ~0.5, indicating no feature drift. As we can see, there is also considerable feature drift for both models. This indicates that the training data was not representative of incoming examples for inference. It might also be the case that data preprocessing requires a few normalization steps to make sure the text is formatted similarly across reference and current data. For our models, we\u0026rsquo;re probably suffering from both problems, as we didn\u0026rsquo;t put much effort into the preprocessing stage. This goes to show the importance of good data engineering.\nEstimated Performance # While previous metrics indicate that our models are not properly calibrated, we\u0026rsquo;re still predicting evaluation scores as high as the ones obtained during testing. On one side, this matches expected behavior, on the other side we know that we have other problems to solve, before we can trust this score overall.\nUser Feedback # Finally, based on user feedback, we\u0026rsquo;re obtaining inconsistent average Brier scores over time, most likely due to the small monitor set sample that we used. However, overall, the lowest (best) values seem to be for the XGBoost model, indicating we might obtain better results by investing on that model, but we\u0026rsquo;d need more information.\nFinal Remarks # Hopefully, this is enough to get you started on MLOps and ML Engineering, providing a comprehensible example of an end-to-end workflow and the types of tasks we\u0026rsquo;re expected to work on as an ML Engineer. In future videos and blog posts, I expect to go deeper into the infrastructure side of Data Engineering and MLOps, moving into model deployment, and real-time model monitoring.\n","date":"27 August 2025","externalUrl":null,"permalink":"/posts/mlops-ab-testing/","section":"","summary":"Summary # Learn how to implement an end-to-end machine learning workflow, from data ingestion, to A/B testing and monitoring, using MLflow, Kafka and DuckLake.","title":"MLOps: A/B Testing with MLflow, Kafka, and DuckLake","type":"posts"},{"content":"","date":"27 August 2025","externalUrl":null,"permalink":"/tags/monitoring/","section":"Tags","summary":"","title":"Monitoring","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/categories/data-science/","section":"Categories","summary":"","title":"Data Science","type":"categories"},{"content":" Summary # In this video, we reproduce the approach that predicts Survivor winners and apply it to Economic Competition Networks to better understand world trade and economic leaders. We build a country to country competition network based on the Export Similarity Index (ESI), and we use several techniques from network science, like PageRank, community detection, weak component analysis, or the recent common out-neighbor (CON) score, to better understand how countries compete with each other within the world economy, identifying dominating or leading economies, as well as their counterpart weaker or smaller economies.\n\u003e Dataset # We use The Atlas of Economic Complexity dataset, which is summarized in the following table. We only provide a top-level overview of the data here. For an in-depth detailed description, click the Download button in each table row of the link above‚Äîthat will open a popup with detailed information on the fields for each CSV file.\nTitle Description Complexity Rankings \u0026amp; Growth Projections Economic Complexity Index (ECI) and partial growth projections for world economies from 1995 to 2023. Country Trade by Product Exports and imports, per country and product, over the years. Different files provide a different product category granularity based on the number of HS92, HS12 or SITC digits. Different files are also provided for services, using a non-standard classification internal to Growth Labs that also provides different digit-based granularities. Total Trade by Country Total exports and imports, per country, over the years. Different files provide data about products and services.\nHow big is the economy for a country?\nHow did it progress over the last 28 years? Total Trade by Product Total exports and imports, per product, over the years. Again, this is provided at different product granularities based on HS92, HS12 or SITC digits. Different files are also provided for services, using a non-standard classification internal to Growth Labs that also provides different digit-based granularities.\nHow big is the market for a product?\nHow did it progress over the last 28 years? Country Trade by Partner Bilateral exports and imports between pairs of countries, over the years. ‚úÖ Country Trade by Partner and Product Bilateral exports and imports between pairs of countries, for a given product, over the years. This is provided at 6-digit granularity based on HS92, HS12 or SITC digits. This is partitioned into multiple files in blocks of 10 years (or 5 years only for 1995-1999).\nA granularity of 4 digits would be enough to distinguish between main product types (e.g., beef vs pork vs poultry, fresh vs frozen; gasoline engines vs diesel engines). With 6 digits we get a lot more detail (e.g., carcasses and half-carcasses of bovine animals, fresh or chilled; engines for aircraft). We use the HS92 data with 6 digits‚Äîthe only one available, but also ideal to capture trade competition between countries, as true competition is only uncovered at a smaller scale. We only look at the 2020-2023 period, for recency, aggregating totals for those three years. ‚úÖ Country Classification Country metadata. Regional Classification Regional classification for countries‚Äîcontinent it belongs to, political region (e.g., European Union), subregion (e.g., Central America, Western Africa), trade regions (e.g., NAFTA, OPEC), etc. HS12 Product Classification Product metadata according to HS12 codes. ‚úÖ HS92 Product Classification Product metadata according to HS92 codes.\nWe use this to inspect products traded by salient countries during the analysis. Services Product Classification Services metadata according to a non-standard classification internal to Growth Labs.\nWe use this to inspect services traded by salient countries during the analysis. SITC Product Classification Product metadata according to SITC codes. Product Space Related Edges HS92 4-digit codes for source and target products in the same space (e.g., women\u0026rsquo;s coats ‚áÑ sweaters). Product Space Layout HS92 4-digit codes for products along with their 2D embedding, where close products are co-exported by countries. Here are the citations for the datasets that we use:\nCountry Trade by Partner and Product:\nThe Growth Lab at Harvard University, 2025, \u0026ldquo;International Trade Data (HS92)\u0026rdquo;, https://doi.org/10.7910/DVN/T4CHWJ, Harvard Dataverse\nCountry Classification \u0026amp; HS92 Product Classification:\nThe Growth Lab at Harvard University, 2025, \u0026ldquo;Classifications Data\u0026rdquo;, https://doi.org/10.7910/DVN/3BAL1O, Harvard Dataverse\nGraph Schema # Out of the three CSV files that we identified above as being used, we produce the following nodes and relationship labels:\nNodes Country node_id ‚Äì globally unique node identifier ‚Äì INT64 Properties from all Country Classification columns Product node_id ‚Äì globally unique node identifier ‚Äì INT64 Properties from all HS92 Product Classification columns Relationships (:Country)-[:CompetesWith]-\u0026gt;(:Country) ESI ‚Äì Export Similarity Index ‚Äì DOUBLE (:Country)-[:Exports]-\u0026gt;(:Product) amount_usd ‚Äì exports dollar amount (2020-2023) ‚Äì INT128 (:Country)\u0026lt;-[:Imports]-\u0026gt;(:Product) amount_usd ‚Äì imports dollar amount (2020-2023) ‚Äì INT128 Take a look at the following diagram, where rectangles represent the raw CSV files, with dashed arrows illustrating the data source, and circles represent nodes, with solid arrows representing relationships.\nJupyter Notebook # The following sections are an adaptation of the Jupyter Notebook that we created to analyze the Economic Competition Network.\nSetup # ETL # For ETL, we directly call the appropriate dlctl commands for:\nIngesting the dataset Transforming using SQL on top of DuckLake Exporting from the data lakehouse into Parquet Loading the graph into Kuzu Computing general analytics scores Be sure to uncomment the cell below and run it once.\n!dlctl ingest dataset -t atlas \\ \u0026#34;The Atlas of Economic Complexity\u0026#34; !dlctl transform -m +marts.graphs.econ_comp !dlctl export dataset graphs econ_comp !dlctl graph load econ_comp !dlctl graph compute con-score econ_comp Country CompetesWith Imports # from pathlib import Path from string import Template from textwrap import dedent from typing import Any, Literal, Optional import kuzu import matplotlib.pyplot as plt import networkx as nx import numpy as np import pandas as pd from scipy.special import expit import graph.visualization as vis from shared.settings import LOCAL_DIR, env Globals # We setup access to the appropriate Kuzu path, based on the shared .env configuration, ensuring the graph exists before running the notebook. Once setup, conn will be used to query the graph directly throughout this notebook.\ndb_path = Path(LOCAL_DIR) / env.str(\u0026#34;ECON_COMP_GRAPH_DB\u0026#34;) assert db_path.exists(), \\ \u0026#34;You need to create the graph DB using dlctl first\u0026#34; db = kuzu.Database(db_path) conn = kuzu.Connection(db) Constants # In order to ensure color consistency for our plots, we extract the color palette from matplotlib into MPL_PALETTE.\nMPL_PALETTE = ( plt.rcParams[\u0026#34;axes.prop_cycle\u0026#34;] .by_key()[\u0026#34;color\u0026#34;] ) We also map a display attribute for each of our note labels, Country and Product. We\u0026rsquo;ll use the short names for both when plotting graph visualizations or related charts.\nLABEL_PROPS = { \u0026#34;Country\u0026#34;: \u0026#34;country_name_short\u0026#34;, \u0026#34;Product\u0026#34;: \u0026#34;product_name_short\u0026#34;, } Functions # We create a few reusable functions, where we run Kuzu queries. In a few cases, it was helpful to debug the query with parameters (e.g., using Kuzu Explorer), so we created a helper function for this (note that this doesn\u0026rsquo;t support string parameters, as we didn\u0026rsquo;t ned them).\ndef print_query(query: str, params: dict[str, Any]): dbg_query = dedent(query).strip() dbg_query = Template(dbg_query) dbg_query = dbg_query.substitute(params) print(dbg_query) We\u0026rsquo;ll also cluster nodes using different strategies and compare groups, so we implement a basic Jaccard similarity function.\ndef jaccard_sim(a: pd.Series, b: pd.Series) -\u0026gt; float: a = set(a) b = set(b) return len(a \u0026amp; b) / len(a | b) We might want to look at the top x% of traded products, based ona USD. The following function will help filter this.\ndef top_frac(df: pd.DataFrame, col: str, frac: float = 0.25): mask = (df[col] / df[col].sum()).cumsum() \u0026lt;= frac return df[mask] Analysis # We focus on the CompetesWith projection, a relationship given by the Export Similarity Index (ESI). Our graph analysis includes:\nDynamic competition analysis. Dominating and weaker economy identification, based on the CON score for each country. Trade basket overlap analysis for top and bottom economies. Competition network analysis. Community analysis, including community mapping, top traded product identification, and trade alignment study (self-sufficiency, external competitiveness). Weak component analysis, following a similar approach to the community analysis‚Äîweak components widen community reach. Community and weak component comparison. Economic pressure analysis. Dynamic Competition Analysis # Top 10 Dominating Economies # These are highly spread economies, able to compete with several other countries, i.e., with a high number of common out-neighbors (CON).\ndom_econ_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) RETURN c, c.node_id AS node_id, c.country_name_short AS country ORDER BY c.con_score DESC LIMIT 10 \u0026#34;\u0026#34;\u0026#34; ).get_as_df()[[\u0026#34;node_id\u0026#34;, \u0026#34;country\u0026#34;]] dom_econ_df.index = pd.RangeIndex( start=1, stop=len(dom_econ_df) + 1, name=\u0026#34;rank\u0026#34; ) dom_econ_df node_id country rank 1 206 United States of America 2 55 Canada 3 34 United Arab Emirates 4 107 Netherlands 5 132 United Kingdom 6 175 Belgium 7 134 Italy 8 223 Spain 9 131 France 10 145 Thailand Top 3 Exports # Looking at the top exports will help contextualize these economies. We only look at the top 3 products, to keep the visualization clean and readable.\ndom_econ_g = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WITH c ORDER BY c.con_score DESC LIMIT 10 MATCH (c)-[e:Exports]-\u0026gt;(p:Product) MATCH (c2:Country)-[:Exports]-\u0026gt;(p) WITH c, e, p, count(DISTINCT c2) AS exporters WHERE exporters \u0026gt; 1 WITH c, e, p ORDER BY c.node_id, e.amount_usd DESC SKIP 0 WITH c, collect({p: p, e: e}) AS export_list UNWIND list_slice(export_list, 0, 3) AS r RETURN c, r.e, r.p ORDER BY c.node_id, r.p.node_id \u0026#34;\u0026#34;\u0026#34; ).get_as_networkx() vis.set_labels(dom_econ_g, LABEL_PROPS) vis.plot(dom_econ_g, scale=1.25, seed=3) Bottom 10 Weaker Economies # These are smaller or weaker economies, in the sense that they have a lower competition power. We also find the Undeclared special country node at rank 1, showing that only a small number of products are undeclared worldwide.\nweak_econ_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) RETURN c, c.node_id AS node_id, c.country_name_short AS country ORDER BY c.con_score ASC LIMIT 10 \u0026#34;\u0026#34;\u0026#34; ).get_as_df()[[\u0026#34;node_id\u0026#34;, \u0026#34;country\u0026#34;]] weak_econ_df.index = pd.RangeIndex( start=1, stop=len(weak_econ_df) + 1, name=\u0026#34;rank\u0026#34; ) weak_econ_df node_id country rank 1 193 Undeclared 2 72 Bouvet Island 3 170 Wallis and Futuna 4 106 Norfolk Island 5 167 Saint Pierre and Miquelon 6 216 Niue 7 66 South Georgia and South Sandwich Islds. 8 121 Northern Mariana Islands 9 161 Heard and McDonald Islands 10 100 Western Sahara Top 3 Exports # If we look at the top 3 exports for each competing country in the bottom of the ranking according CON scores, as expected we find that these are more disconnected economies, mostly focusing on raw materials, or components and machinery.\nweak_econ_g = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WITH c ORDER BY c.con_score ASC LIMIT 10 MATCH (c)-[e:Exports]-\u0026gt;(p:Product) MATCH (c2:Country)-[:Exports]-\u0026gt;(p) WITH c, e, p, count(DISTINCT c2) AS exporters WHERE exporters \u0026gt; 1 WITH c, e, p ORDER BY c.node_id, e.amount_usd DESC SKIP 0 WITH c, collect({p: p, e: e}) AS export_list UNWIND list_slice(export_list, 0, 3) AS r RETURN c, r.e, r.p ORDER BY c.node_id, r.p.node_id \u0026#34;\u0026#34;\u0026#34; ).get_as_networkx() vis.set_labels(weak_econ_g, LABEL_PROPS) vis.plot(weak_econ_g, scale=1.25, seed=3) Dominating vs Weaker Economies # Do dominating economies compete in the same markets as weaker economies? If so, maybe that\u0026rsquo;s why those weaker economies are being pushed to the bottom. ‚òëÔ∏è If not, maybe the products exported by those weaker economies are not the most competitive. Here, we find that, due to the small export diversity, weaker economies are being crushed by dominating economies. Their position of vulnerability comes mostly from geographical isolation and limited area, leading to a lower amount of competition opportunities, where any competitor becomes a risk to the economy.\nBelow, country node classes are visually translated to a colored node border and label text. We assign two classes, for the top and bottom 10 economies, with top economies in the center, and the products and bottom economies in the surrounding area. This forms a star layout, where each arm is a weaker economy or a small cluster of weaker economies.\nWe look at the top 3 most exported products in weaker economies, but relaxing the filter on number of exported products for the weaker economies and looking at more than 3 exported products will reproduce the displayed behavior, with dominating economies still competing for the same products. This doesn\u0026rsquo;t necessarily mean that both dominating and weaker economies produce the same products, as some of them can simply be re-exported.\ndom_vs_weak_econ_g = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (wea)-[we:Exports]-\u0026gt;(p:Product) MATCH (dom)-[de:Exports]-\u0026gt;(p) WHERE dom.node_id IN $dominating_node_ids AND wea.node_id IN $weaker_node_ids WITH wea, we, p, count(DISTINCT dom) AS dom_competitors WHERE dom_competitors \u0026gt; 0 WITH wea, we, p ORDER BY wea.node_id, we.amount_usd DESC SKIP 0 WITH wea, collect({p: p, e: we}) AS export_list UNWIND list_slice(export_list, 0, 3) AS r WITH wea, r.p.node_id AS prod_node_id MATCH (wea)-[we:Exports] -\u0026gt;(prod:Product { node_id: prod_node_id }) MATCH (dom:Country)-[de:Exports]-\u0026gt;(prod) WHERE dom.node_id IN $dominating_node_ids RETURN wea, we, prod, de, dom ORDER BY wea.node_id, prod.node_id, dom.node_id \u0026#34;\u0026#34;\u0026#34;, dict( dominating_node_ids=dom_econ_df.node_id.to_list(), weaker_node_ids=weak_econ_df.node_id.to_list(), ), ).get_as_networkx() node_classes = dict( dominating=dom_econ_df.node_id.to_list(), weaker=weak_econ_df.node_id.to_list(), ) # This adjusts the visualization edge weights # to improve readability for u, v, data in dom_vs_weak_econ_g.edges(data=True): if ( dom_vs_weak_econ_g.nodes[u][\u0026#34;node_id\u0026#34;] in node_classes[\u0026#34;dominating\u0026#34;] and dom_vs_weak_econ_g.nodes[v][\u0026#34;_label\u0026#34;] == \u0026#34;Product\u0026#34; ): data[\u0026#34;vis_weight\u0026#34;] = 1e-5 if ( dom_vs_weak_econ_g.nodes[u][\u0026#34;node_id\u0026#34;] in node_classes[\u0026#34;weaker\u0026#34;] and dom_vs_weak_econ_g.nodes[v][\u0026#34;_label\u0026#34;] == \u0026#34;Product\u0026#34; ): data[\u0026#34;vis_weight\u0026#34;] = 1e-3 vis.set_labels(dom_vs_weak_econ_g, LABEL_PROPS) vis.plot( dom_vs_weak_econ_g, node_classes=node_classes, scale=1.25, seed=5, ) Competition Network # Let\u0026rsquo;s look at the competition network projection for Country nodes and CompetesWith edges. We first install the algo extension for Kuzu and create the compnet projection and NetworkX graph for it.\ntry: conn.execute( \u0026#34;\u0026#34;\u0026#34; INSTALL algo; LOAD algo; \u0026#34;\u0026#34;\u0026#34; ) except Exception as e: print(e) try: conn.execute( \u0026#34;\u0026#34;\u0026#34; CALL drop_projected_graph(\u0026#34;compnet\u0026#34;) \u0026#34;\u0026#34;\u0026#34; ) except Exception as e: print(e) conn.execute( \u0026#34;\u0026#34;\u0026#34; CALL project_graph( \u0026#34;compnet\u0026#34;, {\u0026#34;Country\u0026#34;: \u0026#34;n.country_name_short \u0026lt;\u0026gt; \u0026#39;Undeclared\u0026#39;\u0026#34;}, {\u0026#34;CompetesWith\u0026#34;: \u0026#34;true\u0026#34;} ) \u0026#34;\u0026#34;\u0026#34; ) compnet_g = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (a:Country)-[cw:CompetesWith]-\u0026gt;(b:Country) WHERE a.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND b.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN a, cw, b \u0026#34;\u0026#34;\u0026#34;, ).get_as_networkx() Inspection Functions # The following functions will be useful to plot the cluster and analyze the top exports for a specific cluster ID property:\ndef plot_cluster( prop_name: str, prop_value: int, kind: Literal[\u0026#34;graph\u0026#34;, \u0026#34;map\u0026#34;] = \u0026#34;graph\u0026#34;, ): match kind: case \u0026#34;graph\u0026#34;: compnet_cluster_g = conn.execute( f\u0026#34;\u0026#34;\u0026#34; MATCH (a:Country)-[cw:CompetesWith]-\u0026gt; (b:Country) WHERE a.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND b.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND a.`{prop_name}` = $prop_value AND b.`{prop_name}` = $prop_value RETURN a, cw, b \u0026#34;\u0026#34;\u0026#34;, dict(prop_value=prop_value), ).get_as_networkx() vis.set_labels(compnet_cluster_g, LABEL_PROPS) vis.plot(compnet_cluster_g) case \u0026#34;map\u0026#34;: compnet_cluster_df = conn.execute( f\u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND c.`{prop_name}` = $prop_value RETURN c.country_iso3_code AS iso3_code, c.`{prop_name}` AS `{prop_name}` \u0026#34;\u0026#34;\u0026#34;, dict(prop_value=prop_value), ).get_as_df() vis.plot_map( compnet_cluster_df, code_col=\u0026#34;iso3_code\u0026#34;, class_col=prop_name, ) def trade_per_cluster( prop_name: str, prop_value: int, method: Literal[\u0026#34;imports\u0026#34;, \u0026#34;exports\u0026#34;], n: Optional[int] = None, debug: bool = False, ) -\u0026gt; pd.DataFrame: match method: case \u0026#34;exports\u0026#34;: match_stmt = \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country)-[ie:Exports]-\u0026gt;(p:Product) \u0026#34;\u0026#34;\u0026#34; case \u0026#34;imports\u0026#34;: match_stmt = \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country)\u0026lt;-[ie:Imports]-(p:Product) \u0026#34;\u0026#34;\u0026#34; if n is None: limit_stmt = \u0026#34;\u0026#34; limit_param = dict() else: limit_stmt = \u0026#34;LIMIT $n\u0026#34; limit_param = dict(n=n) query = f\u0026#34;\u0026#34;\u0026#34; {match_stmt} WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND c.`{prop_name}` = $prop_value RETURN p.product_name_short AS product, sum(ie.amount_usd) AS total_amount_usd ORDER BY total_amount_usd DESC {limit_stmt} \u0026#34;\u0026#34;\u0026#34; params = dict(prop_value=prop_value) | limit_param if debug: print_query(query, params) products_df = conn.execute(query, params).get_as_df() return products_df Partner clusters are clusters that import what a cluster is exporting. These are likely to match all clusters due to high connectivity in the world economy, but it might not always be the case, depending on the clustering criteria.\ndef partner_clusters( prop_name: str, prop_value: int, include_self: bool = True, debug: bool = False, ) -\u0026gt; list[int]: include_self_stmt = ( \u0026#34;\u0026#34; if include_self else f\u0026#34;AND c2.`{prop_name}` \u0026lt;\u0026gt; $prop_value\u0026#34; ) query = f\u0026#34;\u0026#34;\u0026#34; MATCH (c:Country)-[:Exports]-(p:Product) MATCH (c2:Country)\u0026lt;-[:Imports]-(p) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; AND c.`{prop_name}` = $prop_value AND c2.`{prop_name}` IS NOT NULL {include_self_stmt} RETURN DISTINCT c2.`{prop_name}` AS cid \u0026#34;\u0026#34;\u0026#34; params = dict(prop_value=prop_value) if debug: print_query(query, params) result = conn.execute(query, params) partner_cluster_ids = sorted( c[0] for c in result.get_all() ) return partner_cluster_ids The following functions will help us compute the intra-cluster and inter-cluster trade alignments, i.e., self-sufficiency and external competitiveness, based on cluster-aggregated market share.\ndef trade_alignment_by_cluster( prop_name: str, prop_value: int, method: Literal[\u0026#34;intra\u0026#34;, \u0026#34;inter\u0026#34;], ) -\u0026gt; pd.DataFrame: exports_df = trade_per_cluster( prop_name, prop_value, method=\u0026#34;exports\u0026#34;, ) match method: case \u0026#34;intra\u0026#34;: imports_df = trade_per_cluster( prop_name, prop_value, method=\u0026#34;imports\u0026#34;, ) case \u0026#34;inter\u0026#34;: imports_df = [] for partner_cid in partner_clusters( prop_name, prop_value, ): partner_imports_df = trade_per_cluster( prop_name, partner_cid, method=\u0026#34;imports\u0026#34;, ) imports_df.append(partner_imports_df) imports_df = ( pd.concat(imports_df) .groupby([\u0026#34;product\u0026#34;]) .sum() ) case _: raise ValueError( f\u0026#34;method not supported: {method}\u0026#34; ) trade_df = exports_df.merge( imports_df, on=\u0026#34;product\u0026#34;, how=\u0026#34;right\u0026#34; if method == \u0026#34;intra\u0026#34; else \u0026#34;left\u0026#34;, suffixes=(\u0026#34;_exports\u0026#34;, \u0026#34;_imports\u0026#34;), ).fillna(0) trade_df[\u0026#34;sdr\u0026#34;] = ( trade_df.total_amount_usd_exports / trade_df.total_amount_usd_imports ) trade_df = trade_df.sort_values(\u0026#34;sdr\u0026#34;, ascending=False) return trade_df As a score for measuring either self-sufficiency or external competitiveness, we use weighted average of the Supply-Demand Ration (SDR), where weights are the total export amount (USD) for a given cluster.\ndef global_sdr_score( trade_df: pd.DataFrame, eps=1e-9, ) -\u0026gt; float: df = trade_df[~np.isinf(trade_df.sdr)] df[\u0026#34;log_sdr\u0026#34;] = np.log(np.clip(df.sdr, eps, None)) weights = df.total_amount_usd_exports score = expit( (weights * df.log_sdr).sum() / weights.sum() ) return score.item() Competing Communities # Are there any communities representing closely tied competitor clusters? If so, maybe there are specific products per cluster? ‚òëÔ∏è If not, we have a global economy that is fairly homogenous and diverse. For each property computed with the algo extension, we\u0026rsquo;ll alter the corresponding node table, recreating the property each time.\nconn.execute( \u0026#34;\u0026#34;\u0026#34; ALTER TABLE Country DROP IF EXISTS louvain_id; ALTER TABLE Country ADD IF NOT EXISTS louvain_id INT64; CALL louvain(\u0026#34;compnet\u0026#34;) WITH node, louvain_id SET node.louvain_id = louvain_id; \u0026#34;\u0026#34;\u0026#34; ) The Louvain method partitions the network by optimizing modularity, which essentially means it will find the best partition of communities within the graph, a community being a dense subgraph, i.e., a subgraph where connections among members are more frequent than to outside nodes.\ncompnet_louvain_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN c.node_id AS node_id, c.country_name_short AS label, c.louvain_id AS louvain_id \u0026#34;\u0026#34;\u0026#34; ).get_as_df() node_classes = { k: g.node_id.to_list() for k, g in compnet_louvain_df.groupby(\u0026#34;louvain_id\u0026#34;) } vis.set_labels(compnet_g, LABEL_PROPS) vis.plot( compnet_g, node_classes=node_classes, hide_edges=True, ) In complex networks, it is not uncommon for a huge community to emerge, along with a low number of moderately large communities, and then a lot of smaller communities. This behavior is not particularly exacerbated here, but it\u0026rsquo;s still visible. Below, we inspect the community size distribution.\ncomm_sizes_df = ( compnet_louvain_df[[\u0026#34;louvain_id\u0026#34;, \u0026#34;node_id\u0026#34;]] .groupby(\u0026#34;louvain_id\u0026#34;) .count() .rename(columns=dict(node_id=\u0026#34;num_nodes\u0026#34;)) ) comm_sizes_df = comm_sizes_df.reindex( comm_sizes_df.num_nodes.sort_values(ascending=False).index ) comm_sizes_df num_nodes louvain_id 5 67 6 33 4 30 8 26 7 19 1 14 2 14 0 13 3 8 9 7 10 3 fig, ax = plt.subplots(figsize=(18, 3)) comm_sizes_df.plot.bar(xlabel=\u0026#34;Community ID\u0026#34;, rot=0, ax=ax) plt.legend([\u0026#34;No. Nodes\u0026#34;]) plt.show() Let\u0026rsquo;s also take a look at the members of each community, from largest to smallest.\nfor louvain_id in comm_sizes_df.index: display(f\u0026#34;LOUVAIN ID: {louvain_id}\u0026#34;) display( compnet_louvain_df[ compnet_louvain_df.louvain_id == louvain_id ] .drop(columns=\u0026#34;louvain_id\u0026#34;) .sort_values(\u0026#34;label\u0026#34;) ) 'LOUVAIN ID: 5' node_id label 115 116 Albania 205 207 Andorra 114 115 Anguilla 173 174 Austria 126 127 Belarus ... ... ... 49 50 Tunisia 232 234 Turkiye 123 124 Turks and Caicos Islands 131 132 United Kingdom 204 206 United States of America 67 rows √ó 2 columns\n'LOUVAIN ID: 6' node_id label 209 211 Algeria 171 172 Angola 51 52 Aruba 11 12 Azerbaijan 177 178 Cameroon 54 55 Canada 154 155 Chad 55 56 Colombia 15 16 Democratic Republic of the Congo 220 222 Ecuador 57 58 Egypt 158 159 Equatorial Guinea 100 101 Fiji 75 76 Gabon 223 225 Greenland 211 213 Guyana 17 18 Iran 89 90 Iraq 184 185 Kazakhstan 152 153 Kuwait 135 136 Libya 138 139 Nigeria 139 140 Norway 26 27 Oman 219 221 Republic of the Congo 63 64 Russia 229 231 Sao Tome and Principe 64 65 Saudi Arabia 28 29 South Sudan 7 8 Timor-Leste 31 32 Trinidad and Tobago 112 113 Venezuela 67 68 Yemen 'LOUVAIN ID: 4' node_id label 170 171 Afghanistan 206 208 Australia 12 13 Benin 24 25 Bhutan 207 209 Bolivia 52 53 Burundi 116 117 Central African Republic 101 102 Guinea 163 164 Kyrgyzstan 185 186 Liberia 77 78 Mali 137 138 Mauritania 4 5 Mozambique 165 166 Niger 5 6 Papua New Guinea 78 79 Rwanda 141 142 Senegal 202 204 Sierra Leone 142 143 Solomon Islands 109 110 Somalia 187 188 Sudan 230 232 Suriname 29 30 Syria 124 125 Tajikistan 111 112 Tanzania 110 111 Togo 167 168 Turkmenistan 8 9 US Minor Outlying Islands 99 100 Western Sahara 216 218 Zambia 'LOUVAIN ID: 8' node_id label 96 97 Antarctica 125 126 Bahrain 176 177 Botswana 97 98 China 14 15 Cocos (Keeling) Islands 210 212 Guam 160 161 Heard and McDonald Islands 87 88 Hong Kong 162 163 Israel 134 135 Japan 102 103 Lesotho 195 197 Macao 46 47 Malaysia 62 63 Malta 164 165 Namibia 120 121 Northern Mariana Islands 215 217 Philippines 47 48 Pitcairn 233 235 Samoa 48 49 Singapore 65 66 South Georgia and South Sandwich Islds. 76 77 South Korea 113 114 Taiwan 79 80 Vatican City 95 96 Vietnam 169 170 Wallis and Futuna 'LOUVAIN ID: 7' node_id label 10 11 Argentina 127 128 Belize 217 219 Brazil 34 35 Burkina Faso 156 157 C√¥te d'Ivoire 94 95 Eswatini 129 130 Ethiopia 38 39 Ghana 86 87 Guatemala 117 118 Honduras 151 152 Kenya 91 92 Malawi 108 109 New Zealand 197 199 Nicaragua 93 94 Paraguay 50 51 Uganda 168 169 Uruguay 146 147 Uzbekistan 68 69 Zimbabwe 'LOUVAIN ID: 1' node_id label 69 70 Bangladesh 16 17 Cabo Verde 143 144 El Salvador 1 2 Falkland Islands 39 40 Haiti 59 60 Kiribati 43 44 Maldives 19 20 Mauritius 74 75 Micronesia 107 108 Nauru 122 123 Seychelles 136 137 Sri Lanka 66 67 Tuvalu 191 192 Vanuatu 'LOUVAIN ID: 2' node_id label 172 173 Armenia 218 220 Chile 181 182 Eritrea 2 3 Georgia 150 151 Jordan 41 42 Lebanon 42 43 Moldova 25 26 Mongolia 225 227 North Macedonia 140 141 Panama 92 93 Peru 32 33 South Africa 190 191 Ukraine 33 34 United Arab Emirates 'LOUVAIN ID: 0' node_id label 0 1 American Samoa 80 81 Antigua and Barbuda 13 14 Barbados 82 83 Cura√ßao 179 180 Cyprus 159 160 Greece 85 86 Grenada 58 59 Jamaica 119 120 Marshall Islands 214 216 Niue 60 61 Saint Lucia 9 10 Saint Vincent and the Grenadines 35 36 The Bahamas 'LOUVAIN ID: 3' node_id label 212 214 Cambodia 56 57 Comoros 3 4 Laos 90 91 Madagascar 226 228 Montenegro 44 45 Myanmar 227 229 Pakistan 186 187 Palau 'LOUVAIN ID: 9' node_id label 132 133 British Indian Ocean Territory 36 37 Cook Islands 182 183 Faroe Islands 155 156 French Southern and Antarctic Lands 222 224 Guinea-Bissau 161 162 Iceland 201 203 Saint Helena, Ascension and Tristan da Cunha 'LOUVAIN ID: 10' node_id label 192 194 Costa Rica 83 84 Dominica 157 158 Dominican Republic largest_louvain_id = comm_sizes_df.index[0].item() largest_louvain_id 5 smallest_louvain_id = comm_sizes_df.index[-1].item() smallest_louvain_id 10 Community Subgraphs # Community subgraphs illustrates clusters where competition is more prevalent among its members than countries outside of the community. For this graph (our Econ CompNet, or compnet), they are almost always (if not always) complete subgraphs. We can plot any cluster by its ID.\nplot_cluster(\u0026#34;louvain_id\u0026#34;, largest_louvain_id) Community Mapping # Network visualization is not always the best approach to understand your data. This is a good example of this. Since we\u0026rsquo;re working with a complete (or nearly complete) subgraph, looking at relationships is less helpful, but looking at a map for a community is a lot more helpful, as we can see below.\nplot_cluster(\u0026#34;louvain_id\u0026#34;, largest_louvain_id, kind=\u0026#34;map\u0026#34;) Top Exported Products # Is there any export overlap between large and small communities? largest_comm_top_exported = top_frac( trade_per_cluster( \u0026#34;louvain_id\u0026#34;, largest_louvain_id, method=\u0026#34;exports\u0026#34; ), \u0026#34;total_amount_usd\u0026#34;, ) largest_comm_top_exported product total_amount_usd 0 Commodities not specified, according to kind 1.506978e+12 1 Oils petroleum, bituminous, distillates 1.382886e+12 2 Medicaments, doses, nes 1.110865e+12 3 Blood 7.572479e+11 4 Petroleum oils, crude 5.977950e+11 5 Automobiles nes, gas turbine powered 5.668137e+11 6 Gold in unwrought forms 5.482768e+11 7 Automobiles, spark ignition, 1500-3000cc 5.231797e+11 8 Transmit-receive apparatus for radio, TV 4.899130e+11 9 Monolithic integrated circuits, digital 4.366876e+11 10 Trade data discrepancies 3.521978e+11 11 Parts of data processing equipment 3.358087e+11 12 Automobiles, spark ignition, 1000-1500cc 2.657018e+11 13 Fixed wing aircraft, \u0026gt;15,000kg 2.641497e+11 14 Motor vehicle parts nes 2.565987e+11 15 Vaccines, human 2.412159e+11 16 Natural gas, liquefied 2.406005e+11 17 Gold, semi-manufactured forms 2.394518e+11 smallest_comm_top_exported = top_frac( trade_per_cluster( \u0026#34;louvain_id\u0026#34;, smallest_louvain_id, method=\u0026#34;exports\u0026#34;, ), \u0026#34;total_amount_usd\u0026#34;, ) smallest_comm_top_exported product total_amount_usd 0 Instruments for medical science, nes 1.032487e+10 1 Medical needles, catheters 8.305035e+09 2 Trade data discrepancies 7.981437e+09 jaccard_sim( largest_comm_top_exported[\u0026#34;product\u0026#34;], smallest_comm_top_exported[\u0026#34;product\u0026#34;] ) 0.05 Top Imported Products # Is there any import overlap between large and small communities? largest_comm_top_imported = top_frac( trade_per_cluster( \u0026#34;louvain_id\u0026#34;, largest_louvain_id, method=\u0026#34;imports\u0026#34;, ), \u0026#34;total_amount_usd\u0026#34;, ) largest_comm_top_imported product total_amount_usd 0 Petroleum oils, crude 1.933629e+12 1 Commodities not specified, according to kind 1.411868e+12 2 Oils petroleum, bituminous, distillates 1.197450e+12 3 Transmit-receive apparatus for radio, TV 9.829336e+11 4 Medicaments, doses, nes 8.788548e+11 5 Trade data discrepancies 7.574121e+11 6 Gold in unwrought forms 7.455694e+11 7 Blood 6.453376e+11 8 Automobiles nes, gas turbine powered 5.992672e+11 9 Natural gas, as gas 5.284708e+11 10 Parts of data processing equipment 5.119915e+11 11 Automobiles, spark ignition, 1500-3000cc 4.830173e+11 12 Monolithic integrated circuits, digital 4.750667e+11 smallest_comm_top_imported = top_frac( trade_per_cluster( \u0026#34;louvain_id\u0026#34;, smallest_louvain_id, method=\u0026#34;imports\u0026#34;, ), \u0026#34;total_amount_usd\u0026#34;, ) smallest_comm_top_imported product total_amount_usd 0 Oils petroleum, bituminous, distillates 1.373957e+10 1 Commodities not specified, according to kind 7.539036e+09 2 Transmit-receive apparatus for radio, TV 3.136932e+09 3 Automobiles, spark ignition, 1500-3000cc 2.425662e+09 4 Jewellery of precious metal 2.326848e+09 5 Instruments for medical science, nes 2.297871e+09 6 Monolithic integrated circuits, digital 2.243517e+09 7 Maize except seed corn 2.138190e+09 8 Natural gas, liquefied 2.103650e+09 9 Petroleum oils, crude 2.089410e+09 10 Propane, liquefied 2.061530e+09 jaccard_sim( largest_comm_top_imported[\u0026#34;product\u0026#34;], smallest_comm_top_imported[\u0026#34;product\u0026#34;], ) 0.3333333333333333 Trade Alignment # Trade alignment can be used to determine a cluster\u0026rsquo;s self-sufficiency by looking at internal country-country trade, or it can be used to determine a cluster\u0026rsquo;s external competitiveness by looking at inter-cluster country-country trade. We determine both dimensions of trade alignment (intra and inter cluster) based on the supply/demand ratio, more specifically the weighted average of log-SDR, with weights being total amounts (USD) of exports/imports, globally per cluster.\nThis score is scaled to a 0..1 range using a sigmoid transformation, so anything above 0.5 should be good. The log-transformation ensures the distribution is not skewed.\nSelf-Sufficiency # Most communities are self-sufficient or nearly self-sufficient, with only community 5 showing a little more vulnerability.\ncomm_self_sufficiency_df = pd.DataFrame( dict( louvain_id=louvain_id, score=global_sdr_score( trade_alignment_by_cluster( \u0026#34;louvain_id\u0026#34;, louvain_id, method=\u0026#34;intra\u0026#34;, ) ), ) for louvain_id in comm_sizes_df.index ).sort_values(\u0026#34;score\u0026#34;, ascending=False) comm_self_sufficiency_df louvain_id score 9 9 0.985065 2 4 0.970089 5 1 0.959150 8 3 0.939180 10 10 0.895896 1 6 0.869495 4 7 0.860508 6 2 0.742791 3 8 0.644520 7 0 0.564580 0 5 0.493976 colors = comm_self_sufficiency_df.score.apply( lambda s: MPL_PALETTE[0] if s \u0026gt;= 0.5 else MPL_PALETTE[1] ) fig, ax = plt.subplots(figsize=(18, 3)) comm_self_sufficiency_df.plot.bar( x=\u0026#34;louvain_id\u0026#34;, y=\u0026#34;score\u0026#34;, xlabel=\u0026#34;Community ID\u0026#34;, color=colors, rot=0, ax=ax, ) plt.axhline( y=0.5, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([ \u0026#34;Self-Sufficiency Threshold\u0026#34;, \u0026#34;Global Log-SDR Score\u0026#34; ]) plt.show() compnet_louvain_df[compnet_louvain_df.louvain_id == 5] node_id label louvain_id 6 7 Qatar 5 18 19 Lithuania 5 20 21 Portugal 5 21 22 Palestine 5 22 23 British Virgin Islands 5 ... ... ... ... 221 223 Spain 5 224 226 India 5 228 230 Romania 5 231 233 Slovenia 5 232 234 Turkiye 5 67 rows √ó 3 columns\nExternal Competitiveness # Most communities are not particularly competitive externally, but this was to be expected due to the criteria used to cluster‚Äîcommunity dense subgraphs also point to higher internal competition.\ncomm_external_comp_df = pd.DataFrame( dict( louvain_id=louvain_id, score=global_sdr_score( trade_alignment_by_cluster( \u0026#34;louvain_id\u0026#34;, louvain_id, method=\u0026#34;inter\u0026#34;, ) ), ) for louvain_id in comm_sizes_df.index ).sort_values(\u0026#34;score\u0026#34;, ascending=False) comm_external_comp_df louvain_id score 0 5 0.360300 3 8 0.304639 1 6 0.200342 2 4 0.121624 4 7 0.089482 6 2 0.072207 5 1 0.055127 8 3 0.033868 9 9 0.018567 10 10 0.012569 7 0 0.010781 colors = comm_external_comp_df.score.apply( lambda s: MPL_PALETTE[0] if s \u0026gt;= 0.5 else MPL_PALETTE[1] ) fig, ax = plt.subplots(figsize=(18, 3)) comm_external_comp_df.plot.bar( x=\u0026#34;louvain_id\u0026#34;, y=\u0026#34;score\u0026#34;, xlabel=\u0026#34;Community ID\u0026#34;, color=colors, rot=0, ax=ax, ) plt.axhline( y=0.5, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([ \u0026#34;External Competitiveness Threshold\u0026#34;, \u0026#34;Global SDR Score\u0026#34; ]) plt.show() compnet_louvain_df[compnet_louvain_df.louvain_id == 8] node_id label louvain_id 14 15 Cocos (Keeling) Islands 8 46 47 Malaysia 8 47 48 Pitcairn 8 48 49 Singapore 8 62 63 Malta 8 65 66 South Georgia and South Sandwich Islds. 8 76 77 South Korea 8 79 80 Vatican City 8 87 88 Hong Kong 8 95 96 Vietnam 8 96 97 Antarctica 8 97 98 China 8 102 103 Lesotho 8 113 114 Taiwan 8 120 121 Northern Mariana Islands 8 125 126 Bahrain 8 134 135 Japan 8 160 161 Heard and McDonald Islands 8 162 163 Israel 8 164 165 Namibia 8 169 170 Wallis and Futuna 8 176 177 Botswana 8 195 197 Macao 8 210 212 Guam 8 215 217 Philippines 8 233 235 Samoa 8 Weakly Connected Competitors # Strongly connected components in our graph would have capture mutual competition among peers, cyclical or balanced rivalries, or equivalent strategic positions. However, once we removed the \u0026ldquo;Undeclared\u0026rdquo; pseudo-country, we weren\u0026rsquo;t able to find any strongly connected components that were not singletons.\nAs such, we compute the weakly connected components, instead capturing the individual or isolated components of countries competing among themselves, regardless of export amount (which establishes direction, in our graph).\nconn.execute( \u0026#34;\u0026#34;\u0026#34; ALTER TABLE Country DROP IF EXISTS wcc_id; ALTER TABLE Country ADD IF NOT EXISTS wcc_id INT64; CALL weakly_connected_components(\u0026#34;compnet\u0026#34;) WITH node, group_id SET node.wcc_id = group_id; \u0026#34;\u0026#34;\u0026#34; ) compnet_wcc_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN c.node_id AS node_id, c.country_name_short AS label, c.wcc_id AS wcc_id \u0026#34;\u0026#34;\u0026#34; ).get_as_df() node_classes = { k: g.node_id.to_list() for k, g in compnet_wcc_df.groupby(\u0026#34;wcc_id\u0026#34;) } vis.set_labels(compnet_g, LABEL_PROPS) vis.plot( compnet_g, node_classes=node_classes, hide_edges=True, ) As we can see, there a multiple weakly connected competitors, but most of them are single nodes in their own SCC. Other than that, there is a large component of 64 countries, and then two other smaller components with over 20 nodes each, that we\u0026rsquo;ll inspect below.\nwcc_sizes_df = ( compnet_wcc_df[[\u0026#34;wcc_id\u0026#34;, \u0026#34;node_id\u0026#34;]] .groupby(\u0026#34;wcc_id\u0026#34;) .count() .rename(columns=dict(node_id=\u0026#34;num_nodes\u0026#34;)) ) wcc_sizes_df = wcc_sizes_df.reindex( wcc_sizes_df.num_nodes.sort_values(ascending=False).index ) wcc_sizes_df num_nodes wcc_id 0 64 1 28 4 24 5 11 2 9 ... ... 209 1 215 1 226 1 228 1 230 1 68 rows √ó 1 columns\nwcc_sizes_ord_df = wcc_sizes_df.reset_index(drop=True) wcc_singleton_threshold = ( wcc_sizes_ord_df[wcc_sizes_ord_df.num_nodes \u0026lt;= 1] .index[0] .item() ) fig, ax = plt.subplots(figsize=(30, 5)) wcc_sizes_df.plot.bar(rot=0, ax=ax) plt.axvline( x=wcc_singleton_threshold, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([\u0026#34;Singleton Threshold\u0026#34;, \u0026#34;No. Nodes\u0026#34;]) plt.show() Let\u0026rsquo;s take a look at the members of each weak component, from largest to smallest.\nfor wcc_id in wcc_sizes_df[wcc_sizes_df.num_nodes \u0026gt; 1].index: display(f\u0026#34;WCC ID: {wcc_id}\u0026#34;) display( compnet_wcc_df[compnet_wcc_df.wcc_id == wcc_id] .drop(columns=\u0026#34;wcc_id\u0026#34;) ) 'WCC ID: 0' node_id label 0 1 American Samoa 6 7 Qatar 11 12 Azerbaijan 14 15 Cocos (Keeling) Islands 18 19 Lithuania ... ... ... 211 213 Guyana 215 217 Philippines 221 223 Spain 224 226 India 231 233 Slovenia 64 rows √ó 2 columns\n'WCC ID: 1' node_id label 1 2 Falkland Islands 16 17 Cabo Verde 19 20 Mauritius 26 27 Oman 39 40 Haiti 43 44 Maldives 47 48 Pitcairn 55 56 Colombia 59 60 Kiribati 66 67 Tuvalu 67 68 Yemen 69 70 Bangladesh 74 75 Micronesia 79 80 Vatican City 102 103 Lesotho 122 123 Seychelles 125 126 Bahrain 132 133 British Indian Ocean Territory 155 156 French Southern and Antarctic Lands 161 162 Iceland 162 163 Israel 176 177 Botswana 177 178 Cameroon 182 183 Faroe Islands 191 192 Vanuatu 201 203 Saint Helena, Ascension and Tristan da Cunha 220 222 Ecuador 223 225 Greenland 'WCC ID: 4' node_id label 4 5 Mozambique 12 13 Benin 38 39 Ghana 50 51 Uganda 52 53 Burundi 73 74 Finland 75 76 Gabon 77 78 Mali 78 79 Rwanda 99 100 Western Sahara 101 102 Guinea 111 112 Tanzania 116 117 Central African Republic 121 122 Sweden 128 129 Switzerland 131 132 United Kingdom 163 164 Kyrgyzstan 165 166 Niger 170 171 Afghanistan 184 185 Kazakhstan 193 195 Ireland 205 207 Andorra 219 221 Republic of the Congo 230 232 Suriname 'WCC ID: 5' node_id label 5 6 Papua New Guinea 8 9 US Minor Outlying Islands 58 59 Jamaica 60 61 Saint Lucia 83 84 Dominica 85 86 Grenada 120 121 Northern Mariana Islands 145 146 Tonga 157 158 Dominican Republic 192 194 Costa Rica 210 212 Guam 'WCC ID: 2' node_id label 2 3 Georgia 3 4 Laos 27 28 North Korea 33 34 United Arab Emirates 44 45 Myanmar 172 173 Armenia 174 175 Belgium 212 214 Cambodia 226 228 Montenegro 'WCC ID: 10' node_id label 10 11 Argentina 86 87 Guatemala 93 94 Paraguay 117 118 Honduras 168 169 Uruguay 197 199 Nicaragua 217 219 Brazil 'WCC ID: 9' node_id label 9 10 Saint Vincent and the Grenadines 56 57 Comoros 90 91 Madagascar 119 120 Marshall Islands 186 187 Palau 'WCC ID: 15' node_id label 15 16 Democratic Republic of the Congo 32 33 South Africa 92 93 Peru 181 182 Eritrea 218 220 Chile 'WCC ID: 42' node_id label 42 43 Moldova 190 191 Ukraine 203 205 Serbia 228 230 Romania 'WCC ID: 22' node_id label 22 23 British Virgin Islands 36 37 Cook Islands 81 82 Bermuda 178 179 Cayman Islands 'WCC ID: 17' node_id label 17 18 Iran 31 32 Trinidad and Tobago 158 159 Equatorial Guinea 'WCC ID: 137' node_id label 137 138 Mauritania 185 186 Liberia 206 208 Australia 'WCC ID: 40' node_id label 40 41 Indonesia 232 234 Turkiye 'WCC ID: 24' node_id label 24 25 Bhutan 216 218 Zambia 'WCC ID: 94' node_id label 94 95 Eswatini 127 128 Belize 'WCC ID: 49' node_id label 49 50 Tunisia 61 62 Morocco 'WCC ID: 110' node_id label 110 111 Togo 141 142 Senegal 'WCC ID: 109' node_id label 109 110 Somalia 187 188 Sudan 'WCC ID: 136' node_id label 136 137 Sri Lanka 143 144 El Salvador 'WCC ID: 21' node_id label 21 22 Palestine 199 201 Poland 'WCC ID: 167' node_id label 167 168 Turkmenistan 207 209 Bolivia 'WCC ID: 160' node_id label 160 161 Heard and McDonald Islands 166 167 Saint Pierre and Miquelon 'WCC ID: 223' node_id label 222 224 Guinea-Bissau 233 235 Samoa largest_wcc_id = wcc_sizes_df.index[0].item() largest_wcc_id 0 smallest_wcc_id = wcc_sizes_df.index[-1].item() smallest_wcc_id 230 Component Subgraphs # plot_cluster(\u0026#34;wcc_id\u0026#34;, largest_wcc_id) Component Mapping # plot_cluster(\u0026#34;wcc_id\u0026#34;, largest_wcc_id, kind=\u0026#34;map\u0026#34;) Top Exported Products # Is there any export overlap between large and small components? largest_wcc_top_exported = top_frac( trade_per_cluster(\u0026#34;wcc_id\u0026#34;, largest_wcc_id, \u0026#34;exports\u0026#34;), \u0026#34;total_amount_usd\u0026#34;, ) largest_wcc_top_exported product total_amount_usd 0 Monolithic integrated circuits, digital 2.880361e+12 1 Petroleum oils, crude 2.815121e+12 2 Oils petroleum, bituminous, distillates 2.185186e+12 3 Commodities not specified, according to kind 1.859842e+12 4 Transmit-receive apparatus for radio, TV 1.629116e+12 5 Trade data discrepancies 9.872075e+11 6 Parts of data processing equipment 7.646240e+11 7 Medicaments, doses, nes 7.542306e+11 smallest_wcc_top_exported = top_frac( trade_per_cluster(\u0026#34;wcc_id\u0026#34;, smallest_wcc_id, \u0026#34;exports\u0026#34;), \u0026#34;total_amount_usd\u0026#34;, ) smallest_wcc_top_exported product total_amount_usd 0 Petroleum oils, crude 24676511.0 jaccard_sim( largest_wcc_top_exported[\u0026#34;product\u0026#34;], smallest_wcc_top_exported[\u0026#34;product\u0026#34;] ) 0.125 Top Imported Products # Is there any import overlap between large and small components? largest_wcc_top_imported = top_frac( trade_per_cluster(\u0026#34;wcc_id\u0026#34;, largest_wcc_id, \u0026#34;imports\u0026#34;), \u0026#34;total_amount_usd\u0026#34;, ) largest_wcc_top_imported product total_amount_usd 0 Petroleum oils, crude 3.075323e+12 1 Monolithic integrated circuits, digital 2.780868e+12 2 Commodities not specified, according to kind 1.510984e+12 3 Oils petroleum, bituminous, distillates 1.420618e+12 4 Transmit-receive apparatus for radio, TV 1.332514e+12 5 Trade data discrepancies 1.248542e+12 6 Medicaments, doses, nes 8.050835e+11 7 Parts of data processing equipment 6.992992e+11 8 Automobiles, spark ignition, 1500-3000cc 6.571986e+11 smallest_wcc_top_imported = top_frac( trade_per_cluster(\u0026#34;wcc_id\u0026#34;, smallest_wcc_id, \u0026#34;imports\u0026#34;), \u0026#34;total_amount_usd\u0026#34;, ) smallest_wcc_top_imported product total_amount_usd 0 Oils petroleum, bituminous, distillates 88922018.0 1 Cargo vessels, not tanker or refrigerated 23292230.0 2 Commodities not specified, according to kind 21288342.0 3 Rice, semi- or wholly-milled 15654678.0 jaccard_sim( largest_comm_top_imported[\u0026#34;product\u0026#34;], smallest_wcc_top_imported[\u0026#34;product\u0026#34;] ) 0.13333333333333333 Trade Alignment # Again, trade alignment can be used to determine a cluster\u0026rsquo;s self-sufficiency by looking at internal country-country trade, or it can be used to determine a cluster\u0026rsquo;s external competitiveness by looking at inter-cluster country-country trade. We determine both dimensions of trade alignment (intra and inter cluster) based on the supply/demand ratio, more specifically the weighted average of log-SDR, with weights being total amounts (USD) of exports/imports, globally per cluster.\nThis score is scaled to a 0..1 range using a sigmoid transformation, so anything above 0.5 should be good. The log-transformation ensures the distribution is not skewed.\nSelf-Sufficiency # Most components are self-sufficient or nearly self-sufficient, with only three of them, components 209, 22 and 196, showing a little more vulnerability.\nwcc_self_sufficiency_df = pd.DataFrame( dict( wcc_id=wcc_id, score=global_sdr_score( trade_alignment_by_cluster( \u0026#34;wcc_id\u0026#34;, wcc_id, method=\u0026#34;intra\u0026#34;, ) ), ) for wcc_id in wcc_sizes_df.index ).sort_values(\u0026#34;score\u0026#34;, ascending=False) wcc_self_sufficiency_df wcc_id score 20 167 0.999962 57 197 0.999932 25 34 0.999926 23 25 0.999721 52 156 0.999568 ... ... ... 2 4 0.586408 0 0 0.518923 63 209 0.464203 9 22 0.300144 56 196 0.280055 68 rows √ó 2 columns\ncolors = wcc_self_sufficiency_df.score.apply( lambda s: MPL_PALETTE[0] if s \u0026gt;= 0.5 else MPL_PALETTE[1] ) fig, ax = plt.subplots(figsize=(30, 5)) wcc_self_sufficiency_df.plot.bar( x=\u0026#34;wcc_id\u0026#34;, y=\u0026#34;score\u0026#34;, xlabel=\u0026#34;Weak Component ID\u0026#34;, color=colors, rot=0, ax=ax, ) plt.axhline( y=0.5, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([ \u0026#34;Self-Sufficiency Threshold\u0026#34;, \u0026#34;Global SDR Score\u0026#34; ]) plt.show() compnet_wcc_df[compnet_wcc_df.wcc_id == 0] node_id label wcc_id 0 1 American Samoa 0 6 7 Qatar 0 11 12 Azerbaijan 0 14 15 Cocos (Keeling) Islands 0 18 19 Lithuania 0 ... ... ... ... 211 213 Guyana 0 215 217 Philippines 0 221 223 Spain 0 224 226 India 0 231 233 Slovenia 0 64 rows √ó 3 columns\nExternal Competitiveness # Most components are not particularly competitive externally, even less so than communities, with the large majority having a SDR-based score lower than 0.1.\nwcc_external_comp_df = pd.DataFrame( dict( wcc_id=wcc_id, score=global_sdr_score( trade_alignment_by_cluster( \u0026#34;wcc_id\u0026#34;, wcc_id, method=\u0026#34;inter\u0026#34;, ) ), ) for wcc_id in wcc_sizes_df.index ).sort_values(\u0026#34;score\u0026#34;, ascending=False) wcc_external_comp_df wcc_id score 0 0 0.424935 11 137 0.135049 2 4 0.112296 7 15 0.086413 5 10 0.084617 ... ... ... 59 195 0.000212 21 160 0.000073 67 230 0.000053 43 114 0.000023 26 65 0.000009 68 rows √ó 2 columns\ncolors = wcc_external_comp_df.score.apply( lambda s: MPL_PALETTE[0] if s \u0026gt;= 0.5 else MPL_PALETTE[1] ) fig, ax = plt.subplots(figsize=(30, 5)) wcc_external_comp_df.plot.bar( x=\u0026#34;wcc_id\u0026#34;, y=\u0026#34;score\u0026#34;, xlabel=\u0026#34;Weak Component ID\u0026#34;, color=colors, rot=0, ax=ax, ) plt.axhline( y=0.5, color=MPL_PALETTE[1], linestyle=\u0026#34;--\u0026#34;, linewidth=2, ) plt.legend([ \u0026#34;External Competitiveness Threshold\u0026#34;, \u0026#34;Global SDR Score\u0026#34; ]) plt.show() compnet_louvain_df[compnet_louvain_df.louvain_id == 0] node_id label louvain_id 0 1 American Samoa 0 9 10 Saint Vincent and the Grenadines 0 13 14 Barbados 0 35 36 The Bahamas 0 58 59 Jamaica 0 60 61 Saint Lucia 0 80 81 Antigua and Barbuda 0 82 83 Cura√ßao 0 85 86 Grenada 0 119 120 Marshall Islands 0 159 160 Greece 0 179 180 Cyprus 0 214 216 Niue 0 Communities vs Components # By matching the clustering (communities and weak components) with the highest number of clusters, and therefore smaller clusters, to the clustering with the lowest number of clusters, we can run a pairwise cluster comparison:\nWhich countries belong to a community, but not the weak component? Which countries belong to a weak component, but not the community? Which countries belong to both? Is there a particular semantic to these countries? len(wcc_sizes_df), len(comm_sizes_df) (68, 11) NN-Clusters # We compute community to weak component similarities, selecting the nearest-neighbor community for each component. Given the higher number of components when compared to communities, we\u0026rsquo;ll necessarily have repeated nearest-neighbor communities.\ncluster_sim_df = [] for wcc_id, wcc in compnet_wcc_df.groupby(\u0026#34;wcc_id\u0026#34;): for louvain_id, comm in ( compnet_louvain_df.groupby(\u0026#34;louvain_id\u0026#34;) ): cluster_sim_df.append( dict( wcc_id=wcc_id, louvain_id=louvain_id, sim=jaccard_sim(wcc.label, comm.label), ) ) cluster_sim_df = pd.DataFrame(cluster_sim_df) cluster_sim_df = cluster_sim_df.loc[ cluster_sim_df .groupby([\u0026#34;wcc_id\u0026#34;]) .idxmax() .sim ] cluster_sim_df wcc_id louvain_id sim 5 0 5 0.297030 12 1 1 0.354839 25 2 3 0.307692 37 4 4 0.317073 54 5 10 0.272727 ... ... ... ... 693 215 0 0.076923 713 223 9 0.125000 717 226 2 0.071429 729 228 3 0.125000 743 230 6 0.030303 68 rows √ó 3 columns\nFor example, community 5 matches with 20 different weak components.\ncluster_sim_df.louvain_id.value_counts() louvain_id 5 20 7 10 4 10 2 7 6 6 8 4 1 3 3 3 0 2 9 2 10 1 Name: count, dtype: int64 cluster_sim_df[cluster_sim_df.louvain_id == 5] wcc_id louvain_id sim 5 0 5 0.297030 126 21 5 0.029851 192 40 5 0.029851 225 49 5 0.029851 269 72 5 0.014925 280 84 5 0.014925 313 98 5 0.014925 335 103 5 0.014925 401 114 5 0.014925 423 126 5 0.014925 489 144 5 0.014925 511 147 5 0.014925 544 153 5 0.014925 599 173 5 0.014925 610 195 5 0.014925 632 197 5 0.014925 643 199 5 0.014925 654 201 5 0.014925 676 209 5 0.014925 687 214 5 0.014925 Set Comparison # Let\u0026rsquo;s select a weakest component and retrieve its NN community to compare.\n## comp_wcc_id = largest_wcc_id comp_wcc_id = compnet_wcc_df.loc[ compnet_wcc_df.label == \u0026#34;Australia\u0026#34;, \u0026#34;wcc_id\u0026#34; ].item() comp_comm_id = cluster_sim_df.loc[ cluster_sim_df.wcc_id == comp_wcc_id, \u0026#34;louvain_id\u0026#34;, ].item() comp_wcc_id, comp_comm_id (137, 4) comp_wcc_countries = set( compnet_wcc_df.loc[ compnet_wcc_df.wcc_id == comp_wcc_id, \u0026#34;label\u0026#34; ] ) comp_louvain_countries = set( compnet_louvain_df.loc[ compnet_louvain_df.louvain_id == comp_comm_id, \u0026#34;label\u0026#34; ] ) WCC Exclusive # pd.Series( list(comp_wcc_countries - comp_louvain_countries), name=\u0026#34;country\u0026#34;, ).sort_values().to_frame() country Community Exclusive # pd.Series( list(comp_louvain_countries - comp_wcc_countries), name=\u0026#34;country\u0026#34;, ).sort_values().to_frame() country 5 Afghanistan 26 Benin 20 Bhutan 14 Bolivia 24 Burundi 18 Central African Republic 4 Guinea 1 Kyrgyzstan 23 Mali 8 Mozambique 25 Niger 17 Papua New Guinea 7 Rwanda 21 Senegal 2 Sierra Leone 13 Solomon Islands 0 Somalia 15 Sudan 12 Suriname 10 Syria 19 Tajikistan 6 Tanzania 3 Togo 9 Turkmenistan 11 US Minor Outlying Islands 16 Western Sahara 22 Zambia WCC and Community Overlap # pd.Series( list(comp_wcc_countries | comp_louvain_countries), name=\u0026#34;country\u0026#34;, ).sort_values().to_frame() country 17 Afghanistan 9 Australia 14 Benin 10 Bhutan 5 Bolivia 29 Burundi 26 Central African Republic 2 Guinea 1 Kyrgyzstan 27 Liberia 28 Mali 8 Mauritania 19 Mozambique 13 Niger 6 Papua New Guinea 18 Rwanda 11 Senegal 15 Sierra Leone 4 Solomon Islands 0 Somalia 24 Sudan 23 Suriname 21 Syria 7 Tajikistan 3 Tanzania 16 Togo 20 Turkmenistan 22 US Minor Outlying Islands 25 Western Sahara 12 Zambia Economic Pressure (PageRank) # Economic pressure can easily be measured using PageRank, as it is a converging metric that aggregates the overall incoming competition strength, increasing its value as the contributing competing countries are themselves under economic pressure.\nconn.execute( \u0026#34;\u0026#34;\u0026#34; ALTER TABLE Country DROP IF EXISTS pagerank; ALTER TABLE Country ADD IF NOT EXISTS pagerank DOUBLE; CALL page_rank(\u0026#34;compnet\u0026#34;, maxIterations := 100) WITH node, rank SET node.pagerank = rank \u0026#34;\u0026#34;\u0026#34; ) Most Pressured Countries # most_pressured_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN c.node_id AS node_id, c.country_name_short AS label, c.pagerank AS pagerank ORDER BY c.pagerank DESC LIMIT 25 \u0026#34;\u0026#34;\u0026#34; ).get_as_df() fig, ax = plt.subplots(figsize=(5, 8)) ( most_pressured_df.iloc[::-1] .plot.barh(x=\u0026#34;label\u0026#34;, y=\u0026#34;pagerank\u0026#34;, ax=ax) ) plt.ylabel(None) plt.legend([]) plt.show() Least Pressured Countries # least_pressured_df = conn.execute( \u0026#34;\u0026#34;\u0026#34; MATCH (c:Country) WHERE c.country_name_short \u0026lt;\u0026gt; \u0026#34;Undeclared\u0026#34; RETURN c.node_id AS node_id, c.country_name_short AS label, c.pagerank AS pagerank ORDER BY c.pagerank ASC LIMIT 25 \u0026#34;\u0026#34;\u0026#34; ).get_as_df() fig, ax = plt.subplots(figsize=(5, 8)) ( least_pressured_df.iloc[::-1] .plot.barh(x=\u0026#34;label\u0026#34;, y=\u0026#34;pagerank\u0026#34;, ax=ax) ) plt.ylabel(None) plt.legend([]) plt.show() Closing Remarks # Economies are complex systems, and the complex relations between markets can be captured using a graph. Determining which nodes and relationships to model is crucial to interpretation‚Äîour graph focused on competition relationships, and so our metrics and partition approaches illustrated this.\nNetwork analysis tools are usually not as exotic as they want to make us believe. Useful graph data science is usually not that complex, particularly now that tooling is widely available, but it can certainly be extremely insightful, specially when the graph is correctly modeled.\nThis is only a small introduction to this topic, using world economy and trade as an example topic, which I have been particularly interested in.\nThe economy and the world overall is suffering. Graphs will help us find solution to complex problems, but it requires the commitment to always ask yourself: could I do this without a graph? When the answer is yes, then you should rethink your approach. If you\u0026rsquo;re not looking at complex relations, you\u0026rsquo;re just doing more of the same.\nBottom line, use graphs and use them correctly.\n","date":"6 August 2025","externalUrl":null,"permalink":"/posts/economic-competition-networks/","section":"","summary":"Summary # In this video, we reproduce the approach that predicts Survivor winners and apply it to Economic Competition Networks to better understand world trade and economic leaders.","title":"Economic Competition Networks","type":"posts"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/economy/","section":"Tags","summary":"","title":"Economy","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/market/","section":"Tags","summary":"","title":"Market","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/network-analysis/","section":"Tags","summary":"","title":"Network-Analysis","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/network-science/","section":"Tags","summary":"","title":"Network-Science","type":"tags"},{"content":"","date":"6 August 2025","externalUrl":null,"permalink":"/tags/world-trade/","section":"Tags","summary":"","title":"World-Trade","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/embeddings/","section":"Tags","summary":"","title":"Embeddings","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/gemma3/","section":"Tags","summary":"","title":"Gemma3","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/graph-rag/","section":"Tags","summary":"","title":"Graph-Rag","type":"tags"},{"content":" Summary # In this video, I\u0026rsquo;ll delve into GraphRAG, learning about K√πzuDB, node embeddings vs text embeddings, and LangChain, running on top of Ollama with phi4 and gemma3.\n\u003e What is GraphRAG? # RAG stands for Retrieval Augmented Generation, which means that, instead of simply providing an answer to the user via prompt engineering (i.e., just using an LLM), we also factor in a knowledge base into the process, to provide context and enrich the generated answer. This knowledge base is usually an index that we can search (hence RAG), but it can also be a graph. If we\u0026rsquo;re retrieving from a graph, to produce a context for an LLM, we\u0026rsquo;re doing GraphRAG. That\u0026rsquo;s it.\nGenerally, a GraphRAG task consists of taking a user query, mapping it into a knowledge graph‚Äîusually through text embedding‚Äîand querying that knowledge graph to build a context around relevant nodes and relationships. That context is then combined with the user query and fed to an LLM to produce a final and improved answer.\nThere also seems to be trend going around that\u0026rsquo;s highlighting \u0026ldquo;context engineering\u0026rdquo; as the hot new skill in in AI‚Äîsee The rise of \u0026ldquo;context engineering\u0026rdquo; on LangChain\u0026rsquo;s blog, and The New Skill in AI is Not Prompting, It\u0026rsquo;s Context Engineering on Philipp Schmid\u0026rsquo;s blog, who is a Senior AI Relation Engineer at Google DeepMind.\nIf you want to stay on trend and learn something cool, then read bellow on how to use graphs to fetch context, and become better at \u0026ldquo;context engineering\u0026rdquo; right now!\nOur Approach # We approach GraphRAG in an unconventional way. Instead of relying on text embeddings, we add an extra step to the process, so that we can alternatively use node embeddings. I\u0026rsquo;ll explain the reason for this decision in the following paragraphs.\nWhen using text to describe nodes (text embeddings) rather than the neighboring graph structure (node embeddings), the user query, which is also text, can be embedded into the same vector space. This means that ANN (approximate nearest neighbors) is directly used to map the user query into a context subgraph‚Äîthis is not only efficient, but also open ended‚ÄîANN based on text embeddings is able to reach semantically similar text, and therefore its associated nodes, rather than just doing exact matches.\nOn the other hand, when using node embeddings, we\u0026rsquo;ll need an extra step to map named entities in the user query to nodes in the knowledge graph. Only then can we compute ANN for those nodes, working as an expansion stage that builds the context. This provides a higher specificity, since we try to exactly match the entities that the user mentions, but, more importantly, it\u0026rsquo;s actually based on the graph and its structure rather than text.\nWhen working with graphs, always ask yourself whether you could have implemented the same workflow without using a graph structure at all. If the answer is yes, then you\u0026rsquo;re not really taking advantage of your graph. To me, that\u0026rsquo;s just RAG, not GraphRAG. On the other hand, if you find yourself expanding to neighbors, computing shortest paths, clustering nodes, or ranking and selecting nodes based on centrality metrics, then you\u0026rsquo;re definitely doing GraphRAG‚Äîand a bit of network science as well.\nArchitecture # We\u0026rsquo;ll be working with music data that includes friendship relations between users in a music platform, as well as user listening profiles pointing to preferred genres and genre-tagged tracks. As such, we are assuming that the system will take a music-related prompt as input. The high level workflow of a GraphRAG system is illustrated below, and then we detail each step for our specific use case.\nStep 1: User Query # The user asks for music recommendations, either mentioning artists, tracks, or genres that they like. Step 2: Graph Retriever # The user query is passed into the Graph Retriever module, which uses an LLM to build a cypher query that matches named entities to nodes in the graph. This will describe the user taste, working as a mini user profile. Step 3: Context Assembler # Each node will have precomputed node embeddings‚Äîwe\u0026rsquo;ll describe this process later on‚Äîwhich we use to find the approximate nearest neighbors, and extend the mini user profile. For each identified node, we compute a sample of relevant paths to depict context‚Äîshortest paths between user profile nodes and neighbor nodes, and random walks from neighbor nodes. Each path is described using its nodes and properties, along with pairwise relationships and their properties, in a format that naturally fits a natural language prompt‚Äîit\u0026rsquo;s not valid cypher code, nor it\u0026rsquo;s properly structured text, but rather a semi-structured, human-readable format. Step 4: Final Answer # The context is composed into the original user prompt and a final prompt is produced. The user is presented with the final response generated using the contextualized prompt‚Äîthis answer should leverage LLM knowledge and the contextual neighborhood of the user\u0026rsquo;s taste according to the knowledge graph, to hopefully provide better or more informed recommendations. Integrating Data as a Graph # We use two Kaggle datasets to build our knowledge graph: Deezer Social Networks and Million Song Dataset + Spotify + Last.fm (MSDSL). If you want to know more about the ETL process, you can read the blog post on Data Lakehouse with dbt and DuckLake, and watch the video about it.\nIn this blog post, we assume that the required node and relationship data is already available as Apache Parquet files that are ready to load into K√πzuDB. So, we\u0026rsquo;ll focus on discussing possible integration strategies‚Äîdirect mapping or integrated mapping‚Äîalong with their pros and cons.\nDirect Mapping # From an ontological or information preservation perspective, directly mapping the entities in our two datasets to their own nodes would make sense. However, purely from a recommender system\u0026rsquo;s perspective, there isn\u0026rsquo;t a clear advantage to doing this, as long as we can identify where a user is from (e.g., via a node property or a connector node).\nKeeping separate node labels with similar semantics‚ÄîDeezer User and MSDSL User, or Genre and Tag‚Äîwould be unhelpful for the recommendation task, as it would increase complexity while offering no real advantage in exchange.\nIntegrated Mapping # Even if Deezer users will never overlap with MSDSL users, we still benefit from combining users as a single entity‚Äîand we can keep that additional information as a source property. While the Friend and Likes relationships exist only for Deezer users, and the ListenedTo and Tagged relationships exist only for MSDSL, we\u0026rsquo;re still able to recommend tracks from MSDSL based on liked genres from Deezer users, or recommend new genres to MSDSL users based on the tags of listened tracks and liked genres by Deezer users.\nThis modeling approach is also more than enough to exemplify how GraphRAG can be useful in this task, showing off how different datasets can live in a common space, and how connections are established. The more data you have, the richer your knowledge graph will be, and the more detail you can provide to your users.\nGraph Storage with K√πzuDB # For storage, we picked K√πzuDB, an embedded graph database solution that, in my opinion, promises to shake the market for the first time since Neo4j and OrientDB were fighting for their place in the community. To me, K√πzuDB feels like the DuckDB of graph databases‚Äîit\u0026rsquo;s for analytics, it has a similar extensions system and functions API, and it even lets you attach DuckDB, so they play well together.\nI spent a few days bugging the team with a few questions on Discord, after having exchanged a couple of e-mails with them a few months ago, where I tried to understand whether it was worth checking it out or not. So many graph databases have come and gone, but this is not one of them. I have only good things to say about the product and the team‚ÄîK√πzuDB feels technically sound and robust, and the team is approachable, willing to help, and pragmatic. If you\u0026rsquo;re reading this, thanks guys!\nPointers vs Partitions # Let\u0026rsquo;s delve a little into the storage scheme for K√πzuDB, putting it in context by looking at its counterpart from Neo4j. While I\u0026rsquo;m partial to Neo4j\u0026rsquo;s approach, and always look for index-free adjacency in graph databases, K√πzuDB\u0026rsquo;s columnar approach looks rock solid and ideal for batch operations, as expected from an analytical store.\nNeo4j: Pointers # OLTP graph databases like Neo4j implement index-free adjacency using a pointer-based storage layout that supports chained traversals. When reading a node, we get a pointer to one of its incident relationships. That relationship then provides two other relationship pointers, one to the next relationship from the source node, and another one to the next relationship from the target node. This produces a linked list of incident relationships, making it possible to iterate over all rels for a given node, as well as traverse the graph by following the rels for source/target nodes that match or do not match the origin node, respectively.\nTo illustrate this, let\u0026rsquo;s assume that, starting from n1, we reach (n1)-[r1]-(n2), and from there we can choose to either jump to another rel incident in n1 (origin node) or another rel incident in n2 (neighbor node). When iterating over all incident rels on n1, then we might jump to (n1)-[r2]-(n3), but if we\u0026rsquo;re traversing we\u0026rsquo;ll likely jump to another rel, such as (n2)-[r3]-(n4), which forms the path (n1)-[r1]-(n2)-[r3]-(n4). Each step in a traversal essentially costs one disk seek operation, including node-to-edge as a step, and not accounting for caching.\nK√πzuDB: Partitions # K√πzuDB approaches storage as an OLAP database, using Node-group based Storage (Issue #1474), which was inspired by the concept of row groups from columnar formats like Apache ORC or Apache Parquet.\nEach node group represents a partition with a given number of nodes or edges with multiple column chunks. A basic column chunk stores a bitmap of null values as well as the values themselves‚Äîfor string and list columns, offsets are also stored, with each value representing the length of the string or list for each value.\nA node group can also represent a column chunk of relationships (rel-lists). Each rel-lists column chunk contains a non-null column of neighbor IDs, and a header with offsets representing the number of neighbors of each node at the corresponding offset in the equivalent node group of nodes. The rel-lists column chunk can also store relationship properties in the same way as node properties are stored. Bit/byte packing and string dictionary compression techniques are also used at the column level to accelerate querying. And a metadata file is kept in memory to help find column chunks based on the node group ID and column name (that can be a rel-lists).\nWhile not index-free in the sense of pointer-chained traversals, K√πzuDB still provides an extremely efficient low-level traversal logic, with the added advantage of columnar storage, which is ideal for analytical tasks‚Äîit really is the DuckDB of graph databases.\nTo illustrate the process, let\u0026rsquo;s say that we start from n1. We lookup the corresponding node group for n1 using the metadata file (always available in memory) and, knowing the node group ID, we repeat the process for the rels-list column chunk in the equivalent node group. Through offset arithmetic we can easily obtain the node IDs for neighboring nodes, and repeat the process to keep traversing. Traversing a path like (n1)-[r1]-(n2)-[r3]-(n4) should require at most one disk read per node group, similar in principle to Neo4j\u0026rsquo;s per-hop seeks, though amortized over a batch of nodes and properties. While in Neo4j we traverse one pointer at a time, but filter in-memory, in K√πzuDB we traverse by loading node groups, being able to filter out entire rows in a single batch, and skipping decompression for irrelevant rows, or irrelevant neighbor lists in rel-lists.\nI might have missed a few details, or gotten a few of them wrong, but this is not meant as an in-depth comparison of the storage and querying approaches of Neo4j and K√πzuDB. It\u0026rsquo;s rather a testament to the effort that the K√πzuDB team has put into designing a robust storage schema that is technically sound and efficient.\nTo put this into context, the graph operations we run in this study are for a graph with over 1.15M nodes and 11.62M relationships (see Graph Statistics), which K√πzuDB easily supports on a single node with moderately-good specs (24 CPU cores, 24 GiB RAM allocated to WSL2).\nGraph Loading # If you watched Data Lakehouse with dbt and DuckLake, then you already knew that we had gone with the integrated mapping approach, and we have our node and relationship parquet files ready to load into a graph database.\nBelow we will describe the steps taken to load the nodes and edges into K√πzuDB. We implemented this in Python, under the KuzuOps class, which run the cypher queries that we describe below.\nCreate Schema # In K√πzuDB, we are required to create tables for all of our nodes and relationships. This is what our schema looks like for the Deezer and MSDSL music data. Let\u0026rsquo;s begin with nodes.\nCREATE NODE TABLE User ( node_id INT64, user_id STRING, source STRING, country STRING, PRIMARY KEY (node_id) ); CREATE NODE TABLE Genre ( node_id INT64, genre STRING, PRIMARY KEY (node_id) ); CREATE NODE TABLE Track ( node_id INT64, track_id STRING, name STRING, artist STRING, year INT16, PRIMARY KEY (node_id) ); As you can see, we defined an INT64 node_id property that is unique across all nodes, regardless of their label (i.e., there is no node_id collision for User, Genre and Track). This is set as the primary key for our nodes, which means an index is also created for node_id.\nThen, we create our relationship tables.\nCREATE REL TABLE Friend( FROM User TO User, MANY_MANY ); CREATE REL TABLE Likes( FROM User TO Genre, MANY_MANY ); CREATE REL TABLE ListenedTo( FROM User TO Track, play_count INT32, MANY_MANY ); CREATE REL TABLE Tagged( FROM Track TO Genre, MANY_MANY ); All of our relationships are MANY_TO_MANY, with ListenedTo also storing a play_count property.\nThis produces a graph schema with three node labels and four rel labels, as illustrated in Integrated Mapping.\nImport Nodes and Edges # Our node and edge parquet files are stored in S3 and, while K√πzuDB can directly read from S3, it does not support disabling SSL, so we were unable to use an S3-based workflow. In production, it\u0026rsquo;s common for SSL to be enabled, but in a lab or prototyping environment it\u0026rsquo;s not. As such, we simply downloaded each parquet file using boto3 and then ran the following COPY commands (we replace filenames with readable names, but we actually used temporary files).\nWe load nodes as follows:\nCOPY User(node_id, user_id, country, source) FROM \u0026#39;nodes/dsn_nodes_users.parquet\u0026#39;; COPY User(node_id, user_id, source) FROM \u0026#39;nodes/msdsl_nodes_users.parquet\u0026#39;; COPY Track(node_id, track_id, name, artist, year) FROM \u0026#39;nodes/msdsl_nodes_tracks.parquet\u0026#39;; COPY Genre(node_id, genre) FROM \u0026#39;nodes/nodes_genres.parquet\u0026#39;; We load relationships as follows:\nCOPY Friend FROM \u0026#39;edges/dsn_edges_friendships.parquet\u0026#39;; COPY Likes FROM \u0026#39;edges/dsn_edges_user_genres.parquet\u0026#39;; COPY ListenedTo FROM \u0026#39;edges/msdsl_edges_user_tracks.parquet\u0026#39;; COPY Tagged FROM \u0026#39;edges/msdsl_edges_track_tags.parquet\u0026#39;; Graph Statistics # After loading our graph, we computed a few basic statistics using the following query:\nMATCH (n) RETURN \u0026#34;No. \u0026#34; + label(n) + \u0026#34; Nodes\u0026#34; AS stat, count(*) AS val UNION MATCH (n) RETURN \u0026#34;Total No. Nodes\u0026#34; AS stat, count(*) AS val UNION MATCH ()-[r]-\u0026gt;() RETURN \u0026#34;No. \u0026#34; + label(r) + \u0026#34; Rels\u0026#34; AS stat, count(*) AS val UNION MATCH ()-[]-\u0026gt;() RETURN \u0026#34;Total No. Rels\u0026#34; AS stat, count(*) AS val; Statistic Value No. User Nodes 1,105,921 No. Genre Nodes 171 No. Track Nodes 50,683 Total No. Nodes 1,156,775 No. Friend Rels 846,915 No. Likes Rels 880,698 No. ListenedTo Rels 9,711,301 No. Tagged Rels 185,313 Total No. Rels 11,624,227 Computing Node Embeddings # K√πzuDB has a vector extension that supports HNSW indexing for vectors, similar to Pinecone, Weaviate, or pgvector. It supports semantic search via ANN, which we\u0026rsquo;ll use on the Graph Retriever component to establish a context based on graph paths.\nWe precompute node embeddings based on a PyTorch implementation‚Äîsee the graph.embedding module in the datalab repo. We use a simplified version of the Fast Random Projection (FastRP) algorithm, where the $L$ component corresponds directly to the node degrees, the $R$ component samples from a normal distribution, and a Multi-Layer Perceptron (MLP) is applied on top of the fixed FastRP embeddings to introduce non-linear activations, enabling the model to learn complex, non-linear mappings for downstream tasks.\nThis implementation can be run for a graph (e.g., music_taste) by calling the following command:\ndlctl graph compute embeddings \u0026#34;music_taste\u0026#34; \\ -d 256 -b 9216 -e 5 This will iterate over batches of 9216 nodes and compute embeddings of dimension 256 over 5 epochs, which are stored in the embedding property of each node. Batching is handled by the KuzuNodeBatcher class that we implement, which relies on the query_nodes_batch and query_neighbors methods from the KuzuOps class.\nThe first method implement the following cypher query for paginating nodes:\nMATCH (n) RETURN n.node_id AS node_id ORDER BY n.node_id SKIP $skip LIMIT $limit And the second method loads the source and target node IDs for all outgoing relationships starting any of the nodes in the batch:\nMATCH (n)--\u0026gt;(m) WHERE n.node_id IN CAST($nodes AS INT64[]) RETURN n.node_id AS source_id, m.node_id AS target_id ORDER BY source_id, target_id We can then create the HNSW indexes, dropping any existing indexes automatically, by running:\ndlctl graph reindex \u0026#34;music_taste\u0026#34; To make this possible, the previous command relies on show_tables(), table_info(), show_indexes(), drop_vector_index() and create_vector_index(), all of which are individually called and arguments are passed from functions like show_tables() to table_index() strictly via f-strings‚Äîsince variables are not supported with CALL, this needs to be done programmatically. Check out the full reindex_embeddings methods to learn the details.\nThe whole process takes approximately 20m to run for the music_taste graph.\nGraphRAG Chain # We use two LLM models to support our operations: phi4 and gemma3. Specifically, we rely on phi4 for tasks that involve following instructions and generating code (cypher), and we use gemma3 to produce user-facing text (the final answer). We tested gemma3 as a general model for all required tasks, replacing phi4 in code generation, but it struggled to generate valid cypher code‚Äîsyntactically and conceptually. The same goes for several other models as well‚Äîphi4 seems to produce the best outcome when it comes to code generation (pending formal evaluation, of course, but I\u0026rsquo;m vibing right now).\nWe implemented GraphRAG using LangChain, as a Runnable, taking advantage of langchain-kuzu by partially reusing some of its components, like the KUZU_GENERATION_PROMPT template, or KuzuGraph for getting a schema to feed to the prompt and to handle graph querying.\nüìå Note\nWhen we decided to implement our workflow using LangChain, we also looked into LangGraph, to determine whether it could be useful when working with Graph RAG.\nWhile LangGraph is quite an interesting framework, designed for the orchestration of stateful agents, it does not provide specific tooling for working with graphs‚Äîthink about graphs in the sense of TensorFlow computational graphs, not graphs like knowledge graphs or social networks. LangGraph supports the integration of LLMs, tools, memory, and other useful features, by settings the conditions under which these components interact.\nSince we didn\u0026rsquo;t need to setup something at this complexity level, we did not use LangGraph here. If you\u0026rsquo;re focusing on stateless Graph RAG, then LangChain is really all you need. In the future, however, we might explore LangGraph, and add state to the implementation we describe here.\nBelow you\u0026rsquo;ll find a diagram detailing the how the Graph RAG chain works, and covering the three major components: Graph Retriever, Context Assembler, and Answer Generator. The floating rectangles on top, with a filling, are the legend. Each small rectangle inside the three major components is a part of the RunnableSequence that makes the component. The code that implements the diagram is available under the graph.rag module‚Äîsymbol names in the code should match the names in the small rectangles below.\nGraph Retriever # The Graph Retriever component extracts and maps named entities to nodes in the graph. It takes the user_query as input, which will be passed to the entities_prompt via the entities_prompt_to_kuzu_inputs step.\nentities_prompt # At first, we tried a zero-shot prompt, but the only way we got a consistent outcome was by providing it an explicit example. Turning our prompt into one-shot was enough to get this to work for our specific graph. While this might not be generalizable‚Äîthe example we provide is specific to our graph‚Äîit is enough to demonstrante an end-to-end pipeline, and it is also closer to what a real-world implementations would do, which are usually designed for a specific use case. This template expects only that the {user_query} slot is replaced.\nYou are an AI assistant that extracts entities from a given user query using named entity recognition and matches them to nodes in a knowledge graph, returning the node_id properties of those nodes, and nothing more. Input: User query: a sentence or question mentioning entities to retrieve from the knowledge graph. Task: Extract all relevant entities from the user query as nodes represented by their node_id property. Rules: - Only use node properties defined in the schema. - Use exact property names and values as extracted from the user query. - If a property value is not specified, do not guess it. - Ignore user query requests, and just return the node_id property for nodes matching named entities explicitly mentioned in the user query. - Do not make recommendations. Only return the node_id properties for extracted entities that have a node in the graph. Example: If the user mentions Nirvana and there is an artist property on a Track node, then all nodes matching Nirvana should be retrieved as follows: MATCH (t:Track) WHERE LOWER(t.artist) = LOWER(\"Nirvana\") RETURN t.node_id AS node_id; If, in addition to Nirvana, the user algo mentions the grunge genre, and there is a genre property of a Genre node, then all nodes matching grunge should be added to be previous query as follows: MATCH (t:Track) WHERE LOWER(t.artist) = LOWER(\"Nirvana\") RETURN t.node_id AS node_id UNION MATCH (g:Genre) WHERE LOWER(g.genre) = LOWER(\"grunge\") RETURN g.node_id AS node_id User query: \"{user_query}\" --- Here are the node_id properties for all nodes matching the extracted entities: [Your output here] langchain-kuzu # This is a LangChain integration for K√πzuDB that helps you query the graph, automatically taking into account its schema, through this prompt (directly extracted from langchain-kuzu):\nYou are an expert in translating natural language questions into Cypher statements. You will be provided with a question and a graph schema. Use only the provided relationship types and properties in the schema to generate a Cypher statement. The Cypher statement could retrieve nodes, relationships, or both. Do not include any explanations or apologies in your responses. Do not respond to any questions that might ask anything else than for you to construct a Cypher statement. Task: Generate a Cypher statement to query a graph database. Schema: {schema} The question is: {question} This means that the final prompt that we pass to our LLM is this prompt with the schema slot replaced by the return value of graph.get_schema‚Äîfrom KuzuGraph, available after calling graph.refresh_schema()‚Äîand the {question} slot replaced by our entities_prompt prompt template.\nCode LLM and KG Query # The previously generated prompt is passed to the phi4 LLM model, running on a local instance of Ollama‚Äîthis requires 8 GiB VRAM‚Äîproducing cypher code that will match named entities, extracted from the original user query, to nodes in the knowledge graph.\nAs an example, let\u0026rsquo;s consider the following user query:\nIf I like metal artists like Metallica or Iron Maiden, but also listen to IDM, what other artists and genres could I listen to? The cypher output produced by the code LLM will look something like this:\nMATCH (g:Genre) WHERE LOWER(g.genre) = LOWER(\u0026#34;metal\u0026#34;) RETURN g.node_id AS node_id UNION MATCH (t:Track) WHERE LOWER(t.artist) = LOWER(\u0026#34;Metallica\u0026#34;) OR LOWER(t.artist) = LOWER(\u0026#34;Iron Maiden\u0026#34;) RETURN t.node_id AS node_id UNION MATCH (g:Genre) WHERE LOWER(g.genre) = LOWER(\u0026#34;idm\u0026#34;) RETURN g.node_id AS node_id By calling query_graph(shuffle=True, limit=100), we produce a runnable that will use KuzuGraph to run the generate cypher code, shuffling the output data frame and returning only the first 100 rows‚Äîif the user mentions more than 100 entities in the query, this will introduce a cap and some run time predictability.\nContext Assembler # The Context Assembler component expands source nodes to nearest neighbors, producing additional context to extend the prompt. It takes the entities data frame of nodes as input, computing nearest-neighbors and finding relevant paths to produce a context. The idea is that this context will guide the LLM into providing a better answer.\nApproximate Nearest Neighbors # We use ANN to build a context with the top $k$ nodes that are the most similar overall to our source nodes (i.e., directly representing entities mentioned in the user query‚Äîfor example, if an artist is mentioned, then the tracks for that artist will directly represent it). When a node appears multiple times, for a being NN with multiple source nodes, then the average distance is used to rank that node. Source nodes will be added to an exclusion list, so they will never be returned as a NN, and nodes are only considered up to a maximum distance, so we might return less than $k$-NN.\nComputing Relevant Paths # Once we return the combined $k$-NN, we compute two types of paths to produce a context:\nSample of shortest paths between each node representing an entity in the user query, and its nearest neighbors. Random length paths departing from nearest neighbors, as a way to profile the neighbors. Think of individual relationships in paths as having the value of sentences‚Äîe.g., \u0026lsquo;John, a Deezer user, listened to Metallica\u0026rsquo;s Nothing Else Matters track 5 times\u0026rsquo; is represented in our graph by (u:User)-[:ListenedTo]-\u0026gt;(t:Track). While we could convert this into natural language, the LLM already has the ability to understand structured languages like cypher. As such, we use a minimal semi-structured representation to list node properties, and relationships from the selected paths:\nNodes: Track(node_id=175760, track_id=TRLOVTV128F92E846E, name=Wrathchild, artist=Iron Maiden, year=1981) Track(node_id=145255, track_id=TRVBEWU128F422E551, name=The Secrets Of Merlin, artist=Grave Digger, year=1999) Genre(node_id=1156680, genre=Indie Rock) Track(node_id=188155, track_id=TRGNOEX128F4265F3E, name=Snap Your Fingers, Snap Your Neck, artist=Prong, year=2014) ... User(node_id=56121, user_id=36144, source=Deezer, country=RO) User(node_id=114060, user_id=3007, source=Deezer, country=HU) Genre(node_id=1156733, genre=Metal) Genre(node_id=1156613, genre=Thrash Metal) Relationships: ({ node_id: 175888 })-[:Tagged]-({ node_id: 1156613 }) ({ node_id: 1156613 })-[:Tagged]-({ node_id: 192487 }) ({ node_id: 113118 })-[:Likes]-({ node_id: 1156680 }) ... ({ node_id: 175760 })-[:ListenedTo {play_count=3}]-({ node_id: 200466 }) ({ node_id: 175748 })-[:Tagged]-({ node_id: 1156613 }) ({ node_id: 1156733 })-[:Likes]-({ node_id: 122589 }) ... ({ node_id: 53808 })-[:Friend]-({ node_id: 114060 }) ... Answer Generation # The text output from the previous component will be fed to the chat LLM, as context within its prompt, and a final answer will be produced by the gemma3 model running on the local instance of Ollama:\nBased on your interest in metal artists like Metallica and Iron Maiden, as well as your enjoyment of IDM (Intelligent Dance Music), you might appreciate exploring a blend of genres that incorporate complex rhythms, intricate compositions, and experimental sounds. Here are some suggestions for artists and genres to explore: #### Artists: 1. **Meshuggah**: Known for their technical proficiency and polyrhythms, Meshuggah blends elements of metal with progressive and avant-garde influences. 2. **Opeth**: This band combines death metal with progressive rock and folk music, offering complex compositions that might appeal to your taste in IDM. 3. **Gojira**: With a focus on environmental themes and technical musicianship, Gojira's sound is both heavy and intricate. 4. **Tool**: Known for their complex rhythms and philosophical lyrics, Tool blends metal with progressive rock and experimental elements. 5. **Cynic**: A band that merges death metal with jazz fusion and ambient music, creating a unique and cerebral listening experience. #### Genres: 1. **Progressive Metal**: This genre often features complex song structures, time signature changes, and intricate instrumental work, similar to the complexity found in IDM. 2. **Technical Death Metal**: Known for its fast tempos, complex guitar riffs, and precise drumming, this subgenre offers a challenging listening experience. 3. **Avant-Garde Metal**: This genre pushes the boundaries of traditional metal with experimental sounds and unconventional song structures. 4. **Math Rock**: Characterized by odd time signatures and syncopated rhythms, math rock shares some similarities with IDM in terms of complexity and structure. #### Additional Recommendations: - **Djent**: A subgenre of progressive metal known for its heavy use of palm-muted, low-pitched guitar riffs and complex rhythms. - **Neoclassical Metal**: Combines elements of classical music with metal, often featuring virtuosic guitar solos and orchestral arrangements. Exploring these artists and genres can provide a rich listening experience that bridges your interests in metal and IDM. And that\u0026rsquo;s it! There\u0026rsquo;s one way to do GraphRAG while actually taking advantage of the graph structure, despite its lower efficiency.\nFinal Remarks # Keep in mind that this was an experiment! For production, I would still rely on text embeddings for to compute node vectors. This will enable us to directly embed a user query into the same vector space, and match it to nodes in the knowledge graph, while seamlessly expanding the context to nodes other than the named entities‚Äîwe match semantically similar nodes rather than the exact entities mentioned in the user query It\u0026rsquo;s a lot less overhead, if we rely purely on text.\nWhen running graph algorithms based on the NN nodes is required‚Äîe.g., traversals or clustering that dynamically depend on those source nodes‚Äîthen stick with GraphRAG. When you simply don\u0026rsquo;t need to run computations on the graph, then just use plain RAG based on a vector store, although we K√πzuDB you essentially get both.\nK√πzuDB Consumer Notes # To close this study, I\u0026rsquo;ll share a few of my notes, that I took as I was working on this project, starting with a few quick highlights on data types and functions, and then providing: a wishlist, with features I missed or would like K√πzuDB to support; a snags list, with small annoyances that I hit during development; and a bugs list, with a few issues I found, some of them already resolved, while others that might require further investigation.\nData Types and Functions # K√πzuDB provides several specialized data types and functions, some of them similar to DuckDB, while others unique to kuzu or specific to graphs, Below, I list some of my favorites, as I went through the docs:\nLists ‚Äì with arrays being a special case of lists. Arrays ‚Äì focusing on vector distance and similarity functions. Maps and Structs ‚Äì maps/dictionaries, with added access options via struct operators (i.e., my_map.my_field). Recursive Relationships ‚Äì node, edge and property accessors for paths, but also functions to test the trail or acyclic properties of paths, or to measure length and cost. Text ‚Äì several standard text utilities (e.g., initcap for title case), also provides a levenshtein implementation for string similarity. Unions ‚Äì makes it possible for multiple data types to be stored in the same column. Wishlist # Integration with dbt üôèüèª ‚Äî there is already a dbt-duckdb, so this would fit nicely with the workflow (DuckDB for ETL and analytics, and KuzuDB for graph data science). No support for S3_USE_SSL, which makes it hard to access object storage setup for testing (had to create a boto3 wrapper based on temporary files). In kuzu CLI, a query terminating with ; should always run when RETURN is pressed, like DuckDB does, even in :multiline mode. And :singleline mode should actually be single line‚Äîpasting might convert new lines to spaces. Alternatively, there could be a secondary shortcut, like Shift+Enter, that would run the query regardless of where the cursor is. Having better schema introspection for edge tables with table_info() would be useful‚Äîspecifically, showing information about FROM and TO. Calling functions using variables and parameters instead of just literals would also be appreciated, e.g., table_info(). Altering a column\u0026rsquo;s type is essential, for a schema-full graph database‚Äîa practical example was when creating a vector index over a DOUBLE[], without a fixed dimension, and it wouldn\u0026rsquo;t work. It would be nice if K√πzuDB Explorer supported multitenancy‚Äîe.g., I\u0026rsquo;d like to be able to provide a read-only graph catalog to my data scientists. A catalog of node embedding algorithms would be appreciated as well‚ÄîI had to implement my own Fast Random Projection. I didn\u0026rsquo;t use PyG, but considered it‚Äîsomething to look forward to, in the future, for sure. A rand() cypher function and a random walk algorithm, maybe using pattern matching‚Äîboth are essential for sampling the graph in different ways. Random walks need to be efficient, so implementing this using cypher, in python, is not the best idea, unless absolutely necessary. This is a fundamental feature for an OLAP graph database! Snags # While having NODE and REL tables with a fixed schema is interesting, this also limits operations a bit. For example, we cannot create an embeddings index over nodes of different types, as K√πzuDB does not support multi-labelled nodes. Having to create individual indexes still works, but it makes it harder to generalize APIs‚Äîe.g., in Neo4j I\u0026rsquo;d use something like a base node label where all nodes of this type were expected to have a specific set of properties, like embedding‚Äîbut, on Kuzu, I\u0026rsquo;ll need to know all relevant node types and also create and query $N$ separate vector indexes, one for each node type. \u0026ldquo;Current Pipeline Progress\u0026rdquo; stuck at 0% until completed for many cases‚Äîtested in K√πzuDB Explorer, e.g., CALL create_vector_index(...). This is not as useful as it could be‚Äîit needs more granularity. Creating vector indexes requires fixed size arrays, created with DOUBLE[n] or FLOAT[n], but this is not clear from the docs. Spent a couple of hours computing embeddings only to find they couldn\u0026rsquo;t be indexed‚Äîalso, the error wasn\u0026rsquo;t clear, telling me it only worked with DOUBLE[] or FLOAT[], which I was using, but not with fixed size. Functions like show_indexes() have column names with a space‚Äîwhy not just use an underscore, instead of making me use backticks? There is no real support for subqueries‚Äîsomething like CALL { ... } in Neo4j. Bugs # When SKIP is over the maximum number of records, the query will never return and a CPU core will be maxed out ad aeternum. There was a bug with COPY, when running twice for the same table, with different data. This was reported via Discord, and the team promptly confirmed it was in fact a bug and fixed it (PR #5534). When creating a vector index, then dropping it and recreating, it will sometimes fail with what seems to be an upper/lower-case related bug, e.g.: Error: Binder exception: _1_genre_embedding_idx_UPPER already exists in catalog According to one of the devs, a workaround is to run CALL enable_internal_catalog=true; and then manually drop any tables that should have been dropped along with the index. This unblocked me, but the issue was also reported and should be fixed in a future version‚ÄîI was using 0.10.0. For UNION queries, the final ORDER statement should affect the global result, not just the last statement‚Äîeither that, or add support for subqueries. ","date":"15 July 2025","externalUrl":null,"permalink":"/posts/graphrag-with-kuzudb/","section":"","summary":"Summary # In this video, I\u0026rsquo;ll delve into GraphRAG, learning about K√πzuDB, node embeddings vs text embeddings, and LangChain, running on top of Ollama with phi4 and gemma3.","title":"GraphRAG with K√πzuDB","type":"posts"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/kuzudb/","section":"Tags","summary":"","title":"Kuzudb","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/langchain/","section":"Tags","summary":"","title":"Langchain","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/neo4j/","section":"Tags","summary":"","title":"Neo4j","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/phi4/","section":"Tags","summary":"","title":"Phi4","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"Rag","type":"tags"},{"content":" Summary # Did you know you can rely on conventional commits, and a simple git branching workflow, to automate semantic releases for your Python projects, using GitHub Actions?\nConventional commits can help you standardize your commit messages and improve the readability of your git history, but they can also be used to automate releases by bumping up version components (MAJOR.MINOR.PATCH) based on the content of your commit messages, since the previous release, as identified by a git tag (e.g., v0.1.0). This is a must-have for any self-respecting developer, who maintains an evolving codebase, even when solo coding.\nRead below, if you want to know how to \u0026lsquo;set it and forget\u0026rsquo;, adding automated semantic releases to your git repo, so you won\u0026rsquo;t ever have to handle it yourself, or manually trigger a workflow to do a release.\n\u003e Conventional Commits # Conventional commits is \u0026ldquo;a specification for adding human and machine readable meaning to commit messages\u0026rdquo;. This means that it will, not only, make it easier for you and other coders to navigate through your repo history, but also enable software tools to advantage of this standard‚Äîthis is what makes semantic release possible.\nIn the table below, there is an overview of conventional commits, based on message types from the Angular convention, which is widely adopted‚Äîused by commitlint, Dependabot, etc. I also describe my personal logic for using each message type, and what version component it affects.\nThe conventional commits spec provides a well-defined template for commit messages, but, at the same time, it\u0026rsquo;s quite loose regarding which message types to use, relying mostly on the Angular convention, softly introduced in the spec as an example tied to commitlint. This is why I believe further comment is required, and I would go as far as to proposing the next revision to conventional commits should include a consensual list of valid, non-overlapping, message types.\nCheck out the following table for my comments on this.\nType Usage Version \u0026lt;type\u0026gt;! The most common trigger for breaking changes and major releases is feat!. I usually don\u0026rsquo;t use BREAKING CHANGE in the message body, since I might want to do a MAJOR bump up regardless of whether there is a breaking change (e.g., when reaching the next maturity level).\nIt\u0026rsquo;s less likely that other types will introduce breaking changes, but it can still happen. For example, a chore!(deps) might update a dependency that changed its output file format and provides no backward compatibility from that point on.\nI know I\u0026rsquo;m being opinionated, but, personally, I avoid breaking changes at all cost in my software. This is why I break convention here. I prefer the idea of \u0026lsquo;maturity level\u0026rsquo; to \u0026lsquo;breaking change\u0026rsquo;, and maybe we should discuss this for future versions of the conventional commits spec. MAJOR feat This is just a regular feature, large or small, added to your software.\nWe avoid constant version bump ups by not having a rolling release here (i.e., avoid using only the main branch‚Äîsee bellow for our proposed minimal git branching workflow. MINOR fix This represents a bug fix‚Äîthat\u0026rsquo;s what patches mostly are.\nSometimes we might consider a change in coding logic to be a kind of fix, but that\u0026rsquo;s a refactor, not a fix. Be critical about it. PATCH perf Performance improvements are tagged with perf, and these do represent a patch‚Äîrightfully so, in my opinion.\nThis is also the default for python-semantic-release when commit_parser is set to conventional (also the default). PATCH chore This is not a part of the Angular convention, but it\u0026rsquo;s still supported by commitlint, and I prefer to use it with a scope instead of the build type‚Äîe.g., chore(dep). none refactor Should be strictly used to signal code rewritings that change nothing besides structure or naming (i.e., code might be reorganized into functions, separate modules, etc., and these might be renamed, but no fixes, features, or performance improvements are to be added in a refactor).\nI use this a lot, as I tend to begin with a single file per Python package, and then refactor it into multiple modules (i.e., separate files) as the code grows.\nIn the course of a refactor, if I find a bug, I usually commit all other files first with refactor and then commit that specific file using fix . Similarly, I give priority to feat over fix. This avoids having commits with multiple message types, which break the convention. It\u0026rsquo;s best to avoid this workflow altogether, as much as possible, but, you know, stuff happens. none style Any purely cosmetic changes, like reformatting or theming, should be tagged with the style type.\nI rarely use this, as I tend to just enable automatic formatting, on save, with black. An obvious application of style in this context might include switching from black to another formatting approach, or switching to lower case SQL strings, when you had been using upper case before. It might also include fringe cases, like mixing different line endings by mistake, when coding in a different OS. It should also be used when visually changing components, for example based on CSS. none test Anything done on tests is tagged with test.\nRegardless of whether the commit is for a new test, a bug fix inside a test, a refactoring within tests, I always use this tag for my tests-related commits. none ci Used when modifying GitHub Actions and similar workflows. This includes any CI/CD configuration files.\nI mostly use it when changing files inside the .github directory, as I haven\u0026rsquo;t found any other case for my personal repos yet. I find it awkward that the type isn\u0026rsquo;t named with CD as well, but maybe I\u0026rsquo;m missing something‚Äîwhy only CI? none docs Any update to documentation, be it files like README.md or docstrings inside your code, should be tagged docs.\nAnother use case is when you produce web pages with the documentation for your project (e.g., building and updating the /docs directory that\u0026rsquo;s being served as GitHub Pages). none build It can be used to signal any change to the build system, including dependencies, or changes to a docker image.\nI personally don\u0026rsquo;t use it all, preferring chore instead‚Äîe.g., chore(deps) or chore(docker). What other non-build-related chores would you have? It\u0026rsquo;s just too tiny of a use case to consider using another type, in my view. none revert When you run git revert, make sure to edit the message so that it uses the revert type. This is not the default, and I\u0026rsquo;ve broken this rule a few times, but I will stick to it in the future.\nA revert might also imply a rollback in version, but this usually requires manual input. Since it\u0026rsquo;s not automated, I consider that the version is unaffected for revert commits. none Git Branching Workflow # In order to avoid a constant trigger of the semantic release process, we bundle all commits, to be considered for a potential release, in a separate branch called dev. Then, we use merge commits to merge to main. The process is simple and it looks something like this:\nIn a scenario where multiple contributors exist, it might make sense to use multiple feat/* branches as well, that would merge to dev. For our particular case, however, keeping only two branches also enables us to keep a tidy git history, without loss of information‚Äîif we used squash merging, we\u0026rsquo;d lose individual commits, and, if we used rebase merging, we\u0026rsquo;d lose the branch of origin instead.\nSemantic Release # Below, you\u0026rsquo;ll find a few examples of how the proposed git branching workflow will affect version updates when using semantic release.\nMINOR Release # The following diagram shows an example of a MINOR version bump up. We branch from main into dev and create three commits, a feat, a fix, and a docs update. We then merge to main and a semantic release happens, bumping up the version from v0.1.0 to v0.2.0. Out of the three commits, only feat or fix could influence version bump up, but feat takes precedence over fix, so we only bump up the MINOR.\ngitGraph commit id: \" \" commit id: \"0.1.0\" tag: \"v0.1.0\" branch dev checkout dev commit id: \"feat: tiny feature\" type: HIGHLIGHT commit id: \"fix: small bug\" commit id: \"docs: update readme\" checkout main merge dev commit id: \"0.2.0\" tag: \"v0.2.0\" commit id: \" \" PATCH Release # Below you\u0026rsquo;ll find two examples, one where there is no version bump up, and another one where PATCH is bumped up. The first merge to main includes only two commits with types chore and docs‚Äîthese do not trigger a version update (see the table above). Then, we keep working on dev and create a fix commit, with no other commits‚Äîthis triggers a PATCH bump up from v0.2.0 to v0.2.1.\ngitGraph commit id: \" \" commit id: \"0.2.0\" tag: \"v0.2.0\" branch dev checkout dev commit id: \"chore(deps): bump up\" commit id: \"docs: install instructions\" checkout main merge dev checkout dev commit id: \"fix(graph.ops): embedder\" type: HIGHLIGHT checkout main merge dev commit id: \"0.2.1\" tag: \"v0.2.1\" commit id: \" \" MAJOR Release # Finally, we have an example of a MAJOR bump up, which, according to conventional commits, should only happen when there is a breaking change. In the diagram below, this is identified by feat!, with BREAKING CHANGE being merely decorative, as it\u0026rsquo;s not a part of the message body‚Äîwere we to remove the ! and a MINOR bump up would occur instead.\ngitGraph commit id: \" \" commit id: \"0.2.1\" tag: \"v0.2.1\" branch dev checkout dev commit commit commit id: \"feat!: BREAKING CHANGE\" type: HIGHLIGHT commit checkout main merge dev commit id: \"1.0.0\" tag: \"v1.0.0\" commit id: \" \" Release Automation # We use python-semantic-release to automate version tagging and release creation on GitHub. This tool will decide, based on your git log, which part of the semantic version to bump up‚ÄîMAJOR.MINOR.PATCH‚Äîas long as you follow conventional commits.\nGitHub Actions # We automate semantic releases using GitHub Actions, based on a very simple workflow that we break down next.\nThe following block creates a Git Workflow named \u0026ldquo;Release\u0026rdquo; that activates on push to main, which means that, once we merge dev into main, it will be triggered. Since the workflow will need to create a commit, push it to main, and tag it with the correct version (e.g., v0.2.0), we also need to give it write permissions‚Äîthis is a better approach than setting \u0026ldquo;Read and write permissions\u0026rdquo; for all actions on your GitHub repo.\nname: Release on: push: branches: - main permissions: contents: write jobs: release: runs-on: ubuntu-latest steps: ... The first step on the release job is to checkout the repo with fetch-depth: 0 to make sure that all history will be fetched, including previous tags, which are factored into the release logic.\n- uses: actions/checkout@v4 with: fetch-depth: 0 Then, we setup uv, to help ups run semantic-release via uvx. We could have, instead, used the official GHA provided as part of python-semantic-release, but, since that required a Docker image to be built on each run, we decided to use this simpler and faster approach instead.\n- name: Set up uv and python uses: astral-sh/setup-uv@v6 with: python-version: 3.13 We define a shell function sr that call semantic-release and then run version and publish. When version is run, it updates the CHANGELOG.md and the project version under pyproject.toml, commits these changes, and tags the commit (e.g., v0.2.0). Then we run publish, which pushes the changes to the repo‚Äîthus requiring GH_TOKEN‚Äîand creates a GitHub Release adding the appropriate section from CHANGELOG.md to the release description, along with zip and tar.gz archives containing the source code for the release version.\n- name: Run semantic-release env: GH_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: | #shell sr() { uvx --from=\u0026#34;python-semantic-release@9.21.1\u0026#34; \\ semantic-release \u0026#34;$@\u0026#34; } sr version sr publish pyproject.toml # This is the configuration we\u0026rsquo;re using under pyproject.toml:\n[tool.semantic_release] commit_parser = \u0026#34;conventional\u0026#34; version_toml = [\u0026#34;pyproject.toml:project.version\u0026#34;] allow_zero_version = true [tool.semantic_release.changelog.default_templates] changelog_file = \u0026#34;CHANGELOG.md\u0026#34; In here, commit_parser will be set to work with conventional commits, version_toml ensures that the release version under pyproject.toml is bumped up, and allow_zero_version will produce versions starting from 0.1.0 rather than 1.0.0. While we were also explicit with changelog_file, this was already the default.\nRollback Strategy # Finally, if, for some reason, you need to rollback a release, you can use the following workflow.\nPrepare dev # First, switch to dev and make sure it contains all changes from main:\ngit merge-base --is-ancestor main dev \u0026amp;\u0026amp; \\ echo \u0026#34;OK\u0026#34; || \\ echo \u0026#34;NOT OK\u0026#34; If it doesn\u0026rsquo;t (NOT OK), then rebase main‚Äîwith the git branching workflow we\u0026rsquo;re following, there should be no conflicts, specially if you had just merged dev into main:\ngit rebase main Revert Commit # Once dev is synced up with main, let\u0026rsquo;s revert the automatic commit generated by semantic-release (e.g., undo v0.2.0). Don\u0026rsquo;t forget to edit the commit message, when the editor opens (default behavior), and use revert:¬†\u0026hellip; to comply with conventional commits.\ngit revert v0.2.0 git push Delete Tag # Then, we\u0026rsquo;ll need to delete the v0.2.0 tag:\ngit tag -d v0.2.0 git push origin :refs/tags/v0.2.0 Delete Release # And finally just go into the GitHub release that you want to undo and press the delete button on the UI to remove it:\n","date":"8 July 2025","externalUrl":null,"permalink":"/posts/automated-semantic-releases/","section":"","summary":"Summary # Did you know you can rely on conventional commits, and a simple git branching workflow, to automate semantic releases for your Python projects, using GitHub Actions?","title":"Automated Semantic Releases on GitHub","type":"posts"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/conventional-commits/","section":"Tags","summary":"","title":"Conventional-Commits","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/git-branching/","section":"Tags","summary":"","title":"Git-Branching","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/github-actions/","section":"Tags","summary":"","title":"Github-Actions","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/semantic-releases/","section":"Tags","summary":"","title":"Semantic-Releases","type":"tags"},{"content":" Summary # Static sites, by definition, don\u0026rsquo;t have a backend, but you can still automate a lot of your workflow using GitHub Actions. Read below if you want to learn how to setup your GitHub repo for managing and deploying a Hugo static site. You\u0026rsquo;ll also learn how to schedule blog posts, so they go online at a later date without the need for any manual action. And you\u0026rsquo;ll learn how to use RSS and GitHub Actions to automate social media posting for Bluesky, Reddit, and Discord‚Äîyou can easily add more options yourself, with a little Python coding.\n\u003e Deploying a Static Site on GitHub # A static website doesn\u0026rsquo;t have a backend, so, in theory, you wouldn\u0026rsquo;t be able to schedule posts. However, there is a way to circumvent this. Hosting a static site on GitHub can be done for free, if you create a repo named \u0026lt;username\u0026gt;.github.io. Once you do this, whatever you drop into the main branch of your repo will be published as a website on that page.\nLet\u0026rsquo;s use Hugo as an example for a static website generator. You have a few options, to manage the source code for Hugo.\nSource and Target Code: Repos vs Branches # You can either create a separate repository, where public/ is added to .gitignore, and then you just setup public/ as the \u0026lt;username\u0026gt;.github.io repo, by either keeping track of individual changes manually with proper commits, or by re-initializing and force pushing each time:\ncd public/ rm -rf .git/ git init -b main git config core.sshCommand \u0026#34;ssh -i ~/.ssh/\u0026lt;gh-key\u0026gt;\u0026#34; git config user.name \u0026#34;Your Name\u0026#34; git config user.email \u0026#34;your@email\u0026#34; git add . git commit -m \u0026#34;chore: local build deployment\u0026#34; git remote add origin \u0026lt;pages-repo-url\u0026gt; git push --force origin main:gh-pages Or, you can use the \u0026lt;username\u0026gt;.github.io repo for everything, by setting a separate branch for your compiled website (usually gh-pages), under Settings ‚Üí Pages ‚Üí Branch.\nPersonally, I use the second option, either deploying via a Makefile or, more often, via GitHub Actions (GHA). This is where you can also set a custom domain, if you have one.\nScheduling Hugo Blog Posts # When you create a blog post in Hugo, there are three relevant front matter flags you can set: draft, date, and expiryDate. If you set draft to true, use a date in the future, or an expiryDate in the past, then the post won\u0026rsquo;t be compiled when running hugo without arguments.\nYou still can compile drafts with the -D flag, future content with the -F flag, or expired content with the -E flag. In my Makefile, I\u0026rsquo;ve got a few rules so I don\u0026rsquo;t forget it:\nbuild: hugo --minify dev: hugo server -DF -b http://localhost:1313 future: hugo server -F -b http://localhost:1313 preview: hugo server -b http://localhost:1313 I use make dev when I\u0026rsquo;m working on a post, make future to confirm scheduled posts, and make preview to view the website as it will be published now.\nBuild and Deploy via GitHub Actions # For public repos, GitHub provides free and unlimited usage for a few runners, out of which ubuntu-latest for x64 is included, providing 4 CPU cores, 16 GB of RAM, and 14 GB of storage. This is perfect to help us manage a few simple tasks, such as compiling and deploying a Hugo static site.\nWe can use future dates to schedule posts, as long as we rebuild the website, after that date, but doing this manually is not that much of a scheduling‚ÄîI\u0026rsquo;d rather just unflag from draft. This is where GitHub Actions come in:\nname: Hugo Rebuild Weekly on: schedule: - cron: \u0026#39;15 11 * * 2\u0026#39; workflow_dispatch: jobs: build-deploy: if: github.ref != \u0026#39;refs/heads/gh-pages\u0026#39; runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 with: submodules: true - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.111.3\u0026#39; - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_branch: gh-pages publish_dir: ./public Let\u0026rsquo;s break it down, step by step.\nWhen using GHAs, you can set it to activate manually, on push, and/or on a schedule. I avoid push, since I don\u0026rsquo;t want my website to be published as I\u0026rsquo;m working on a blog post, or changing the website design, before the content is ready. Personally, I set it to run on a schedule, a few minutes after my weekly YouTube video release, which is also when I publish a companion blog post for it:\nname: Hugo Rebuild Weekly on: schedule: - cron: \u0026#39;15 11 * * 2\u0026#39; workflow_dispatch: You can keep other branches besides main, and work on those, without ever worrying about that version being published automatically, as schedule only triggers for the default branch (usually main). You can, however, manually trigger the workflow for any branch. This is why we setup a single job that can be triggered for any branch that is not gh-pages:\njobs: build-deploy: if: github.ref != \u0026#39;refs/heads/gh-pages\u0026#39; runs-on: ubuntu-latest steps: ... Let\u0026rsquo;s now go through each step. First, we checkout the repo into the root of the runner, including submodules, which is required if you added your theme as a submodule, as is frequently the case:\n- uses: actions/checkout@v3 with: submodules: true Then, we install Hugo into the runner:\n- name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.111.3\u0026#39; And we compile the Hugo source code on the root of our repo:\n- name: Build run: hugo --minify Finally, we use peaceiris/actions-gh-pages to help us deploy the contents of public/ to our gh-pages branch:\n- name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_branch: gh-pages publish_dir: ./public In order for this last step to work, we are required to go into Settings ‚Üí Actions ‚Üí General ‚Üí Workflow permissions and switch to \u0026ldquo;Read and write permissions\u0026rdquo;. This will let the actions for this repo write into the gh-pages branch, which is required for deploying a new version of the website.\nOnce this is setup, you can set your cron date:\non: schedule: - cron: \u0026#39;15 11 * * 2\u0026#39; And this action will run for the default branch (most likely main) whenever the date matches. You can use Crontab.guru to help you configure the date, which can be set with expressions like @weekly, @daily or @hourly as well.\nOn the set date, a new runner will be created, the repo\u0026rsquo;s main branch checked out, hugo will run, and the output produce in public/ will replace your gh-pages content. If a post scheduled for a future date is now in the past, it will be rendered automatically.\nAutomating Social Media Posts with RSS # Once you\u0026rsquo;ve got your static site running, it will usually provide an RSS feed with the latest posts. Hugo does this by default, even if you or your theme don\u0026rsquo;t provide a custom feed template. Each directory under content will contain its own index.xml.\nFor example, for my website, you\u0026rsquo;ll find one in the root, which includes all pages, even the privacy and cookie policies:\nhttps://datalabtechtv.com/index.xml Another one under /posts, which is the main RSS feed:\nhttps://datalabtechtv.com/posts/index.xml And even one per category (e.g., Data Engineering):\nhttps://datalabtechtv.com/categories/data-engineering/index.xml There are a few GitHub Actions on the Marketplace to handle RSS (e.g., Feed to Bluesky), but I decided to create my own repo and workflow, based on a custom Python script that publishes to Bluesky, Reddit and Discord at the same time.\nLet\u0026rsquo;s go through the workflow first, and then through the Python script.\nGitHub Actions \u0026amp; Secrets and Variables # The overall strategy for the GitHub Actions workflow consisted of:\nInstalling uv and creating a virtual environment with all Python dependencies. Running a Python script to read RSS from my static site and produce social media posts from it, posting to my socials (for now, Bluesky, Reddit, and Discord). Storing the last run dates (one per feed) directly on the repo, writing to the .last_runs.json file, committing and pushing. Step 3 will be used as persistence so that, when there are no new RSS articles, nothing will be published to social media.\nI also added the option to force post the latest n articles, when desired. I used this during development for testing, but it can also be used after you change your static site content and delete old social media posts, so that you can repost simultaneously to all your socials via manual trigger.\nThe action is setup to trigger weekly, 15 minutes after my static site is recompiled:\nname: Post from RSS env: LAST_RUNS_PATH: .last_runs.json on: schedule: - cron: \u0026#39;30 11 * * 2\u0026#39; workflow_dispatch: inputs: force_latest: description: force_latest type: number default: 0 You can customize the path where the last run dates will be stored via LAST_RUNS_PATH and, as you can see, we also provide a force_latest input, which defaults to zero (no forcing), that can optionally be set to a positive integer when manually running this workflow.\nOur workflow then has five steps. First, we checkout the repo, install uv and run uv sync to install required Python dependencies:\n- name: Checkout repo uses: actions/checkout@v4 - name: Set up uv and Python uses: astral-sh/setup-uv@v6 - name: Create venv and install dependencies run: uv sync Then we run our custom Python script, which is largely configured via environment variables that we set under Settings ‚Üí Secrets and Variables ‚Üí Actions.\nWe run the script and set all environment variables as follows:\n- name: Run RSS to Social env: FORCE_LATEST: ${{ github.event.inputs.force_latest }} RSS_FEED_URLS: ${{ vars.RSS_FEED_URLS }} ACTIVE_SOCIALS: ${{ vars.ACTIVE_SOCIALS }} BSKY_USERNAME: ${{ vars.BSKY_USERNAME }} BSKY_PASSWORD: ${{ secrets.BSKY_PASSWORD }} REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }} REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }} REDDIT_USERNAME: ${{ vars.REDDIT_USERNAME }} REDDIT_PASSWORD: ${{ secrets.REDDIT_PASSWORD }} REDDIT_SUBREDDIT: ${{ vars.REDDIT_SUBREDDIT }} DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }} run: uv run rss_to_social.py Any variables set under env will also be available to the script automatically (only LAST_RUNS_PATH in our case).\nHere\u0026rsquo;s a summary table for the required environment variables:\nEnvironment Variable Description Secret? FORCE_LATEST Number of RSS articles to force post to social media, even their date is older than the last run. ‚ùå LAST_RUNS_PATH Path for the JSON file that stores last runs dates per feed as an object. It must be relative to the repo\u0026rsquo;s root (e.g., .runs/last_runs.json. ‚ùå RSS_FEED_URLS New line separated feed URLs (both RSS and Atom are supported). We use new lines, because it\u0026rsquo;s easy to setup in GitHub vars and secrets, and it improves readability. ‚ùå ACTIVE_SOCIALS Only socials listed here will be posted to when the action runs. Values are new line separated and support: bluesky, reddit, and discord. ‚ùå BSKY_USERNAME The username for the Bluesky account you want to post to (e.g., datalabtechtv.bsky.social or datalabtechtv.com). ‚ùå BSKY_PASSWORD The password for the Bluesky account you want to post to. ‚úÖ REDDIT_CLIENT_ID The client ID for your Reddit app (you can create one by vising https://www.reddit.com/prefs/apps/. ‚úÖ REDDIT_CLIENT_SECRET The client secret for your Reddit app. ‚úÖ REDDIT_USERNAME The username for the Reddit account that will be posting. We recommend using a dedicated bot account (e.g., we use DataLabTechBot). ‚ùå REDDIT_PASSWORD The password for the Reddit account that will be posting. ‚úÖ REDDIT_SUBREDDIT The name of the subreddit where you\u0026rsquo;ll be posting to (e.g., we post to DataLabTechTV). ‚ùå DISCORD_WEBHOOK The Discord webhook URL for a given channel. This can be created by going into Edit Channel ‚Üí Integrations ‚Üí Webhooks ‚Üí Create Webhook. ‚úÖ The script also takes a --force-latest command line argument, which takes the value from the workflow input we set. Posts will be filtered based on the last run date and published to social media, and a new .last_runs.json file will be produced, committed and pushed:\n- name: Commit updated ${{ env.LAST_RUNS_PATH }} if changed run: | git config user.name \u0026#34;github-actions\u0026#34; git config user.email \u0026#34;github-actions@github.com\u0026#34; git add $LAST_RUNS_PATH git diff --cached --quiet \\ || git commit -m \u0026#34;Update $LAST_RUNS_PATH\u0026#34; continue-on-error: true - name: Push changes if: success() uses: ad-m/github-push-action@v0.8.0 with: github_token: ${{ secrets.GITHUB_TOKEN }} Make sure to set \u0026ldquo;Read and write permissions\u0026rdquo; under Settings ‚Üí Actions ‚Üí General ‚Üí Workflow, so that the .last_runs.json file can be committed.\nPython Script: RSS to Socials # The following flowchart provides an overview of how the rss_to_social.py script works.\nflowchart TB LR[Last Runs] \u0026 FL[Force Latest] \u0026 FU[Feed URLs] \u0026 AS[Active Socials] --\u003e NE subgraph Each Feed NE[Determine New Entries] --\u003e CP[Create Post] subgraph Each New Entry CP --\u003e PB \u0026 PR \u0026 PD subgraph Post Socials PB[Bluesky] PR[Reddit] PD[Discord] end end end It first loads a dictionary of last run dates per feed from the file pointed by LAST_RUNS_PATH, as well as the values for the following environment variables:\nFORCE_LATEST ‚Äì number of recent articles to force post (defaults to zero) RSS_FEED_URLS ‚Äì one feed URL per line (usually a single feed per site). ACTIVE_SOCIALS ‚Äì one social media platform name per line (can be bluesky, reddit, and/or discord). It then determines the new (or forced) entries to be posted. An entry will be considered when any of the following conditions apply:\nThere is no last run for the feed URL‚Äîall entries will be posted. The entry is one of the most recent, up to FORCE_LATEST. The entry\u0026rsquo;s date is older than the last run date. Each entry is then transformed into a Post, with a title, description (from the entry summary), link, and, if available, an image_path pointing to a temporary file with the downloaded image from the entry\u0026rsquo;s media content, along with an image_alt.\nFinally, for each of the ACTIVE_SOCIALS, we publish a post. For Bluesky, we use atproto, for Reddit, we use PRAW, and, for Discord, we just use requests.\nFinal Remarks # In the future, I might turn this into a reusable GitHub Action, but, for now, feel free to fork my repo at DataLabTechTV/rss-to-social and adapt it to your own needs. Remember that you\u0026rsquo;ll need to setup your own \u0026ldquo;Secrets and variables\u0026rdquo; and enable \u0026ldquo;Read and write permissions\u0026rdquo; for GHA on your repo.\nIf you like content about all things data, including DevOps and other tangents, such as this one, make sure to subscribe to my YouTube channel at @DataLabTechTV!\n","date":"1 July 2025","externalUrl":null,"permalink":"/posts/automate-blog-and-social-media/","section":"","summary":"Summary # Static sites, by definition, don\u0026rsquo;t have a backend, but you can still automate a lot of your workflow using GitHub Actions.","title":"Automating Hugo Blog and Social Media with GitHub Actions","type":"posts"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/rss/","section":"Tags","summary":"","title":"Rss","type":"tags"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/scheduling/","section":"Tags","summary":"","title":"Scheduling","type":"tags"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/static-site/","section":"Tags","summary":"","title":"Static-Site","type":"tags"},{"content":" Summary # Now that you can use DuckDB to power your data lakehouse through DuckLake, you\u0026rsquo;ll also save space on snapshots due to the ability to reference parts of parquet files (yes, you can keep all old versions, with little impact to storage), and you\u0026rsquo;ll get improved performance for small change operations due to data inlining, which lets data be stored directly within the metadata database (SQLite, PostgreSQL, etc.).\nWith a little help from dbt and an unreleased branch of dbt-duckdb adapter, we were able to design a data lakehouse strategy, covering data ingestion, transformation, and exporting, almost exclusively based on SQL!\nThis project is available as open source on GitHub, at DataLabTechTV/datalab, and the README will cover most of the details you need to understand it and get it running. In this blog post, we cover some of the most interesting components or issues, and provide a few comments about the whole process. You can also see data lab in action, and learn more about it, by watching the video below!\n\u003e Architecture # Here\u0026rsquo;s an overview of a Data Lab workflow to retrieve and organize a dataset (ingest), transform it into structured tables tracked by DuckLake (transform), and export them for external use (export):\nStorage Layout # Let\u0026rsquo;s begin with the storage layout. We use an S3 compatible object store (MinIO), but you could store your files locally as well (not supported by Data Lab, but easy to implement, as DuckLake supports it).\ns3://lakehouse/ ‚îú‚îÄ‚îÄ backups/ ‚îÇ ‚îî‚îÄ‚îÄ catalog/ ‚îÇ ‚îú‚îÄ‚îÄ YYYY_MM_DD/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HH_mm_SS_sss/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ engine.duckdb ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stage.sqlite ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ marts/*.sqlite ‚îÇ ‚îî‚îÄ‚îÄ manifest.json ‚îú‚îÄ‚îÄ raw/ ‚îÇ ‚îî‚îÄ‚îÄ \u0026lt;dataset-name\u0026gt;/ ‚îÇ ‚îú‚îÄ‚îÄ YYYY_MM_DD/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HH_mm_SS_sss/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ *.csv ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ *.json ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ *.parquet ‚îÇ ‚îî‚îÄ‚îÄ manifest.json ‚îú‚îÄ‚îÄ stage/ ‚îÇ ‚îî‚îÄ‚îÄ ducklake-*.parquet ‚îú‚îÄ‚îÄ marts/ ‚îÇ ‚îî‚îÄ‚îÄ \u0026lt;domain\u0026gt;/ ‚îÇ ‚îî‚îÄ‚îÄ ducklake-*.parquet ‚îî‚îÄ‚îÄ exports/ ‚îî‚îÄ‚îÄ \u0026lt;domain\u0026gt;/ ‚îî‚îÄ‚îÄ \u0026lt;dataset-name\u0026gt;/ ‚îú‚îÄ‚îÄ YYYY_MM_DD/ ‚îÇ ‚îî‚îÄ‚îÄ HH_mm_SS_sss/ ‚îÇ ‚îú‚îÄ‚îÄ *.csv ‚îÇ ‚îú‚îÄ‚îÄ *.json ‚îÇ ‚îî‚îÄ‚îÄ *.parquet ‚îî‚îÄ‚îÄ manifest.json The directory structure above contains:\nraw/, which is where you drop your datasets, as they come (usually uncompressed, e.g., if it\u0026rsquo;s a zip file), and you can do this either manually or exclusively via the CLI. stage/ is where parquet files for DuckLake are stored for intermediate transformations. marts/ contains a subdirectory per data mart‚ÄîI set up mine based on types of data (e.g., graphs) or relevant subjects I\u0026rsquo;m exploring (e.g., economics), but the classical setup is by company department (this shouldn\u0026rsquo;t be your case, as this is not production-ready, it\u0026rsquo;s a lab). exports/ contains exported datasets, usually in parquet format (the only one supported by Data Lab right now), ready to be used or loaded elsewhere. backups/ contains snapshots of the catalog databases, including the DuckDB engine DB and the SQLite DuckLake metadata DBs. In some cases, you\u0026rsquo;ll find a path containing a directory representing a date and a subdirectory representing a time‚Äîthis is the timestamp from when the associated operation was started. We call these \u0026lsquo;dated directories\u0026rsquo;. Each dated directory contains a manifest.json with the dataset name (or snapshot name, for backups), along with the S3 path with the location of the latest version of that dataset.\nTech Stack # MinIO # We used MinIO, but you can use any S3-compatible object storage. If you\u0026rsquo;re using MinIO and are having trouble connecting, make sure that your S3_REGION matches the MinIO region. You might have to setup a custom region (e.g. eu-west-1) via the Web UI under Configurations ‚Üí Region.\nUnfortunately, that feature, along with most features for the Community Edition of MinIO, was recently scraped‚Äîyes, sadly this includes users, groups, and policies as well.\nWe will soon research other viable S3-compatible object storage alternatives, perhaps in the neighborhood of SeaweedFS.\nDuckDB and DuckLake # While MinIO provides storage, DuckDB provides compute (we call it the \u0026ldquo;engine\u0026rdquo;), and DuckLake provides a catalog layer to manage metadata and external tables, tracking changes and offering snapshots, schema evolution, and time travel.\ndbt # Tools like dbt (data-build-tool) appear once in a lifetime‚Äîmuch like DuckDB.\n‚ùì Did you know that\u0026hellip;\ndbt is lower case‚Äîlike sushi chefs that don\u0026rsquo;t like you to use a fork, the community also dislikes it when you wrongly spell dbt in upper case.\ndbt is for SQL templating and organization (models), and data documentation and testing (schemas). It provides a few configuration files to help you link up to external data sources as well, but, overall, this is it. Beautifully simple, yet extremely useful. Truly a gem!\nSince dbt provides multiple adapters, we were able to use the dbt-duckdb adapter and, through an unreleased branch, connect to our DuckLake catalogs via DuckDB. We then used pure DuckDB SQL queries to extract our data from the S3 ingestion bucket (raw/) and transform it into usable structured tables (stage/ and marts/).\nOperations # Below is an overview of the main lakehouse-related operations that Data Lab supports.\nWhen required, the dlctl CLI tool will read the manifests and export environment variables pointing to the S3 path with the latest version for each dataset. For example, RAW__DEEZER_SOCIAL_NETWORKS__HR__HR_EDGES will point to something like s3://lakehouse/raw/deezer_social_networks/2025_06_11/11_56_29_470/HR/hr_edges.csv, where the key point here is the date, 2025-06-11T11:56:29.470, which points to the most recent ingestion of this dataset.\nBoth ingestions (raw/) and exports (exports/) can be listed using the CLI:\ndlctl ingest ls dlctl ingest ls -a dlctl export ls dlctl export ls -a As well as pruned (i.e., all versions except the last one are deleted, per dataset):\ndlctl ingest prune dlctl export prune Other than that, catalog backups can be created:\n# Create a backup and update manifest.json accordingly dlctl backup create Listed:\n# List backup root directories dlctl backup ls # List all backed up files dlctl backup ls -a And restored:\n# Restore the latest catalog snapshot into local/ dlctl backup restore # Restore the latest catalog snapshot into /tmp/xpto # instead of local/ dlctl backup restore --target /tmp/xpto # Restore a specific snapshot into local/ dlctl backup restore --source 2025-06-17T16:24:31.349 Ingestion # The dlctl ingestion dataset command supports directory structure creation for manual uploads, as well as direct retrieval from Kaggle or Hugging Face.\nThis will create a dated directory for dataset snap_facebook_large (snake case is always used):\ndlctl ingest dataset --manual \u0026#34;SNAP Facebook Large\u0026#34; And the following commands will ingest two datasets, from Kaggle and Hugging Face, respectively:\ndlctl ingest dataset \u0026#34;https://www.kaggle.com/datasets/andreagarritano/deezer-social-networks\u0026#34; dlctl ingest dataset \u0026#34;https://huggingface.co/datasets/agusbegue/spotify-artists\u0026#34; Transformation # The dlctl transform, dlctl test, and dlctl docs commands are wrappers for dbt, although parametrization is specific to dlctl (at least for now).\nWe can run a specific model (and, therefore, its SQL transformations) as follows:\ndlctl transform -m stage.deezer_social_networks dlctl transform -m stage.million_song_dataset_spotify_lastfm dlctl transform -m marts.graphs.music_taste.nodes_genres Notice that the first two runs are for schemas (and all of their tables), while the last run is for a specific table, nodes_genres, within the graphs catalog and the music_taste schema, which is organized under the marts/ models.\nThis is the stage that produces DuckLake catalogs, storing DuckLake managed parquet files under the stage/ and marts/ S3 directories.\nDownstream/upstream triggering is also supported, using the regular dbt syntax (+):\ndlctl transform -m +marts.graphs.music_taste Finally, you can also run all data tests as follows:\ndlctl test And generate and serve model documentation as follows:\ndlctl docs generate dlctl docs serve Export # In order to be able to use a dataset externally, you first need to export it, from the DuckLake-specific parquet format into a usable format, like parquet (or CSV, or JSON).\nThis can be done by running:\ndlctl export dataset graphs music_taste Where graphs is a data mart catalog and music_taste is the schema. A few specific table exports, with names matching *nodes* and *edges*, will be stored in subdirectories‚Äînodes/ and edges/ in this case. A similar logic can be added to the export process in the future, for other categorizable tables. Otherwise, files will live directly in the root directory, matching the schema name (e.g., music_taste).\nDuckDB Highlights # Most of our SQL code was boring, standard stuff, which is not unusual, but there were also a few interesting points that we cover next.\nHandling DuckDB MinIO Secret # Secrets in DuckDB are ephemeral, and exist only for the active session. As such, we store them in a .env file, which we automatically load via dlctl. We also offer a command to create an init.sql file under the local/ directory, directly generated from your .env configuration, once you set it up.\nAccordingly, you can access your Data Lakehouse locally by running:\n# Generate local/init.sql from your .env dlctl tools generate-init-sql # Connect to the data lakehouse duckdb -init local/init.sql local/engine.duckdb Useful List Functions # Datasets frequently contain string columns with comma-separated lists of items‚Äîin our case, it was tags‚Äîso having access to list functions was extremely useful. Here\u0026rsquo;s the transformation that we used:\nSELECT list_transform( string_split(tags, \u0026#39;, \u0026#39;), tag -\u0026gt; list_aggregate( list_transform( string_split(tag, \u0026#39;_\u0026#39;), tag_word -\u0026gt; ucase(substring(tag_word, 1, 1)) || lcase(substring(tag_word, 2)) ), \u0026#39;string_agg\u0026#39;, \u0026#39; \u0026#39; ) ) AS tags FROM ... Let\u0026rsquo;s go through it.\nlist_transform will apply a lambda to each tag, given by string_split (we split by comma and space). list_aggregate just applies string_agg to concatenate all words in a tag with spaces. Words are obtained from string_split on underscore, and list_transform is used to convert to title case. Title case was achieved by taking the first letter of a word via substring and converting to ucase (upper case). The remaining substring was converted to lcase (lower case)‚Äîif not already. For step 4, we could have used a Python UDF (user-defined function) which took a str and just returned input.title(), or we could have implemented it fully in Python, taking the original comma-separated string of tags as the argument. There would have been a slight overhead, since C++ is faster than Python, but it would have been perfectly viable for such tiny data.\nSetting up such a function using dbt-duckdb is done on top of the plugins API, and would look something like this:\n# Module: funcs from duckdb import DuckDBPyConnection from dbt.adapters.duckdb.plugins import BasePlugin # Title case conversion function def to_title(input: str) -\u0026gt; str: return input.title() # Data-specific tag parsing function def parse_tags(tags: str) -\u0026gt; list[str]: return [ t.replace(\u0026#34;_\u0026#34;, \u0026#34; \u0026#34;).title() for t in tags.split(\u0026#34;, \u0026#34;) ] class Plugin(BasePlugin): def configure_connection(self, conn: DuckDBPyConnection): # Register as UDFs, with the same name, in DuckDB conn.create_function(\u0026#34;to_title\u0026#34;, to_title) conn.create_function(\u0026#34;parse_tags\u0026#34;, parse_tags) A reference to this module would need to be added to the duckdb profile config:\ntransform: outputs: lakehouse: type: duckdb # ... plugins: - module: funcs Either way, I would avoid using this unless strictly necessary‚Äîif a working implementation exists in pure SQL, it\u0026rsquo;s usually more efficient than Python.\nAbnormally Slow JSON Parsing # One of the datasets we ingested and ran transformations for was andreagarritano/deezer-social-networks, which is found on Kaggle. It contains user data and friendship relationships for three subsets of Deezer users from Croatia (HR), Hungary (HU), and Romania (RO). For each country, there are two files: *_edges.csv and *_genres.json. The genres JSON looks something like this, but unformatted:\n{ \u0026#34;13357\u0026#34;: [\u0026#34;Pop\u0026#34;], \u0026#34;11543\u0026#34;: [\u0026#34;Dance\u0026#34;, \u0026#34;Pop\u0026#34;, \u0026#34;Rock\u0026#34;], \u0026#34;11540\u0026#34;: [\u0026#34;International Pop\u0026#34;, \u0026#34;Jazz\u0026#34;, \u0026#34;Pop\u0026#34;], \u0026#34;11541\u0026#34;: [\u0026#34;Rap/Hip Hop\u0026#34;], // ... } These files were extremely slow to parse in DuckDB. The largest genres JSON file is for Croatia, and is only 4.89 MiB. However, when we tried to load and transform this file using the following query, we got extremely high memory usage (hitting 14 GiB for DuckDB), and abnormally slow response time:\nCREATE TABLE users AS SELECT CAST(je.key AS INTEGER) AS user_id, CAST(je.value AS VARCHAR[]) AS genres FROM read_json(\u0026#39;HR/HR_genres.json\u0026#39;) j, json_each(j.json) je; Run Time (s): real 638.958 user 837.703249 sys 438.916651 That\u0026rsquo;s nearly 14m‚ÄºÔ∏è\nSo, we tried to turn the JSON object into JSON lines, using:\njq \u0026#34;to_entries[] | {key: .key, value: .value}\u0026#34; \\ HR/HR_genres.json \u0026gt;HR/HR_genres.jsonl Which ran in sub-second time, as expected:\nExecuted in 481.52 millis fish external usr time 407.70 millis 596.00 micros 407.10 millis sys time 68.76 millis 914.00 micros 67.85 millis Reading HR/HR_genres.jsonl inside DuckDB was then instant and completely RAM-efficient, also as expected:\nCREATE TABLE users AS SELECT CAST(j.key AS INTEGER) AS user_id, CAST(j.value AS VARCHAR[]) AS genres FROM read_json(\u0026#39;HR/HR_genres.jsonl\u0026#39;) j; Run Time (s): real 0.082 user 0.106543 sys 0.032145 At first, since so much RAM was in use, we thought the query was actually a¬†CROSS JOIN¬†that replicated the whole JSON object for each produced line in the final table, but then we noticed that this is not the case, since the docs mentioned json_each¬†as being a¬†LATERAL JOIN (see here).\nWe also tried several other approaches, like first creating a table with the JSON object preloaded and querying over that, but this changed nothing. Besides parsing and transforming the file outside DuckDB before reading it in SQL, I don\u0026rsquo;t think there was much else we could do here, which was disappointing, as this would mean we\u0026rsquo;d have to add another layer to Data Lab, between ingestion and transformation, that would also do transformation but now in Python.\nI decided in favor of sticking with a pure SQL solution, so we posted a question in the GitHub Discussion for DuckDB, describing the issue, and one of the users was kind enough to debug the problem with us.\nFirst Proposed Solution # The first suggestion was to use one of the following approaches:\nFROM read_json(\u0026#39;HR/HR_genres.json\u0026#39;, records=false) SELECT unnest(json_keys(json)) AS user_id, unnest(list_value(unpack(columns(json.*)))) AS genres UNPIVOT (FROM read_json(\u0026#39;HR/HR_genres.json\u0026#39;)) ON * Let\u0026rsquo;s quickly unpack what is going on with these two queries.\nQuery #1 # Let\u0026rsquo;s start with the first one. First, we call read_json with records=false. By default, records is set to auto. For records=true, each key will become a column with its corresponding value. For records=false, it will either return a map(varchar, varchar[]) or a struct with each user ID identified as a field (the latter is what we want to happen here). You can read more about records here.\nAccording to the docs, COLUMNS is essentially an expansion on column selection, like j.*, col1, *, but where we can apply filtering by, or manipulate, the column name(s):\nCOLUMNS()¬†expression¬†can be used to execute the same expression on multiple columns:\nwith regular expressions with¬†EXCLUDE¬†and¬†REPLACE with lambda functions For details on COLUMNS(), go here.\nThen, UNPACK works essentially like *lst would do in Python (i.e., it will expand the elements of a list into arguments for a function). In this example, the target function is list_value, which takes multiple arguments and returns a list with those arguments.\nFinally, for completion sake, unnest just unwinds a list or array into rows, and json_keys returns the keys in a JSON object.\nQuery #2 # For the second query, we\u0026rsquo;re just reading the JSON object using records=auto, which translates into records=true for the small sample JSON. Then, we are applying UNPIVOT on all columns (the user IDs), so that each becomes a row of user ID and list of genres.\nDocs about UNPIVOT here.\nBoth Query #1 and Query #2 worked fine for the small example that we provided (same as above), but failed for the larger original file.\nSecond Proposed (Working) Solution # We found out that the JSON object was parsed differently from 200 user records onward, and discovered there is a default of map_inference_threshold=200 for read_json. For long JSON objects like the one we have, this means that it will stop parsing object keys as structure fields from 200 keys onward, thus returning a map(varchar, varchar[]) instead of a struct, making the original query fail.\nKnowing this, the suggestion was to disable the threshold for map inference and run the query:\nFROM read_json( \u0026#39;HR/HR_genres.json\u0026#39;, records=false, map_inference_threshold=-1 ) SELECT unnest(json_keys(json)) AS user_id, unnest(list_value(unpack(columns(json.*)))) AS genres This brough down the run time from 14m to 24s, which is a significant speedup, but still extremely slow compared to the sub-second run time we got from first parsing the JSON object via jq to turn it into JSON lines.\nThis also made it possible to run the UNPIVOT query, which was even faster, taking only 6s to run:\nUNPIVOT ( FROM read_json( \u0026#39;HR/HR_genres.json\u0026#39;, records=true, map_inference_threshold=-1 ) ) ON *; Third Proposed (Creative) Solution # Finally, a third solution based on a variable to store the JSON object keys was also proposed:\nSET variable user_ids = ( FROM read_json_objects(\u0026#39;HR/HR_genres.json\u0026#39;) SELECT json_keys(json) ); FROM read_json_objects(\u0026#39;HR/HR_genres.json\u0026#39;) SELECT unnest( getvariable(\u0026#39;user_ids\u0026#39;) )::INTEGER AS user_id, unnest( json_extract( json, getvariable(\u0026#39;user_ids\u0026#39;) )::VARCHAR[] ) AS genres We find it interesting that such a solution design is possible in DuckDB. Regardless, this version was less efficient than the UNPIVOT query on the second solution, so we went with that.\nThe question remains. If the best solution takes 6s to run for a 5 MiB file, is this something that we might need to address in DuckDB, specially when the same process can run in milliseconds with a little command line magic?\nDuckDB and DuckLake Wishlist # While we were working with DuckDB and DuckLake, we created a bit of a wishlist, which we share with you below.\n1. Externally Loadable Parquet Files # Currently, using data from DuckLake tables externally requires exporting (e.g., to parquet). Maybe there isn\u0026rsquo;t a better solution, but it also defies the purpose of a data lakehouse, as data won\u0026rsquo;t be ready to use without a DuckLake adapter on target tools, which doesn\u0026rsquo;t seem reasonable to expect any time soon.\nIt\u0026rsquo;s not clear whether an DuckLake parquet file can be directly read by external processes, but it seems unlikely to be the case. Whether this is desirable, or a solid design choice, it is surely up for discussion, but, if we\u0026rsquo;re building on top of external storage, shouldn\u0026rsquo;t the stored files be ready to use directly?\nIf we follow that direction, then there is no clear way to match external files to the tables in DuckLake, as the current naming schema is not designed for identifying which parquet files are which.\n2. Hierarchical Schemas # Hierarchical schemas would be useful (e.g., marts.graphs.music.nodes), as this comes up a lot.\nIt is how dbt sets up its model logic‚Äîthey use an _ for different levels‚Äîand it is also the way disk storage works (i.e., directories are hierarchical), and the natural way to organize data.\nSome teams are even looking into graph-based structures for data cataloging (e.g., Netflix). Maybe that could be an interesting as a feature for DuckDB and DuckLake, not to mention an additional distinguishing factor.\n3. Sequences in DuckLake # It would be nice to have access to sequences in DuckLake, if that makes sense technically as well. For example, while preparing nodes, generating node IDs could be done using sequences a opposed to something like:\nWITH other_nodes AS ( SELECT max(node_id) AS last_node_id FROM ... ) SELECT o.last_node_id + row_number() OVER () AS node_id FROM ..., other_nodes o ","date":"24 June 2025","externalUrl":null,"permalink":"/posts/data-lakehouse-dbt-ducklake/","section":"","summary":"Summary # Now that you can use DuckDB to power your data lakehouse through DuckLake, you\u0026rsquo;ll also save space on snapshots due to the ability to reference parts of parquet files (yes, you can keep all old versions, with little impact to storage), and you\u0026rsquo;ll get improved performance for small change operations due to data inlining, which lets data be stored directly within the metadata database (SQLite, PostgreSQL, etc.","title":"Data Lakehouse with dbt and DuckLake","type":"posts"},{"content":"","date":"24 June 2025","externalUrl":null,"permalink":"/tags/dbt/","section":"Tags","summary":"","title":"Dbt","type":"tags"},{"content":"","date":"24 June 2025","externalUrl":null,"permalink":"/tags/etl/","section":"Tags","summary":"","title":"Etl","type":"tags"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/databases/","section":"Tags","summary":"","title":"Databases","type":"tags"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/extensions/","section":"Tags","summary":"","title":"Extensions","type":"tags"},{"content":" Summary # Unedited research notes for my \u0026ldquo;PostgreSQL Maximalism\u0026rdquo; series. This is likely more than enough information, if you\u0026rsquo;re looking into extending Postgres for your storage and querying needs. For an easier-to-digest, follow-up version, check the video series. For your convenience, each extension category is properly annotated in the videos as chapters.\n\u003e Research # Document Store # Built-In Key-Value Support Built-In XML Support Built-In JSON Support Native types and functions for writing and reading JSON data. pg_jsonschema Adds JSON schema validation functions for robustness. Column Store and Analytics # pg_mooncake \u0026ldquo;Postgres-Native Data Warehouse\u0026rdquo; Provides column stores in PostgreSQL (Iceberg, Delta Lake). Uses DuckDB to query. Unlike pg_duckdb and pg_analytics, pg_mooncake can write out data to Iceberg or Delta Lake formats via transactional INSERT/UPDATE/DELETE. pg_duckdb Developed by Hydra and MotherDuck. MotherDuck integration. Maybe this will replace pg_mooncake when DuckDB extends integration with Iceberg or Delta Lake. pg_analytics Part of ParadeDB. Based on DuckDB. Recently archived and deprecated in favor of pg_search. pg_lakehouse Precursor to pg_analytics. Based on Apache DataFusion. Used to be a part of the ParadeDB codebase. columnar Developed by Hydra. Also used in pg_timeseries. Time Series Store and Real-Time # timescaledb A solution by Timescale. Provides a lot more functions to handle time series than pg_timeseries. Low latency makes it adequate for real-time analytics. Supports incremental views through continuous aggregates. Has some overlap with pg_mooncake, but can\u0026rsquo;t write to Iceberg or Delta Lake, using them directly as the storage layer. Supports tiered storage pg_timeseries A solution by Tembo. \u0026ldquo;The Timescale License would restrict our use of features such as compression, incremental materialized views, and bottomless storage.\u0026rdquo; Supports incremental materialized views. Vector Store # pgvector Vector database. Approximate indexing HNSW: Hierarchical Navigable Small World IVFFlat: Inverted File Flat Supported by GCP Cloud SQL and AWS RDS. How does it compare to pg_search? pgvectorscale A solution by Timescale. Learns from Microsoft\u0026rsquo;s DiskANN: \u0026ldquo;Graph-structured Indices for Scalable, Fast, Fresh and Filtered Approximate Nearest Neighbor Search\u0026rdquo; Efficiency layer over pgvector via: StreamingDiskANN indexing approach Statistical Binary Quantization Label-based filtered vector search Artificial Intelligence # pgai A solution by Timescale. \u0026ldquo;A suite of tools to develop RAG, semantic search, and other AI applications\u0026rdquo; Takes advantage of pgvectorscale for improved performance. Features Loading datasets from Hugging Face. Computing vector embeddings. Chunking text. Semantic search or RAG via OpenAI, Ollama, or Cohere. pg_vectorize A solution by Tembo (powers their VectorDB stack). Similar to pgai, supporting RAG and semantic search, but relies directly on pgvector. Supports Hugging Face\u0026rsquo;s Sentence-Transformers as well as OpenAI\u0026rsquo;s embeddings. Supports direct interaction with LLMs. pgrag Rust-based, experimental solution by Neon. Complete pipeline support from text extraction (PDF, DOCX) to chat completion based on ChatGPT\u0026rsquo;s API. Support for bge-small-en-v1.5 or OpenAI\u0026rsquo;s embeddings. Distance computation and ranking based on pgvector. Reranking based on jina-reranker-v1-tiny-en also available. Full-Text Search # Built-In Full-Text Search Support Generalized Inverted Index (GIN). tsvector and tsquery data types. Text preprocessing pipeline configurations usable by to_tsvector and to_tsquery. The english configuration runs the following operations: Tokenize text by spaces and punctuation; Convert to lower case; Remove stop words; Apply Porter stemmer. Example ranking/scoring functions ts_rank and ts_rank_cd. ParadeDB Search and Analytics. Search pg_search (ParadeDB\u0026rsquo;s rust-based version) Previously named pg_bm25. This is huge and it takes a long long time to compile! Adds a ton of Lucene-like features, based on Tantivy, a Rust-based Lucene alternative. Still doesn\u0026rsquo;t provide a text-based query parser. Analytics pg_analytics This has been deprecated, due to refocus on pg_search, even for analytics. Part of ParadeDB. pg_trgm Built-in extension. Character-based trigrams. Useful for fuzzy search based on string similarity (e.g., product name matching). Can be optimized via GIN and GiST indexes. pg_fuzzystrmatch Built-in extension. Provides functions to match and measure similar-sounding strings. pg_similarity Text similarity functions. Supported by GCP Cloud SQL and AWS RDS. Last commit over 5 years ago. Has forks to fix compilation with the latest versions of PostgreSQL. Graph Store # pgrouting Extension of PostGIS Graph algorithms AgensGraph PostgreSQL fork, not an extension. If it doesn\u0026rsquo;t integrate, why use this instead of a more specialized graph database, like Neo4j? Architecture diagram shows that it has its own separate graph storage layer. Query language SQL (ANSI) Cypher (openCypher) Visualization age Extension inspired by AgensGraph. Query language ANSI SQL Cypher (openCypher) No Gremlin support (yet) Visualization No graph algorithms. pggraph SQL implementations of Dijkstra and Kruskal. DOA (Dead On Arrival), this has been abandoned for 9 years. Message Queue # pgmq A solution by Tembo. Type: extension. Official libraries for Rust and Python. Community libraries for Dart, Go, Elixir, Java, Kotlin, JavaScript, TypeScript, .NET. Actively maintained. pgq A solution by Skype and a part of SkyTools. Type: extension. Official library for Python (last released for Python 3.8). Still maintained, but no meaningful changes over the last two years. Documentation pg_message_queue Type: extension. Abandoned for over 8 years. pgqueuer Type: Python library. Needs to be setup via pgq install, which creates required tables and indexes. pg-boss Type: JavaScript library. Setup is done in-code, by creating the queue. queue_classic Type: Ruby library. Postgres connection is setup via an environment variable. Description # Documents # Document formats, like JSON, XML, or YAML, model hierarchical data that follow a tree-like structure.\nThis kind of data is frequently stored in document databases like MongoDB (BSON), or simply using a key-value store, like RocksDB, where the value can only be deserialized and used in-code.\nPostgres natively supports JSON via its json and jsonb data types, as well as XML via its xml data type. It also supports key-value storage via the hstore extension, which is available by default. While the xml data type supports XML validation via the xmlschema_valid function, for JSON there is an extension called pg_jsonschema that adds support for validation based on a JSON Schema.\nAnalytics and Time Series # Transactional and analytics operations have different requirements. By default, Postgres is row-oriented, which is ideal for transactions (e.g., updating a user profile), but for analytics it\u0026rsquo;s usually more efficient to rely on column-oriented storage (e.g., averaging movie ratings, per age group). In this context, partitioning data is often a requirement as well, as to reduce complexity thus increasing performance.\nOn the data engineering community, formats like Apache Iceberg or Delta Lake, which add a metadata layer on top of Apache Parquet, are becoming a requirement for data lakehouse architectures. This layer tracks snapshots (data versioning), schema structure, partition information, and parquet file locations.\nAnother trend in the DE community is DuckDB, an in-process column-oriented database. Built for analytics, DuckDB is able to support medium scale data science tasks on a single laptop, and that\u0026rsquo;s why we love it! Think of it as a counterpart to SQLite, which is a well-liked row-oriented in-process database.\nColumn-oriented and analytics has been brought to Postgres via extensions like pg_mooncake, pg_duckdb, or pg_analytics.\nThere are also time series specific extensions that support real time and analytics by providing additional features, like incremental views, or functions like time_bucket_gapfill (add missing dates), or locf and interpolate to fill-in missing values.\nTime series specific extensions include the well-known timeseriesdb, or the more recent pg_timeseries.\nVectors and AI # One of the fundamental requirements of vector stores is that they provide efficient vector similarity calculations. This is usually achieved through specialized indexing that supports approximated similarity computations.\nExtensions like the well-known pgvector, or its complement pgvectorscale, both support querying nearest-neighbors, on pgvector via HNSW and IVFFlat indexes, and on pgvectorscale via a StreamDiskANN index. Nearest-neighbors can be computed based on multiple distance functions, such as Euclidean/L2, cosine, or Jaccard.\nRegardless of whether AI operations belongs in the database, extensions to facilitate text embedding and LLM integration still exist, integrating with pgvector.\nAI extensions include pgai and pg_vectorize, both supporting direct LLM querying, text embedding, and RAG and similarity search. In both extensions, text embedding is made possible either based on Hugging Face models, by querying OpenAI\u0026rsquo;s embedding API, or via Ollama\u0026rsquo;s API, which also powers the direct access to LLMs and RAG features. There is also pgrag, a more recent, experimental extension focused on delivering a complete pipeline for RAG, being the only one that supports text extraction from PDF or DOCX files, as well as a specialized reranking model to help improve the outcome before generating the text completion.\nAll this is made possible by accessing Python APIs under the hood, via PL/Python. While these features can be convenient at times, I tend to think that they do not belong in the database, but rather on its own Python codebase. The database should be exclusively concerned with storage and retrieval, so, unless there are performance reasons that justify integrating complex data processing features with the database, I believe this should be avoided. An example of this is pgvector and pgvectorscale, where indexing approaches were required to efficiently solve the vector distance computations ‚Äî and indexing belongs in the database.\nSearch # Built-In # Full-Text Search # PostgreSQL provides basic full-text search features out-of-the-box with its Generalized Inverted Index (GIN), tsvector and tsquery data types, and corresponding functions and operators (e.g., @@ for matching a tsvector with a tsquery, or || for concatenating tsvector), supporting negation (!!), conjunction (\u0026amp;\u0026amp;), disjunction (||), and phrase queries (\u0026lt;-\u0026gt;). Documents and queries can be parsed using to_tsvector or to_tsquery, which default to the english configuration ‚Äî tokenizes text by spaces and punctuation, normalizes to lower case, removes stop words, and applies Porter stemmer.\nSince PostgreSQL 11, there is the websearch_to_tsquery function, which gives us the ability to parse keyword queries directly, like we do on Google or with Apache Lucene, however there are differences.\nFor example, parsing the following keyword query:\n\u0026#34;data science\u0026#34; \u0026#34;state of the art\u0026#34; algorithms models Would result in the PostgreSQL equivalent:\n\u0026#39;data\u0026#39; \u0026lt;-\u0026gt; \u0026#39;scienc\u0026#39; \u0026amp; \u0026#39;state\u0026#39; \u0026lt;3\u0026gt; \u0026#39;art\u0026#39; \u0026amp; \u0026#39;algorithm\u0026#39; \u0026amp; \u0026#39;model\u0026#39; Which is essentially a conjunction (AND) of the two phrases and the two terms, along with stemming and stop word handling.\nHowever, search engines commonly default to disjunction (OR). Since results are often ranked, search engines just push results with a less and less matched tokens to the end of the results list.\nWe can rank the matched documents, either by using ts_rank, which is based on term frequency and proximity, or by using ts_rank_cd, which factors in cover density ranking (i.e., query term distance in the document). However, in order to retrieve documents that only partially match the query, we\u0026rsquo;d need to manually parse the query:\nphraseto_tsquery(\u0026#39;data science\u0026#39;) || phraseto_tsquery(\u0026#39;state of the art\u0026#39;) || to_tsquery(\u0026#39;algorithm | model\u0026#39;) Which is query-dependent and require us to build a query parser to handle this outside of SQL. If we do that, we can rank our documents by creating the GIN index:\nCREATE INDEX idx_doc_fulltext ON doc USING GIN (to_tsvector(\u0026#39;english\u0026#39;, content)); And using the following query:\nWITH search AS ( SELECT id, content, to_tsvector(\u0026#39;english\u0026#39;, content) AS d, phraseto_tsquery(\u0026#39;data science\u0026#39;) || phraseto_tsquery(\u0026#39;state of the art\u0026#39;) || to_tsquery(\u0026#39;algorithm | model\u0026#39;) AS q FROM doc ) SELECT ts_rank(d, q) AS score, id, content FROM search WHERE d @@ q ORDER BY score DESC; Note that Postgres uses the designation \u0026ldquo;rank\u0026rdquo; to refer to the score. For example, in the docs, when describing the weights for to_tsrank, they use phrases like \u0026ldquo;2 divides the rank by the document length\u0026rdquo;, where \u0026ldquo;rank\u0026rdquo; is really the returned score (no ranks are returned by ts_rank or ts_rank_cd).\nAlso note that these functions do not return the raw score, so the following won\u0026rsquo;t be equivalent:\nts_rank(d, q, 2) ts_rank(d, q) / length(d) Fuzzy String Matching # By default, Postgres also provides two extensions called pg_trgm and fuzzystrmatch. The first uses character-based trigrams to provide fuzzy string matching and compute string similarity. The second provides functions to match and measure similar-sounding strings.\nExtensions # There are other third-party extensions to compute string similarity, like pg_similarity, which provides several distance functions like L1/Manhattan, L2/Euclidean or Levenshtein, and other less commonly used methods like Monge-Elkan, Needleman-Wunsch or Smith-Waterman-Gotoh. While frequently offered in cloud services, including GCP and AWS, this extension appears to be unmaintained and incompatible with the latest versions of Postgres (forks exist to make it compilable).\nFinally, there is a fairly large and mature project, called ParadeDB, which provides a pg_search extension. Since the built-in support for full-text search on Postgres only provides two example ranking functions, the pg_search extension, initially called pg_bm25, was created to bring the BM25 ranking function to Postgres. It has since matured quite a lot, providing innumerous features supported by a new bm25 index. This index provides configurable segment sizes, as well as a separate text preprocessing configuration that can be set per field during indexing. A new operator @@@ is also introduced for matching, and field-based queries and boosting are supported. Several useful functions are provided to for checking term existence, fuzzy matching, range filtering, set matching, or phrase matching. JSON can also be indexed and queried, and \u0026ldquo;more like this\u0026rdquo; queries are also supported. Similarity search is supported via the pgvector extension.\nGraphs # This category is where Postgres does not shine. While graph storage can be done directly by creating a table for nodes and for relationships, this does not scale for real-world graph querying, particularly for demanding graph algorithms. A graph database usually relies on index-free adjacency to ensure efficiency, which is not supported by Postgres. The alternative is to index the ID columns of the relationships table, which means that complex graph queries, that require long walks or traversals, will need to query an index for each step it takes, without considering caching. For large graphs, this is highly inefficient. As far as I know, there is no Postgres extension that solves this problem at this moment.\nAlternatives for graph storage include AgensGraph, which is a Postgres fork rather than an extension, as well as Apache AGE (A Graph Extension), which was inspired by AgensGraph. Both support ANSI SQL as well as openCypher for querying, with AGE having an open issue on GitHub to implement Apache Gremlin support as well. AgensGraph has limited support for graph algorithms, while AGE has none at all, rather providing user defined functions. An project called pggraph implemented the Dijkstra and Kruskal graph algorithms using pure SQL, but has since been abandoned ‚Äî it didn\u0026rsquo;t provide any specialized storage, but rather just functions to apply to your own relationships table via a SQL query parameter.\nFinally, perhaps the most interesting extension we can use for graph algorithms is pgrouting, which is built on top of postgis, as it is designed to add network analysis support for geospatial routing. While this still does not provide a custom storage layer for graphs, with index-free adjacency, it does provide a wide range of graph algorithms.\nMessage Queues # Message queueing software implements the producer-consumer pattern (one-to-one) and usually supports the publish-subscribe pattern as well (one-to-many / topics / events). Well-known software in this category includes Redis, ZeroMQ, RabbitMQ, or Apache Kafka, all of which provide interface libraries for several different languages ‚Äî this is a requirement for message-oriented middleware, as different components are often written in different languages.\nWhile any of the previous options are likely more efficient than a Postgres-based implementation, for simple use cases there are a few extensions and libraries that implement message queues on top of Postgres. There is pgmq, from Tembo, the same authors of pg_timeseries and pg_vectorize. This integrates with over 10 languages via official (Rust and Python) and community libraries, providing a create queue function, as well as send and read functions, alongside other utilities, to support the producer-consumer pattern. For the publish-subscribe pattern, we only found the pgq extension from Skype, which similarly provides a create_queue function, as well as insert_event, register_consumer and get_batch_events functions.\nAll other extensions and libraries we found only implement the producer-consumer pattern. There is pg_message_queue which is an extension that provides functions pg_mq_create_queue, pg_mq_send_message, and pg_mq_get_msg_bin (bytes) or pg_mq_get_msg_text (plain text) ‚Äî it also supports LISTEN for asynchronous notifications. There are also libraries supported on Postgres to help handle job queues: pgqueuer for Python, pg-boss for JavaScript, and queue_classic, que, good_job or delayed_job for Ruby.\nComparison # PGDG - PostgreSQL Global Development Group\nDocuments # Alternatives to: RocksDB, eXist-db, MongoDB\nExtension Author Created Description üî¥ hstore PGDG 2008 Bundled key-value type and functions. üî¥ xml PGDG 2008 Native XML type and functions. üü¢ json / jsonb PGDG 2012 / 2014 Native JSON types and functions. üü¢ pg_jsonschema Supabase 2022 JSON schema validation. Analytics and Time Series # Alternatives to: DuckDB, Apache Cassandra, Amazon RedShift, Google BigQuery, Snowflake, InfluxDB, Prometheus, Amazon Timestream\nExtension Author Created Description üü¢ pg_mooncake Mooncake Labs 2024 Column store based on Iceberg or Delta Lake, that transparently uses DuckDB vectorization for analytics queries, but also lets us extract Iceberg or Delta Lake for processing externally (e.g., using polars or duckdb). üî¥ pg_duckdb Hydra \u0026amp; MotherDuck 2024 Official extension for DuckDB that integrates with MotherDuck and cloud storage (e.g., AWS S3, Google GCS). üî¥ pg_analytics ParadeDB 2024 Similar to pg_duckdb. Added support for DuckDB as part of ParadeDB, but it was discontinued in favor of integrating analytics directly into pg_search instead. üî¥ pg_lakehouse ParadeDB 2024 Added support for Apache DataFusion to ParadeDB, but it was deprecated in favor of pg_analytics and a DuckDB backend. üü° columnar Hydra 2022 Columnar storage engine at the core of Hydra, a data warehouse replacement built on top of PostgreSQL. üü¢ timescaledb Timescale 2017 Well-known time series storage solution based on the hypertable, a temporally partitioned table. Adequate for real-time solutions due to its low latency and incremental materialized views. Provides a wide range of useful analytics functions. üî¥ pg_timeseries Tembo 2024 Similar to timescaledb, but extremely lacking in analytics functions. Built to compete with the limiting Timescale License. Vectors and AI # Alternatives to: Pinecone, Weaviate, Milvus, Azure AI Search\nExtension Author Created Description üü¢ pgvector Andrew Kane et al. 2021 Provides a vector type, as well as several similarity functions that power kNN. Efficiency is reached by implementing the HNSW and IVFFlat approximate indexing strategies. üî¥ pg_vectorscale Timescale 2023 Extends pgvector with the StreamingDiskANN index (inspired by Microsoft\u0026rsquo;s DiskANN), and adds Statistical Binary Quantization for compression, and label-based filtered vector search for vector operations with added filtering over categories. üü¢ pgai Timescale 2024 Relies on pg_vectorscale to provide semantic search, RAG via OpenAI, Ollama or Cohere, text chunking, computing text embeddings, or loading Hugging Face datasets. üî¥ pg_vectorize Tembo 2023 Similar to pgai, but relies directly on pgvector to provide semantic search, and RAG via Hugging Face\u0026rsquo;s Sentence-Transformers, OpenAI\u0026rsquo;s embeddings or Ollama. It also supports direct interactions with LLMs. üî¥ pgrag Neon 2024 Focused on providing a complete RAG pipeline, provides text extraction from PDF or DOCX, as well as support for reranking via jinaai/jina-reranker-v1-tiny-en. Embeddings are either based on BAAI/bge-small-en-v1.5 or OpenAI, and it only supports ChatGPT for generation. Search # Alternatives to: Elasticsearch, Apache Solr\nExtension Author Created Description üü¢ tsvector / tsquery PGDG 2008 Native text preprocessing, document/query vector representation and matching, basic ranking functions, and GIN index to support efficient full-text search. üü¢ pg_search ParadeDB 2023 Historically introduced as pg_bm25, as it focused on bringing BM25 into Postgres, it now also provides several Lucene-like features, supported on Tantivy, a Rust-based Lucene alternative. It also provides its own bm25 index with several text preprocessing settings (e.g. support for n-grams). It supports field-based and range queries, as well as set filtering, and boosting. üü¢ pg_trgm PGDG 2011 Bundled character-based trigram matching, useful for string similarity and autocompletion. üî¥ fuzzystrmatch PGDG 2005 Bundled string similarity functions, with support for matching similar-sounding names via Daitch-Mokotoff Soundex. üî¥ pg_similarity Euler Taveira et al. 2011 Large collection of text-similarity functions, like L1/Manhattan, L2/Euclidean or Levenshtein, and other less known approaches like Monge-Elkan, Needleman-Wunsch or Smith-Waterman-Gotoh. Graphs # Alternatives to: Neo4j, OrientDB, KuzuDB\nExtension Author Created Description üü¢ pgrouting pgRouting community 2010 Built on top of postgis, it was designed to add network analysis support for geospatial routing. Despite its focus, this is likely the most complete graph extension for Postgres, supporting multiple graph algorithms, although none of the state-of-the-art approaches (e.g., embeddings). üî¥ AgensGraph SKAI Worldwide 2016 Technically a Postgres fork, supporting ANSI SQL and openCypher, with few graph algorithms. üî¥ age Apache 2020 Apache AGE (A Graph Extension) supports ANSI SQL and openCypher, and might come to support Apache Gremlin. Unfortunately, no graph algorithms are provided. üî¥ pggraph Rait Raidma 2016 Meant as a collection of graph algorithms for Postgres, it only implemented Dijkstra and Kruskal, but the project has been abandoned. Message Queues # Alternatives to: Redis (Queue, Pub/Sub), ZeroMQ, RabbitMQ, Apache Kafka, Amazon Simple Queue Service, Google Cloud Pub/Sub\nTwo main categories: producer-consumer (one-to-one), and publish-subscribe (one-to-many, event-driven).\nLibraries are focused on job queues and support scheduling as well.\nExtension Author Created Description üü¢ pgmq Tembo 2023 Provides a create queue function, as well as send and read functions, alongside other utilities, to support the producer-consumer pattern. Integrates with over 10 languages via official (Rust and Python) and community libraries. üî¥ pgq Skype 2016 Provides a create_queue function, as well as insert_event, register_consumer and get_batch_events functions. It supports the publish-subscribe pattern. üî¥ pg_message_queue Chris Travers 2013 Provided the functions pg_mq_create_queue, pg_mq_send_message, and pg_mq_get_msg_bin (bytes) or pg_mq_get_msg_text (plain text), and also supported LISTEN for asynchronous notifications. Originally published via an SVN repository and later migrated to Google Code, the code by the original creator is no longer available or maintained. While a fork exists on GitHub, the project has been abandoned. üî¥ pgqueuer Jan Bj√∏rge L√∏vland et al. 2024 Python library (pgqueuer) and CLI tool (pgq) that relies on asyncpg instead of psycopg2 (like pgmq). It can be configured using default Postgres environment variables, but there is no default env var to set the connection string. Queues are managed programmatically and via the CLI and only one queue exists per database, stored in the pgqueuer_jobs table. üî¥ pg-boss Tim Jones et al. 2016 Node.js library that provides the PgBoss object, instantiated with a connection string. This creates the pgboss schema where the queues are named and managed. üî¥ delayed_job Shopify 2008 Ruby library extracted from Shopify. It supports Active Job and it is not specific to Postgres. It provides multiple features to handle diverse tasks at Shopify and one of the features is named queues. Not the best option for a general purpose message queue library on top of Postgres. üî¥ que Chris Hanks et al. 2013 Ruby library that focuses on reliability and performance, taking advantage of PostgreSQL\u0026rsquo;s advisory locks, which are application-specific locks that can be set at session-level or transaction-level. These fail immediately when locked instead of blocking like row-level locks do, so workers can try another job. üî¥ good_job Ben Sheldon et al. 2020 Ruby library. Inspired by delayed_job and que, it also uses advisory locks, but provides Active Job and Rails support. üî¥ queue_classic Ryan Smith et al. 2011 Ruby library specialized in concurrent locking and supporting multiple queues and workers that can handle any of those named queues. Bits # Before PostgreSQL there was Postgres, which didn\u0026rsquo;t support SQL but an implementation of QUEL (POSTQUEL). QUEL was inspired by relational algebra and created as a part of the Ingres Database. pgcli is a useful pgsql alternative that adds syntax highlighting, autocompletion, multiline editing, and external editor support. Harlequin is a SQL IDE for the command line, supporting DuckDB natively, but also SQLite, PostgreSQL, or MariaDB, via plugins. - It can be installed via uv by running: uv tool install harlequin[postgres] - Create a Postgres profile via: uvx harlequin --config - And then connect using: uvx harlequin --profile \u0026lt;profile\u0026gt; - If it\u0026rsquo;s the default profile, you can just run: uvx harlequin WhoDB is as web client with support for PostgreSQL, MongoDB, Redis, SQLite, etc. that can be deployed as a docker image and connect to our postgresql-maximalism via host.docker.internal. It also supports conversational querying via an Ollama supported LLM ‚Äî must have Ollama installed, along with the required models, and run ollama serve. When looking for Postgres extensions, there are two registries we can search: PGXN, the PostgreSQL eXtension Network. pgxn can be installed using pip install pgxnclient. Install extension: pgxn install pgmq Load extension: pgxn load -d dbname pgmq Trunk, a Postgres extension registry. trunk can be installed using cargo install pg-trunk. Install extension: trunk install pgmq will install the pgmq extension. Load extension: psql -d dbname -c \u0026quot;CREATE EXTENSION pgmq;\u0026quot; On Postgres, temporary tables are scoped to a session defaulting to ON COMMIT PRESERVE ROWS, however SQL clients often timeout sessions, so be aware of this if you\u0026rsquo;re working interactively. For example, VSCode\u0026rsquo;s SQLTools requires idleTimeoutMillis to be set per connection, or else it will default to 10s before closing idle sessions. I set mine to 1h (3,600,000ms). Make sure to save the connection and reconnect, when changing this. You can also set it to 0, in which case only manually disconnecting and reconnecting will force the session to be closed. Did you know that $$ ... $$ blocks are just a different way to quote strings? And did you know that these blocks can be nested by using a quote identifier like $myblock$ ... $myblock$? PIGSTY (PostgreSQL In Great STYle) is a PostgreSQL local-first RDS alternative that supports nearly all extensions we tested, excluding pgai, but it does support it\u0026rsquo;s competitor, pg_vectorize, from Tembo. Resources # PGXN - PostgreSQL Extension Network Trunk - A Postgres Extension Registry Just use Postgres Postgres as a Graph Database: (Ab)using pgRouting GCP: Configure PostgreSQL extensions AWS: Extension versions for Amazon RDS for PostgreSQL Elasticsearch as a column store pg_mooncake: Fast Analytics in Postgres with Columnstore Tables and DuckDB Anomaly Detection in Time Series Using Statistical Analysis PostgreSQL Wiki: Incremental View Maintenance Everything You Need to Know About Incremental View Maintenance What Goes Around Comes Around\u0026hellip; And Around\u0026hellip; Faster JSON Generation with PostgreSQL PIGSTY (PostgreSQL In Great STYle) TimescaleDB: Best Practices for Time Partitioning TimescaleDB: Compression policy TimeScale Forum: Tuple decompression limit exceeded by operation Hugging Face Datasets: jettisonthenet/timeseries_trending_youtube_videos_2019-04-15_to_2020-04-15 Hugging Face Datasets: wykonos/movies Ollama: nomic-embed-text timescale/pgai : Migrating from the extension to the python library Timescale: SQL interface for pgvector and pgvectorscale ","date":"27 May 2025","externalUrl":null,"permalink":"/posts/postgresql-maximalism/","section":"","summary":"Summary # Unedited research notes for my \u0026ldquo;PostgreSQL Maximalism\u0026rdquo; series.","title":"PostgreSQL Maximalism","type":"posts"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/research-notes/","section":"Tags","summary":"","title":"Research-Notes","type":"tags"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/series/","section":"Tags","summary":"","title":"Series","type":"tags"},{"content":"","date":"27 May 2025","externalUrl":null,"permalink":"/tags/unedited/","section":"Tags","summary":"","title":"Unedited","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"Last updated: May 25, 2025\nThis website uses cookies to improve your browsing experience and to analyze how visitors use our site. We use only Google Analytics cookies for this purpose.\nWhat Are Cookies? # Cookies are small text files placed on your device by websites you visit. They help websites remember information about your visit, such as language preferences or how you interact with the site.\nWhich Cookies Do We Use? # We use Google Analytics cookies to collect anonymous information about website usage, including:\nNumber of visitors Pages visited Time spent on the site Referring websites Browser and device type These cookies help us understand and improve how visitors use our website.\nHow We Use Google Analytics Cookies # IP addresses are anonymized to protect your privacy. Data collected is aggregated and does not identify individual users. We do not use Google Analytics cookies for advertising or personal profiling. Data is retained for a maximum of 14 months. Your Consent and Control # We will only set Google Analytics cookies after you provide explicit consent through our cookie banner.\nYou can manage or withdraw your consent at any time by:\nChanging your cookie preferences in the cookie settings link available on the website. Adjusting your browser settings to block or delete cookies. More Information # For more details about Google Analytics and how your data is handled, please visit Google\u0026rsquo;s Privacy Policy.\n","externalUrl":null,"permalink":"/cookies/","section":"Data Lab Tech TV","summary":"Last updated: May 25, 2025","title":"Cookie Policy","type":"page"},{"content":"Last updated: May 25, 2025\nWe respect your privacy and are committed to protecting your personal data. This privacy policy explains how we collect, use, and protect information when you visit our website.\n1. Who We Are # Data Lab Tech TV\nEmail: mail@datalabtechtv.com\nWebsite: https://datalabtechtv.com\nWe act as the Data Controller for your personal data as defined under the General Data Protection Regulation (GDPR).\n2. What Data We Collect # We do not collect personal data unless you voluntarily provide it (e.g., by contacting us). However, we use Google Analytics to collect anonymized usage data, such as:\nPages viewed Browser type and version Operating system Referring website Approximate geographic location (based on anonymized IP) Time and duration of visit ‚ö†Ô∏è We do not collect names, email addresses, or other personally identifiable information (PII) via analytics.\n3. Legal Basis for Processing # We process analytics data based on your explicit consent through our cookie banner. You may withdraw consent at any time.\n4. Use of Data # We use Google Analytics data to:\nUnderstand how our website is used Improve website performance and user experience We do not:\nUse your data for advertising or marketing Share your data with third parties for commercial purposes 5. Cookies and Analytics # We use only Google Analytics cookies with:\nIP anonymization enabled Data sharing with Google disabled Data retention set to 14 months For more details, see our Cookie Policy.\n6. International Data Transfers # Google may transfer data to servers in the United States. We rely on appropriate safeguards including:\nA Data Processing Agreement with Google Standard Contractual Clauses (SCCs), as approved by the European Commission 7. Your Rights Under GDPR # You have the right to:\nAccess your personal data Request correction or deletion Withdraw your consent Object to data processing Lodge a complaint with a supervisory authority To exercise any of these rights, contact us at mail@datalabtechtv.com.\n8. Data Security # We take appropriate technical and organizational measures to ensure a level of security appropriate to the risk.\n9. Retention # We retain anonymized analytics data for a maximum of 14 months.\n10. Changes to This Policy # We may update this policy from time to time. Any changes will be posted on this page with an updated revision date.\n11. Contact # If you have questions or concerns about this privacy policy or your personal data, contact us at:\nData Lab Tech TV\nEmail: mail@datalabtechtv.com\n","externalUrl":null,"permalink":"/privacy/","section":"Data Lab Tech TV","summary":"Last updated: May 25, 2025","title":"Privacy Policy","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]